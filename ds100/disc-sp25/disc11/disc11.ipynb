{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Logistic Regression\"\n",
    "execute:\n",
    "  echo: true\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "    code-tools: true\n",
    "    toc: true\n",
    "    toc-title: \"Logistic Regression\"\n",
    "    page-layout: full\n",
    "    theme:\n",
    "      - cosmo\n",
    "      - cerulean\n",
    "    callout-icon: false\n",
    "jupyter:\n",
    "  jupytext:\n",
    "    text_representation:\n",
    "      extension: .qmd\n",
    "      format_name: quarto\n",
    "      format_version: '1.0'\n",
    "      jupytext_version: 1.16.1\n",
    "  kernelspec:\n",
    "    display_name: data100quarto\n",
    "    language: python\n",
    "    name: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Link to Slides](https://docs.google.com/presentation/d/1Uity00kUjV7JK2PUXR3LvIb0tIxF2y74bGXA4pYGwOg/edit?usp=sharing)\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "Suppose we are given the following dataset, with two features ($\\Bbb{X}_{:, 0}$ and $\\Bbb{X}_{:, 1}$) and one binary response variable ($y$)\n",
    "\n",
    "<center>\n",
    "\n",
    "|$\\mathbb{X}_{:, 0}$ | $\\mathbb{X}_{:, 1}$ | $y$ |\n",
    "| :-: | :-: | :-: |\n",
    "| 2 | 2 | 0 |\n",
    "| 1 | -1 | 1 |\n",
    "\n",
    "</center>\n",
    "\n",
    "Here, $\\vec{x}^T$ corresponds to a single row of our data matrix, not including the $y$ column. Thus, we can write $\\vec{x}_1^T$ as $\\vec{x}_1^T = \\left[2 \\quad 2\\right]$. Note that there is no intercept term!\n",
    "\n",
    "Suppose you run a Logistic Regression model to determine the probability that $Y=1$ given $\\vec{x}$. We denote probability as $P_{\\hat{\\theta}}$ as opposed to just $P$ to show that $\\hat{\\theta}$ is a \\textit{learned parameter} like it is in OLS, where we denote our function as $f_{\\theta}(x)$.\n",
    "\n",
    "$$P_{\\hat{\\theta}}(Y=1|\\vec{x}) = \\sigma(\\vec{x}^T \\theta) = \\frac{1}{1 + \\exp(- \\vec{x}^T \\theta)}$$\n",
    "\n",
    "Your algorithm learns that the optimal $\\hat\\theta$ value is $\\hat\\theta = \\left[-\\frac{1}{2} \\quad-\\frac{1}{2}\\right]^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\n",
    "\n",
    "$\\sigma(z)$ is called a \"sigmoid\" function with the equation $$\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$$ for some arbitrary real number $z$. What is the range of possible values for $\\sigma(\\cdot)$?\n",
    "\n",
    "<details>\n",
    "<summary><b>Answer</b></summary>\n",
    "\n",
    "* $x \\to -\\infty: \\frac{1}{1 + e^{-z}} \\to 0$\n",
    "\n",
    "* $x \\to \\infty: \\frac{1}{1 + e^{-z}} \\to \\frac{1}{1 + 0} = 1$\n",
    "\n",
    "$0 < \\sigma(z) < 1$. Note that the sigmoid function can never *equal* 0 or 1.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\n",
    "\n",
    "Calculate $P_{\\hat{\\theta}}(Y=1|\\vec{x}^T=\\left[1 \\quad   0\\right])$.\n",
    "\n",
    "<details>\n",
    "<summary><b>Answer</b></summary>\n",
    "\n",
    "Calculate $\\vec{x}^T\\hat{\\theta}$ first:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\vec{x}^T\\hat{\\theta} &= \\begin{bmatrix}1 & 0\\end{bmatrix}\\begin{bmatrix} -\\frac{1}{2} \\\\ -\\frac{1}{2} \\end{bmatrix}\\\\\n",
    "    &= 1*-\\frac{1}{2} + 0*-\\frac{1}{2}\\\\\n",
    "    &= -\\frac{1}{2}\n",
    "\\end{align*}\n",
    "\n",
    "We can then plug this into the sigmoid function:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\sigma(-\\frac{1}{2}) &= \\frac{1}{1 + e^{\\frac{1}{2}}}\\\\\n",
    "    &\\approx 0.38\n",
    "\\end{align*}\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\n",
    "Using a threshold of $T = 0.5$, what would our algorithm classify $y$ as given the results of part b?\n",
    "\n",
    "<details>\n",
    "<summary><b>Answer</b></summary>\n",
    "\n",
    "Our $p$ of $0.38$ is *smaller* than the threshold of $0.5$, so our algorithm would classify $y$ as class $0$.\n",
    "\n",
    "A **Decision Rule** tells us how to classify a data point:\n",
    "\n",
    "$$\\hat{y} = \n",
    "\\begin{cases}\n",
    "\\text{Class 1} &\\text{if } \\quad p \\geq T\\\\\n",
    "\\text{Class 0} &\\text{if } \\quad p \\lt T\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d)\n",
    "\n",
    "The empirical risk using cross-entropy loss is given by the following expression. Remember, whenever you see $\\log$ in this course, you must assume the natural logarithm (base-$e$) unless explicitly told otherwise.\n",
    "\n",
    "\\begin{align*}\n",
    "\tR(\\theta) &= -\\dfrac{1}{n} \\sum_{i=1}^{n} \\big( y_i \\log P_{\\theta}(Y=1|\\vec{x_i}) + (1-y_i) \\log  P_{\\theta}(Y=0|\\vec{x_i}) \\big)\n",
    "\\end{align*} \n",
    "\n",
    " Suppose we run a different algorithm and obtain $\\hat\\theta_{new} = \\left[0 \\quad 0\\right]^T$. Calculate the empirical risk for $\\hat\\theta_{new}$ on our dataset.\n",
    "\n",
    "<details>\n",
    "<summary><b>Answer</b></summary>\n",
    "\n",
    "\\begin{align*}\n",
    "\tR(\\hat\\theta_{new}) &= -\\dfrac{1}{2} \\sum_{i=1}^{2} \\big( y_i \\log P_{\\hat{\\theta_{new}}}(Y=1|\\vec{x_i}) + (1-y_i) \\log  P_{\\hat{\\theta_{new}}}(Y=0|\\vec{x_i}) )\\\\\n",
    "\t&= -\\frac{1}{2} [(0 \\log P_{\\hat{\\theta_{new}}}(Y=1|\\vec{x_1}) + 1 \\log  P_{\\hat{\\theta_{new}}}(Y=0|\\vec{x_1})) + \\\\ & (1 \\log P_{\\hat{\\theta_{new}}}(Y=1|\\vec{x_2}) + 0 \\log  P_{\\hat{\\theta_{new}}}(Y=0|\\vec{x_2}))] \\\\\n",
    "\t&= -\\frac{1}{2} (\\log P_{\\hat{\\theta_{new}}}(Y=0|\\vec{x_1}) + \\log P_{\\hat{\\theta_{new}}}(Y=1|\\vec{x_2})) \\\\\n",
    "\t&= -\\frac{1}{2} (\\log (1 - \\sigma(0)) + \\log \\sigma(0)) \\\\ \n",
    "\t&= -\\log(0.5) = \\log 2 \\approx 0.693\n",
    "\\end{align*}\n",
    "\n",
    "Notice that the inputs to the sigmoid function are $0$ because $\\hat{\\theta}_{new}$ is the zero vector, making all $\\vec{x}^T\\hat{\\theta}_{new} = 0$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linearly Separable Data\n",
    "\n",
    "Suppose we have two different Logistic Regression models, A and B, and we run gradient descent for 1000 steps to obtain the model parameters $\\hat\\theta_A = \\left[-\\frac{1}{2} \\quad-\\frac{1}{2}\\right]^T$ and $\\hat\\theta_B = \\left[0 \\quad0\\right]^T$. How do they compare?\n",
    "\n",
    "The dataset is reproduced below for your convenience.\n",
    "\n",
    "<center>\n",
    "\n",
    "|$\\mathbb{X}_{:, 0}$ | $\\mathbb{X}_{:, 1}$ | $y$ |\n",
    "| :-: | :-: | :-: |\n",
    "| 2 | 2 | 0 |\n",
    "| 1 | -1 | 1 |\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\n",
    "Is our dataset linearly separable? If so, write the equation of a hyperplane that separates the two classes. Otherwise, briefly explain why not (Hint: draw the two data points).\n",
    "\n",
    "<details>\n",
    "<summary><b>Answer</b></summary>\n",
    "\n",
    "Yes, the line $\\mathbb{X}_{:, 1} = 0$ (line on horizontal axis) separates the data in feature space!\n",
    "\n",
    "<center><img src = \"images/q2a.png\" width = 500></center>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\n",
    "If we let gradient descent keep running indefinitely for our two models, will either of them converge given the design matrix above? Why? If not, how can we remedy this?\n",
    "\n",
    "<details>\n",
    "<summary><b>Answer</b></summary>\n",
    "\n",
    "No.\n",
    "\n",
    "Our dataset is linearly separable, so the *optimal* cross-entropy loss is 0. However, a cross-entropy loss of 0 can never be achieved. Remember that $\\sigma(z)$ never outputs *exactly* 0 or 1, but it can get arbitrarily close! \n",
    "\n",
    "Hence, no single value of $\\theta$ will ever \"minimize\" cross-entropy loss (ie. let $\\sigma(x^T \\theta) = 0$), but gradient descent can bring the cross-entropy loss closer and closer to 0 as $\\theta$ goes to $\\pm \\infty$.\n",
    "\n",
    "To avoid our absolute values of the weights diverging to $\\infty$, we can regularize our cross-entropy loss and penalize arbitrarily large $\\theta$! \n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\n",
    "Assume we add the data point $[3, -2]$ to the design matrix such that our resulting design matrix is as follows:\n",
    "\n",
    "<center>\n",
    "\n",
    "| $\\mathbb{X}_{:, 0}$ | $\\mathbb{X}_{:, 1}$ | $y$ |\n",
    "| :-: | :-: | :-: |\n",
    "| 2 | 2 | 0 |\n",
    "| 1 | -1 | 1 |\n",
    "| 3 | -2 | 0 |\n",
    "\n",
    "</center>\n",
    "\n",
    "Is it possible to achive a perfect accuracy using logistic regression?\n",
    "\n",
    "<details>\n",
    "<summary><b>Answer</b></summary>\n",
    "\n",
    "The data is still linearly separable, so we can train a logistic regression model to achieve perfect accuracy! For example, $\\mathbb{X}_{:, 0} = 1.5$ would result in perfect accuracy.\n",
    "\n",
    "<center><img src = \"images/q2c.png\" width = 500></center>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curves\n",
    "Consider the following ROC (receiver operating characteristic) curves that were each created from different models. \n",
    "\n",
    "<center><img src=\"images/roc.png\" width = 500></center>\n",
    "\n",
    "### (a)\n",
    "Compare the Area Under the Curve (AUC) of Line 1 and Line 4. What kind of model would predict Line 1? What kind of model would predict Line 4? \n",
    "\n",
    "<details>\n",
    "<summary><b>Answer</b></summary>\n",
    "\n",
    "Line 1 (solid black line) is known as a \"perfect predictor\"; it always predicts the correct class for $y$, so its true positive rate (TPR) is 1, and its false positive rate (FPR) is 0. Because we want our classifier to be as close as possible to the perfect predictor, we aim to maximize the AUC.\n",
    "\n",
    "On the other hand, Line 4 (solid grey line) is a random predictor that predicts $y=1$ with a probability of 0.5 and $y=0$ with a probability of 0.5. It's AUC = 0.5, and it does no better than a random coin flip (proof in course notes).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\n",
    "Suppose we fix the decision threshold for all 4 models such that we get an FPR of 0.1 when we evaluate our model. In order of most to least preferred, rank models given their ROC curve. \n",
    "\n",
    "<details>\n",
    "<summary><b>Answer</b></summary>\n",
    "\n",
    "Since the FPR is now fixed at 0.1, the only thing that we can adjust is the TPR. To determine the most to least preferred model, we look at each model's TPR at an FPR of 0.1. Higher TPRs are better because they indicate that a greater proportion of true positive cases are correctly identified by the model. Hence, we get the following ranking: \n",
    "$$\\text{Line 1} > \\text{Line 3} >\\text{Line 2} >\\text{Line 4} $$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics\n",
    "\n",
    "Here are some classification performance metrics (from Fall 2023 Final Reference Sheet):\n",
    "\n",
    "<center>\n",
    "\n",
    "| Metric | Formula | Other Names |\n",
    "| :-: | :-: | :-: |\n",
    "| Accuracy | $\\frac{TP+TN}{n}$ | |\n",
    "| Precision | $\\frac{TP}{TP + FP}$ | |\n",
    "| Recall/TPR | $\\frac{TP}{TP + FN}$ | True Positive Rate, Sensitivity |\n",
    "| FPR | $\\frac{FP}{FP + TN}$ | False Positive Rate, Specificity |\n",
    "\n",
    "</center>\n",
    "\n",
    "Suppose we train a binary classifier on the following dataset where $y$ is the set of true labels, and $\\hat{y}$ is the set of predicted labels:\n",
    "\n",
    "<center>\n",
    "\n",
    "| $y$ | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 |\n",
    "| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\n",
    "| $\\hat{y}$ | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 |\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\n",
    "Fill out the confusion matrix for the given data.\n",
    "*Hint:* The first row contains the true negatives and false positives, and the second row contains false negatives and true positives (in that order).\n",
    "\n",
    "<center>\n",
    "\n",
    "| TN: &nbsp;| FP: &nbsp;|\n",
    "| :-: | :-: |\n",
    "| **FN:** &nbsp;| **TP:** &nbsp;|\n",
    "\n",
    "</center>\n",
    "\n",
    "<details>\n",
    "<summary><b>Answer</b></summary>\n",
    "\n",
    "<center>\n",
    "\n",
    "| TN: 1| FP: 4|\n",
    "| :-: | :-: |\n",
    "| **FN: 3** | **TP: 2** |\n",
    "\n",
    "</center>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\n",
    "The precision of our classifier. Write your answer as a simplified fraction.\n",
    "\n",
    "<details>\n",
    "<summary><b>Answer</b></summary>\n",
    "\n",
    "$\\text{Precision} = \\frac{TP}{TP + FP} = \\frac{2}{2 + 4} = \\frac{1}{3}$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\n",
    "The recall of our classifier. Write your answer as a simplified fraction.\n",
    "\n",
    "<details>\n",
    "<summary><b>Answer</b></summary>\n",
    "\n",
    "$\\text{Recall} = \\frac{TP}{TP + FN} = \\frac{2}{2+3} \\frac{2}{5}$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d)\n",
    "**(Discussion)** It is revealed that this dataset describes the results of an algorithm used to predict whether someone is at risk of developing a severe disease with expensive treatment. You are tasked with improving the classifier. Which metrics should you aim to optimize for (Accuracy/Precision/Recall)? Explain your reasoning. (Things to consider: cost of treatment, the severity of disease)\n",
    "\n",
    "<details>\n",
    "<summary><b>Answer</b></summary>\n",
    "\n",
    "There is no singular correct answer, but here are some examples of reasons.\n",
    "        \n",
    "* **Accuracy**: Since this dataset is fairly balanced, accuracy is not too bad of a metric. (3/10). The main flaw of using accuracy as the metric is that it is agnostic towards the original class of the data point when evaluating performance. \n",
    "\n",
    "* **Precision**: Optimizing for precision means that we care more about making sure the positives we output are truly positive. In this setting, we want to ensure that the people we predict to have the disease truly have the disease. If the cost of treatment is very expensive, we don't want to overburden people who may not have this disease financially.\n",
    "\n",
    "* **Recall**: Optimizing for recall means we care more about detecting all the true positives from the dataset. In this setting, we want to make sure that almost everyone who has the disease knows they have it. If the disease is particularly deadly, then we would be aiming to save the most lives. \n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "history": [
   {
    "code": "import numpy as np",
    "id": "e03ee822cc7941e5863f0723eca7b32f",
    "idx": 0,
    "time": "2021-02-01T23:27:15.701Z",
    "type": "execution"
   },
   {
    "id": "e03ee822cc7941e5863f0723eca7b32f",
    "time": "2021-02-01T23:27:16.012Z",
    "type": "completion"
   },
   {
    "code": "psum = 0\nfor i in np.arange(1,25):\n    psum += i",
    "id": "a99f4962d8764231858c8662c0eda629",
    "idx": 1,
    "time": "2021-02-01T23:27:40.950Z",
    "type": "execution"
   },
   {
    "id": "a99f4962d8764231858c8662c0eda629",
    "time": "2021-02-01T23:27:42.055Z",
    "type": "completion"
   },
   {
    "code": "psum",
    "id": "42ea2fe1dcfb42178b4db5cbad763b27",
    "idx": 2,
    "time": "2021-02-01T23:27:44.325Z",
    "type": "execution"
   },
   {
    "id": "42ea2fe1dcfb42178b4db5cbad763b27",
    "time": "2021-02-01T23:27:44.419Z",
    "type": "completion"
   },
   {
    "code": "def foo(t, c, o):\n    return t + c + o",
    "id": "8babcdfe4430400abec0840f90a19be2",
    "idx": 1,
    "time": "2021-02-02T00:05:27.779Z",
    "type": "execution"
   },
   {
    "id": "8babcdfe4430400abec0840f90a19be2",
    "time": "2021-02-02T00:05:27.852Z",
    "type": "completion"
   },
   {
    "code": "psum = 0\nfor t in range(0, 25):\n    for c in range(0, 25-t):\n        o = 24 - t -c\n        psum += foo(t, c, o)\npsum",
    "id": "f23e1481f10b4f03a9a1af1daced3232",
    "idx": 3,
    "time": "2021-02-02T00:06:12.517Z",
    "type": "execution"
   },
   {
    "id": "f23e1481f10b4f03a9a1af1daced3232",
    "time": "2021-02-02T00:06:12.593Z",
    "type": "completion"
   },
   {
    "code": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline",
    "id": "06ffec8eed2f4a27b109de801c9e979b",
    "idx": 0,
    "time": "2021-02-08T22:29:47.543Z",
    "type": "execution"
   },
   {
    "id": "06ffec8eed2f4a27b109de801c9e979b",
    "time": "2021-02-08T22:29:48.050Z",
    "type": "completion"
   },
   {
    "code": "x =  np.random.choice(np.arange(10), 10000)",
    "id": "4fe735effc854b38867c42ca2f125d47",
    "idx": 1,
    "time": "2021-02-08T22:30:56.492Z",
    "type": "execution"
   },
   {
    "id": "4fe735effc854b38867c42ca2f125d47",
    "time": "2021-02-08T22:30:56.558Z",
    "type": "completion"
   },
   {
    "code": "x",
    "id": "b2d1b0807a8641c9803595eca99bc556",
    "idx": 2,
    "time": "2021-02-08T22:30:59.899Z",
    "type": "execution"
   },
   {
    "id": "b2d1b0807a8641c9803595eca99bc556",
    "time": "2021-02-08T22:30:59.976Z",
    "type": "completion"
   },
   {
    "code": "x_bias =  x + np.random.normal(size=10000)",
    "id": "18cd4fe3ba4e459f8feabc6d297a1a6d",
    "idx": 3,
    "time": "2021-02-08T22:31:20.570Z",
    "type": "execution"
   },
   {
    "id": "18cd4fe3ba4e459f8feabc6d297a1a6d",
    "time": "2021-02-08T22:31:20.645Z",
    "type": "completion"
   },
   {
    "code": "x_bias",
    "id": "ebeef9a125e24f858a7eb0bd63c41748",
    "idx": 4,
    "time": "2021-02-08T22:31:21.852Z",
    "type": "execution"
   },
   {
    "id": "ebeef9a125e24f858a7eb0bd63c41748",
    "time": "2021-02-08T22:31:21.920Z",
    "type": "completion"
   },
   {
    "code": "x_bias = x + np.random.choice([-1, 0, 1, 2], size=10000)",
    "id": "18cd4fe3ba4e459f8feabc6d297a1a6d",
    "idx": 3,
    "time": "2021-02-08T22:31:58.985Z",
    "type": "execution"
   },
   {
    "id": "18cd4fe3ba4e459f8feabc6d297a1a6d",
    "time": "2021-02-08T22:31:59.091Z",
    "type": "completion"
   },
   {
    "code": "x_bias",
    "id": "ebeef9a125e24f858a7eb0bd63c41748",
    "idx": 4,
    "time": "2021-02-08T22:31:59.857Z",
    "type": "execution"
   },
   {
    "id": "ebeef9a125e24f858a7eb0bd63c41748",
    "time": "2021-02-08T22:31:59.960Z",
    "type": "completion"
   },
   {
    "code": "np.mean(x)",
    "id": "35e1a08427744f458ae6df6cbbe76b43",
    "idx": 5,
    "time": "2021-02-08T22:32:10.910Z",
    "type": "execution"
   },
   {
    "id": "35e1a08427744f458ae6df6cbbe76b43",
    "time": "2021-02-08T22:32:13.118Z",
    "type": "completion"
   },
   {
    "code": "means = []\nn = 10\nfor _ in range(1000):\n    sam = np.random.choice(x_bias, size=n)\n    means.append(np.mean(sam))",
    "id": "d8146f43f8de4d32900171422fa79676",
    "idx": 6,
    "time": "2021-02-08T22:32:55.821Z",
    "type": "execution"
   },
   {
    "id": "d8146f43f8de4d32900171422fa79676",
    "time": "2021-02-08T22:32:55.983Z",
    "type": "completion"
   },
   {
    "code": "plt.hist(means)",
    "id": "d9355ed602a64c008badc48996df4e13",
    "idx": 7,
    "time": "2021-02-08T22:32:58.406Z",
    "type": "execution"
   },
   {
    "id": "d9355ed602a64c008badc48996df4e13",
    "time": "2021-02-08T22:32:58.841Z",
    "type": "completion"
   },
   {
    "code": "means = []\nn = 100\nfor _ in range(1000):\n    sam = np.random.choice(x_bias, size=n)\n    means.append(np.mean(sam))",
    "id": "d8146f43f8de4d32900171422fa79676",
    "idx": 6,
    "time": "2021-02-08T22:33:03.668Z",
    "type": "execution"
   },
   {
    "id": "d8146f43f8de4d32900171422fa79676",
    "time": "2021-02-08T22:33:03.856Z",
    "type": "completion"
   },
   {
    "code": "plt.hist(means)",
    "id": "d9355ed602a64c008badc48996df4e13",
    "idx": 7,
    "time": "2021-02-08T22:33:04.064Z",
    "type": "execution"
   },
   {
    "id": "d9355ed602a64c008badc48996df4e13",
    "time": "2021-02-08T22:33:04.329Z",
    "type": "completion"
   },
   {
    "code": "means = []\nn = 100\nfor n in [10, 50, 100, 300, 500]:\n    means = []\n    for _ in range(1000):\n        sam = np.random.choice(x_bias, size=n)\n        means.append(np.mean(sam))\n    plt.hist(means)",
    "id": "d8146f43f8de4d32900171422fa79676",
    "idx": 6,
    "time": "2021-02-08T22:33:29.042Z",
    "type": "execution"
   },
   {
    "id": "d8146f43f8de4d32900171422fa79676",
    "time": "2021-02-08T22:33:29.678Z",
    "type": "completion"
   },
   {
    "code": "means = []\nn = 100\nfor n in [10, 50, 100, 300, 500]:\n    means = []\n    for _ in range(1000):\n        sam = np.random.choice(x, size=n)\n        means.append(np.mean(sam))\n    plt.hist(means)",
    "id": "2afd6ab47d784e24ad5196d21087cce0",
    "idx": 7,
    "time": "2021-02-08T22:33:49.689Z",
    "type": "execution"
   },
   {
    "id": "2afd6ab47d784e24ad5196d21087cce0",
    "time": "2021-02-08T22:33:50.360Z",
    "type": "completion"
   },
   {
    "code": "means = []\nn = 100\nfor n in [10, 50, 100, 300, 500]:\n    means = []\n    for _ in range(1000):\n        sam = np.random.choice(x_bias, size=n)\n        means.append(np.mean(sam))\n    plt.hist(means)\n    plt.xlim(1, 9)",
    "id": "05d972515cea4580886753b9ab5fdf3b",
    "idx": 14,
    "time": "2021-02-09T19:17:58.105Z",
    "type": "execution"
   },
   {
    "id": "05d972515cea4580886753b9ab5fdf3b",
    "time": "2021-02-09T19:17:58.255Z",
    "type": "completion"
   },
   {
    "code": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline",
    "id": "bd55791b6cc644c79509ae8671d5ff5b",
    "idx": 0,
    "time": "2021-02-09T19:18:04.311Z",
    "type": "execution"
   },
   {
    "code": "x = np.random.choice(np.arange(10), 10000)",
    "id": "aadf2b19475c4be0a4d2563e4d29297e",
    "idx": 1,
    "time": "2021-02-09T19:18:04.313Z",
    "type": "execution"
   },
   {
    "code": "x",
    "id": "8f08070f418644508de6c795fb20c2c9",
    "idx": 2,
    "time": "2021-02-09T19:18:04.315Z",
    "type": "execution"
   },
   {
    "code": "x_bias = x + np.random.choice([-1, 0, 1, 2], size=10000)",
    "id": "f2ccf76d197d45099bd62b103840e3ce",
    "idx": 3,
    "time": "2021-02-09T19:18:04.316Z",
    "type": "execution"
   },
   {
    "code": "x_bias",
    "id": "22f916c8254e4e6a8ea3d7918ae2d427",
    "idx": 4,
    "time": "2021-02-09T19:18:04.317Z",
    "type": "execution"
   },
   {
    "code": "np.mean(x)",
    "id": "b0dab4614d5746c783d6902a7ed82212",
    "idx": 5,
    "time": "2021-02-09T19:18:04.318Z",
    "type": "execution"
   },
   {
    "code": "means = []\nn = 100\nfor n in [10, 50, 100, 300, 500]:\n    means = []\n    for _ in range(1000):\n        sam = np.random.choice(x_bias, size=n)\n        means.append(np.mean(sam))\n    plt.hist(means)\n    plt.xlim(1, 9)",
    "id": "05d972515cea4580886753b9ab5fdf3b",
    "idx": 14,
    "time": "2021-02-09T19:18:04.329Z",
    "type": "execution"
   },
   {
    "code": "means = []\nn = 100\nfor n in [10, 50, 100, 300, 500]:\n    means = []\n    for _ in range(1000):\n        sam = np.random.choice(x, size=n)\n        means.append(np.mean(sam))\n    plt.hist(means)\n    plt.xlim(1, 9)",
    "id": "722dc1e6340e42aa8ae45019cfe47c82",
    "idx": 15,
    "time": "2021-02-09T19:18:04.331Z",
    "type": "execution"
   },
   {
    "id": "bd55791b6cc644c79509ae8671d5ff5b",
    "time": "2021-02-09T19:18:04.979Z",
    "type": "completion"
   },
   {
    "id": "aadf2b19475c4be0a4d2563e4d29297e",
    "time": "2021-02-09T19:18:04.985Z",
    "type": "completion"
   },
   {
    "id": "8f08070f418644508de6c795fb20c2c9",
    "time": "2021-02-09T19:18:05.027Z",
    "type": "completion"
   },
   {
    "id": "f2ccf76d197d45099bd62b103840e3ce",
    "time": "2021-02-09T19:18:05.058Z",
    "type": "completion"
   },
   {
    "id": "22f916c8254e4e6a8ea3d7918ae2d427",
    "time": "2021-02-09T19:18:05.103Z",
    "type": "completion"
   },
   {
    "id": "b0dab4614d5746c783d6902a7ed82212",
    "time": "2021-02-09T19:18:05.153Z",
    "type": "completion"
   },
   {
    "id": "05d972515cea4580886753b9ab5fdf3b",
    "time": "2021-02-09T19:18:06.082Z",
    "type": "completion"
   },
   {
    "id": "722dc1e6340e42aa8ae45019cfe47c82",
    "time": "2021-02-09T19:18:06.898Z",
    "type": "completion"
   },
   {
    "code": "means = []\nn = 100\nplt.figure(figsize=[10, 12])\nfor i, pop in enumerate([x_bias, x]):\n    plt.subplot(2, 1, i + 1)\n    for n in [10, 50, 100, 300, 500]:\n        means = []\n        for _ in range(1000):\n            sam = np.random.choice(pop, size=n)\n            means.append(np.mean(sam))\n        plt.hist(means)\n        plt.xlim(1, 9)",
    "id": "05d972515cea4580886753b9ab5fdf3b",
    "idx": 14,
    "time": "2021-02-09T19:19:27.290Z",
    "type": "execution"
   },
   {
    "id": "05d972515cea4580886753b9ab5fdf3b",
    "time": "2021-02-09T19:19:28.587Z",
    "type": "completion"
   },
   {
    "code": "means = []\nn = 100\nplt.figure(figsize=[10, 12])\nfor i, pop in enumerate([x_bias, x]):\n    plt.subplot(2, 1, i + 1)\n    for n in [10, 50, 100, 300, 500]:\n        means = []\n        for _ in range(1000):\n            sam = np.random.choice(pop, size=n)\n            means.append(np.mean(sam))\n        plt.hist(means)\n        plt.xlim(1, 9)\n        if i == 0:\n            plt.title(\"Bias Sampling Procedure\")\n        else:\n            plt.title(\"Unbias Sampling Procedure\")",
    "id": "05d972515cea4580886753b9ab5fdf3b",
    "idx": 14,
    "time": "2021-02-09T19:19:49.527Z",
    "type": "execution"
   },
   {
    "id": "05d972515cea4580886753b9ab5fdf3b",
    "time": "2021-02-09T19:19:50.843Z",
    "type": "completion"
   },
   {
    "code": "means = []\nn = 100\nplt.figure(figsize=[10, 12])\nfor i, pop in enumerate([x_bias, x]):\n    plt.subplot(2, 1, i + 1)\n    for n in [10, 50, 100, 300, 500]:\n        means = []\n        for _ in range(1000):\n            sam = np.random.choice(pop, size=n)\n            means.append(np.mean(sam))\n        plt.hist(means)\n        plt.xlim(1, 9)\n        if i == 0:\n            plt.title(\"Bias Sampling Procedure\")\n        else:\n            plt.title(\"Unbiased Sampling Procedure\")",
    "id": "05d972515cea4580886753b9ab5fdf3b",
    "idx": 14,
    "time": "2021-02-09T19:19:51.357Z",
    "type": "execution"
   },
   {
    "id": "05d972515cea4580886753b9ab5fdf3b",
    "time": "2021-02-09T19:19:52.644Z",
    "type": "completion"
   },
   {
    "code": "means = []\nn = 100\nplt.figure(figsize=[10, 12])\nfor i, pop in enumerate([x_bias, x]):\n    plt.subplot(2, 1, i + 1)\n    for n in [10, 50, 100, 300, 500]:\n        means = []\n        for _ in range(1000):\n            sam = np.random.choice(pop, size=n)\n            means.append(np.mean(sam))\n        plt.hist(means)\n        plt.xlim(1, 9)\n        if i == 0:\n            plt.title(\"Biased Sampling Procedure\")\n        else:\n            plt.title(\"Unbiased Sampling Procedure\")",
    "id": "05d972515cea4580886753b9ab5fdf3b",
    "idx": 14,
    "time": "2021-02-09T19:20:03.045Z",
    "type": "execution"
   },
   {
    "id": "05d972515cea4580886753b9ab5fdf3b",
    "time": "2021-02-09T19:20:04.367Z",
    "type": "completion"
   },
   {
    "code": "np,mean(x_bias)",
    "id": "1cc76e95c4d74d779e7e4b9ed9aa0abf",
    "idx": 6,
    "time": "2021-02-09T19:20:35.284Z",
    "type": "execution"
   },
   {
    "id": "1cc76e95c4d74d779e7e4b9ed9aa0abf",
    "time": "2021-02-09T19:20:35.403Z",
    "type": "completion"
   },
   {
    "code": "np.mean(x_bias)",
    "id": "1cc76e95c4d74d779e7e4b9ed9aa0abf",
    "idx": 6,
    "time": "2021-02-09T19:20:38.077Z",
    "type": "execution"
   },
   {
    "id": "1cc76e95c4d74d779e7e4b9ed9aa0abf",
    "time": "2021-02-09T19:20:38.164Z",
    "type": "completion"
   }
  ],
  "kernelspec": {
   "display_name": "data100quarto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
