<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>10&nbsp; Bias and Variance – Data 100 Discussion Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../disc08/disc08.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-ca5a086e270bb62b76934925835b48c3.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../disc09/disc09.html"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Bias and Variance</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Data 100 Discussion Notes</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">disc-sp25</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../disc01/disc01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Math Prerequisites</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../disc02/disc02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Pandas I</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../disc03/disc03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Pandas II, EDA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../disc04/disc04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Regex, Visualization, and Transformation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../disc05/disc05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Transformations, Sampling, and SLR</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../disc06/disc06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Models, OLS</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../disc07/disc07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Gradient Descent, Feature Engineering, Housing</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../disc08/disc08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Cross-Validation and Regularization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../disc09/disc09.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Bias and Variance</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Bias and Variance</h2>
   
  <ul>
  <li><a href="#link-to-slides" id="toc-link-to-slides" class="nav-link active" data-scroll-target="#link-to-slides"><span class="header-section-number">10.0.1</span> Link to Slides</a></li>
  <li><a href="#bias-variance-tradeoff" id="toc-bias-variance-tradeoff" class="nav-link" data-scroll-target="#bias-variance-tradeoff"><span class="header-section-number">10.1</span> Bias-Variance Tradeoff</a>
  <ul class="collapse">
  <li><a href="#a" id="toc-a" class="nav-link" data-scroll-target="#a"><span class="header-section-number">10.1.1</span> (a)</a></li>
  <li><a href="#b" id="toc-b" class="nav-link" data-scroll-target="#b"><span class="header-section-number">10.1.2</span> (b)</a></li>
  <li><a href="#c" id="toc-c" class="nav-link" data-scroll-target="#c"><span class="header-section-number">10.1.3</span> (c)</a></li>
  </ul></li>
  <li><a href="#bias-variance-tradeoff-1" id="toc-bias-variance-tradeoff-1" class="nav-link" data-scroll-target="#bias-variance-tradeoff-1"><span class="header-section-number">10.2</span> Bias-Variance Tradeoff</a>
  <ul class="collapse">
  <li><a href="#a-1" id="toc-a-1" class="nav-link" data-scroll-target="#a-1"><span class="header-section-number">10.2.1</span> (a)</a></li>
  <li><a href="#b-1" id="toc-b-1" class="nav-link" data-scroll-target="#b-1"><span class="header-section-number">10.2.2</span> (b)</a></li>
  <li><a href="#c-1" id="toc-c-1" class="nav-link" data-scroll-target="#c-1"><span class="header-section-number">10.2.3</span> (c)</a></li>
  </ul></li>
  <li><a href="#regularization-and-bias-variance-tradeoff" id="toc-regularization-and-bias-variance-tradeoff" class="nav-link" data-scroll-target="#regularization-and-bias-variance-tradeoff"><span class="header-section-number">10.3</span> Regularization and Bias-Variance Tradeoff</a>
  <ul class="collapse">
  <li><a href="#a-2" id="toc-a-2" class="nav-link" data-scroll-target="#a-2"><span class="header-section-number">10.3.1</span> (a)</a></li>
  <li><a href="#b-2" id="toc-b-2" class="nav-link" data-scroll-target="#b-2"><span class="header-section-number">10.3.2</span> (b)</a></li>
  <li><a href="#c-2" id="toc-c-2" class="nav-link" data-scroll-target="#c-2"><span class="header-section-number">10.3.3</span> (c)</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Bias and Variance</span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta column-body">

    
  
    
  </div>
  


</header>


<section id="link-to-slides" class="level3" data-number="10.0.1">
<h3 data-number="10.0.1" class="anchored" data-anchor-id="link-to-slides"><span class="header-section-number">10.0.1</span> <a href="https://docs.google.com/presentation/d/1gUzOsBf1IXG5Bw0G7tZprEZxii_ceZKTGb-AOyGkgjQ/edit?usp=sharing">Link to Slides</a></h3>
</section>
<section id="bias-variance-tradeoff" class="level2" data-number="10.1">
<h2 data-number="10.1" class="anchored" data-anchor-id="bias-variance-tradeoff"><span class="header-section-number">10.1</span> Bias-Variance Tradeoff</h2>
<p>Your team would like to train a machine learning model to predict the next YouTube video a user will click on based on the videos the user has watched. We extract up to <span class="math inline">\(d\)</span> attributes (such as length of video, view count, etc.) from each video, and our model will be based on the previous <span class="math inline">\(m\)</span> videos watched by that user. Hence, the number of features for each data point for the model is <span class="math inline">\(m \times d\)</span>. Currently, you’re not sure how many videos to consider.</p>
<section id="a" class="level3" data-number="10.1.1">
<h3 data-number="10.1.1" class="anchored" data-anchor-id="a"><span class="header-section-number">10.1.1</span> (a)</h3>
<p>Your colleague, Lillian, generates the following plot, where the value <span class="math inline">\(d\)</span> on the <span class="math inline">\(x\)</span>-axis denotes the number of features used for a particular model. However, she forgot to label the <span class="math inline">\(y\)</span>-axis. Assume that the features are added to the model in decreasing levels of importance: More important features are first, and less important ones are after.</p>
<center>
<img src="images/bias_var_curve1.png" width="500">
</center>
<p>Which of the following could the <span class="math inline">\(y\)</span>-axis represent? Select all that apply.</p>
<p><span class="math inline">\(\Box\)</span> <strong>A</strong>. Training Error</p>
<p><span class="math inline">\(\Box\)</span> <strong>B.</strong> Validation error</p>
<p><span class="math inline">\(\Box\)</span> <strong>C.</strong> Bias</p>
<p><span class="math inline">\(\Box\)</span> <strong>D.</strong> Variance</p>
<details>
<summary>
<b>Answer</b>
</summary>
<p><strong>Training Error, Validaiton Error, Bias</strong></p>
<ul>
<li>Training Error: Can decrease as we add more features.</li>
<li>Validation Error: This can be true depending on the underlying complexity of the data.</li>
<li>Bias: Typically decreases with increasing model complexity.</li>
<li>Variance: Typically increases with increasing model complexity.</li>
</ul>
</details>
</section>
<section id="b" class="level3" data-number="10.1.2">
<h3 data-number="10.1.2" class="anchored" data-anchor-id="b"><span class="header-section-number">10.1.2</span> (b)</h3>
<p>Lillian generates the following plot, where the value <span class="math inline">\(d\)</span> is on the <span class="math inline">\(x\)</span>-axis. However, she forgot to label the <span class="math inline">\(y\)</span>-axis again.</p>
<center>
<img src="images/bias_var_curve3.png" width="500">
</center>
<p>Which of the following could the <span class="math inline">\(y\)</span>-axis represent? Select all that apply.</p>
<p><span class="math inline">\(\Box\)</span> <strong>A</strong>. Training Error</p>
<p><span class="math inline">\(\Box\)</span> <strong>B.</strong> Validation error</p>
<p><span class="math inline">\(\Box\)</span> <strong>C.</strong> Bias</p>
<p><span class="math inline">\(\Box\)</span> <strong>D.</strong> Variance</p>
<details>
<summary>
<b>Answer</b>
</summary>
<p><strong>Validation Error</strong></p>
<ul>
<li>Training Error: Cannot increase when increasing model complexity. If a feature increases training error, its weight will be set to 0.</li>
<li>Validation Error: Typically decreases up to a certain point, and then starts to increase as model becomes very complex.</li>
<li>Bias: Doesn’t increase with increasing model complexity.</li>
<li>Variance: Doesn’t decrease with increasing model complexity.</li>
</ul>
</details>
</section>
<section id="c" class="level3" data-number="10.1.3">
<h3 data-number="10.1.3" class="anchored" data-anchor-id="c"><span class="header-section-number">10.1.3</span> (c)</h3>
<p>Explain what happens to the error on the holdout set as we increase <span class="math inline">\(d\)</span>. Why?</p>
<details>
<summary>
<b>Answer</b>
</summary>
<p><em>Note</em>: The holdout set refers to a test set, not a validation set</p>
<p>For smaller <span class="math inline">\(d\)</span>, we are underfitting the training data since the model is less complex. The number of features we have does not allow us to fully understand the pattern and so our holdout error is high. As we increase <span class="math inline">\(d\)</span>, we have more features that allow us to pick up on some of the trends in the data and so the holdout error decreases a bit. However, there comes a point after which the features result in overfitting. Our model becomes more complex and does not seem to generalize as well to unseen data resulting in a high holdout error once again.</p>
</details>
</section>
</section>
<section id="bias-variance-tradeoff-1" class="level2" data-number="10.2">
<h2 data-number="10.2" class="anchored" data-anchor-id="bias-variance-tradeoff-1"><span class="header-section-number">10.2</span> Bias-Variance Tradeoff</h2>
<p>We randomly sample <span class="math inline">\(n\)</span> data points, <span class="math inline">\((x_i, y_i)\)</span>, and use them to fit a model <span class="math inline">\(f_{\hat\theta}(x)\)</span> according to some procedure (e.g.&nbsp;OLS, Ridge, LASSO). Then, we sample a new data point (independent of our existing points) from the same underlying data distribution. Furthermore, assume that we have a function <span class="math inline">\(g(x)\)</span>, which represents the ground truth and <span class="math inline">\(\epsilon\)</span>, which represents random noise. <span class="math inline">\(\epsilon\)</span> is produced by some noise generation process such that <span class="math inline">\(\mathbb{E}\left\lbrack\epsilon\right\rbrack = 0\)</span> and <span class="math inline">\(\text{Var}(\epsilon)=\sigma^2\)</span>. Whenever we query <span class="math inline">\(Y\)</span> at a given <span class="math inline">\(x\)</span>, we are given <span class="math inline">\(Y = g(x) + \epsilon\)</span>, a corrupted version of the real ground truth output. A new <span class="math inline">\(\epsilon\)</span> is generated each time, independent of the last. We showed in the lecture that:</p>
<p><span class="math display">\[\underbrace{\mathbb{E}[(Y - f_{\hat\theta}(x))^2}]_{} = \underbrace{\sigma^2}_{} + \underbrace{(g(x) - \mathbb{E}[f_{\hat\theta}(x)]})^2_{} + \underbrace{\mathbb{E}[(f_{\hat\theta}(x) - \mathbb{E}[f_{\hat\theta}(x)]_{})^2}]\]</span></p>
<section id="a-1" class="level3" data-number="10.2.1">
<h3 data-number="10.2.1" class="anchored" data-anchor-id="a-1"><span class="header-section-number">10.2.1</span> (a)</h3>
<p>Label each of the terms above using the following word bank. Not all words will be used.</p>
<ul>
<li>Observation variance</li>
<li>Model variance</li>
<li>(Observation Bias) <span class="math inline">\(^2\)</span></li>
<li>(Model Bias) <span class="math inline">\(^2\)</span></li>
<li>Model Risk</li>
<li>Empirical Mean Squared Error</li>
</ul>
<details>
<summary>
<b>Answer</b>
</summary>
<p><span class="math display">\[\underbrace{\mathbb{E}{[(Y - f_{\hat\theta}(x))^2}]}_{\text{model risk}}
   = \underbrace{\sigma^2}_{\text{observation variance}}
   + \underbrace{(g(x) - \mathbb{E}[f_{\hat\theta}(x)])^2}_{(\text{model bias})^2}
   + \underbrace{\mathbb{E}[{ (f_{\hat\theta}(x) - \mathbb{E}[{f_{\hat\theta}(x)}]_{})^2}]}_{\text{model variance}}\]</span></p>
</details>
</section>
<section id="b-1" class="level3" data-number="10.2.2">
<h3 data-number="10.2.2" class="anchored" data-anchor-id="b-1"><span class="header-section-number">10.2.2</span> (b)</h3>
<p>What quantities are random variables in the above equation? In our assumed data-generation process, where is the randomness in each variable coming from (i.e., which part of the assumed underlying model makes each random variable “random”)?</p>
<details>
<summary>
<b>Answer</b>
</summary>
<ul>
<li><span class="math inline">\(Y\)</span> - this is the new observation at <span class="math inline">\(x\)</span>. Its randomness comes from the random noise <span class="math inline">\(\epsilon\)</span>.</li>
<li><span class="math inline">\(f_{\hat\theta}\)</span> - this is the model fitted from the data. Its randomness comes from sampling and the random noise <span class="math inline">\(\epsilon\)</span>.</li>
<li><span class="math inline">\(\hat{\theta}\)</span> - these are the optimal theta parameters. Like the model itself, the randomness stems from the sampling and the random noise <span class="math inline">\(\epsilon\)</span>.</li>
</ul>
</details>
</section>
<section id="c-1" class="level3" data-number="10.2.3">
<h3 data-number="10.2.3" class="anchored" data-anchor-id="c-1"><span class="header-section-number">10.2.3</span> (c)</h3>
<p>Calculate the value of <span class="math inline">\(\mathbb{E}[\epsilon f_{\hat\theta}(x')]\)</span>, where <span class="math inline">\(f_{\hat{\theta}}(x')\)</span> is some predicted value of the response variable at some new fixed <span class="math inline">\(x'\)</span> using a model trained on a random sample, and <span class="math inline">\(\epsilon\)</span> is the observation error for a new data point at this fixed value of <span class="math inline">\(x'\)</span>.</p>
<details>
<summary>
<b>Answer</b>
</summary>
<p>Note that <span class="math inline">\(f_{\hat{\theta}}(x)\)</span> is a random variable whose randomness comes from <span class="math inline">\(\hat{\theta}\)</span> being random, which is itself random because of randomness in the training data. The data-generating mechanism for our new data point at <span class="math inline">\(x\)</span> is independent of all the other data used to train our model by assumption, so we have that the sources of randomness for <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(f_{\hat{\theta}}(x)\)</span> are independent. Since <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(\hat\theta\)</span> are independent, <span class="math display">\[\mathbb{E}[\epsilon f_{\hat\theta}(x)] = \mathbb{E}[\epsilon]\mathbb{E}[f_{\hat\theta}(x)]\]</span></p>
<p>And since <span class="math inline">\(\mathbb{E}[\epsilon] = 0\)</span> by the definition we have given. Therefore, <span class="math display">\[\mathbb{E}[\epsilon f_{\hat\theta}(x)] = \mathbb{E}[\epsilon]\mathbb{E}[f_{\hat\theta}(x)] = 0\]</span></p>
<p>Note that <span class="math inline">\(\epsilon\)</span> here refers to the noise associated with our new data point <span class="math inline">\(x'\)</span>, which is independently drawn (as stated at the start of the question). This can seem a bit confusing since we’ve previously used <span class="math inline">\(\epsilon\)</span> to refer to the noise associated with each of the data points, but it is used to simplify the notation. It might help to think of the noise associated with each of our points <span class="math inline">\(x_i\)</span> as being <span class="math inline">\(\epsilon_i\)</span> (which are i.i.d.), with the noise associated with <span class="math inline">\(x'\)</span> as being <span class="math inline">\(\epsilon'\)</span>. It then becomes clearer why <span class="math inline">\(\epsilon'\)</span> and <span class="math inline">\(f_{\hat\theta}(x)\)</span> are independent.</p>
</details>
</section>
</section>
<section id="regularization-and-bias-variance-tradeoff" class="level2" data-number="10.3">
<h2 data-number="10.3" class="anchored" data-anchor-id="regularization-and-bias-variance-tradeoff"><span class="header-section-number">10.3</span> Regularization and Bias-Variance Tradeoff</h2>
<p>We will use a simple constant model <span class="math inline">\(f_\theta(x) = \theta\)</span> to show the effects of regularization on bias and variance. For the sake of simplicity, we will assume that there is no noise or observational variance, so the ground truth output is equal to the observed outputs: <span class="math inline">\(Y = g(x)\)</span>.</p>
<section id="a-2" class="level3" data-number="10.3.1">
<h3 data-number="10.3.1" class="anchored" data-anchor-id="a-2"><span class="header-section-number">10.3.1</span> (a)</h3>
<p>Recall that the optimal solution for the constant model with an MSE loss and a dataset <span class="math inline">\(\mathcal{D}\)</span> with <span class="math inline">\(y_1, y_2, ..., y_n\)</span> is the mean <span class="math inline">\(\bar{y}\)</span>.</p>
<p>We use L2 regularization with a regularization penalty of <span class="math inline">\(\lambda &gt; 0\)</span> to train another constant model. Derive the optimal solution to this new constant model to minimize the objective function below.</p>
<p><span class="math display">\[
R(\theta) = \arg \min_\theta \left[ \left( \frac{1}{n} \sum_{i=1}^n (y_i - \theta)^2 \right)+ \lambda \theta^2 \right]
\]</span></p>
<p><strong>Note:</strong> As mentioned in the lecture, we do not impose a regularization penalty on the bias term and this problem only serves as a practice.</p>
<details>
<summary>
<b>Answer</b>
</summary>
<p>We use calculus to derive this result below:</p>
<p><span class="math display">\[\frac{dR}{d\theta} = \left( \frac{1}{n} \sum_i \frac{d}{d\theta} (y_i - \theta)^2 \right) + \lambda \frac{d}{d\theta}\theta^2 = \left( -\frac{2}{n} \sum_i (y_i - \theta) \right) + 2\lambda \theta\]</span></p>
<p>Set the derivative to 0, and solve for the optimal <span class="math inline">\(\hat{\theta}\)</span>.</p>
<p><span class="math display">\[ \left( -\frac{2}{n}\sum_i (y_i - \hat{\theta}) \right) + 2\lambda \hat{\theta} = 0 \implies \left( -\frac{1}{n}\sum_i y_i \right) + \hat{\theta} + \lambda \hat{\theta} = 0 \]</span></p>
<p>The optimal <span class="math inline">\(\hat{\theta}\)</span> is:</p>
<p><span class="math display">\[\frac{1}{n}\sum_i y_i = \hat{\theta} (1+\lambda)\]</span></p>
<p><span class="math display">\[
\hat{\theta} = \frac{1}{n(1 + \lambda)} \sum_i y_i = \frac{1}{1 + \lambda} \bar{y}
\]</span></p>
</details>
</section>
<section id="b-2" class="level3" data-number="10.3.2">
<h3 data-number="10.3.2" class="anchored" data-anchor-id="b-2"><span class="header-section-number">10.3.2</span> (b)</h3>
<p>Use the bias-variance decomposition to show that for a constant model <strong>with L2 regularization</strong>, its optimal expected loss on a sample test point <span class="math inline">\((x, Y)\)</span> in terms of the training data <span class="math inline">\(y\)</span> is equal to the following.</p>
<p><span class="math display">\[\mathbb{E}_\mathcal{D}[(Y - f_{\hat\theta}(x))^2] = (Y - \frac{1}{1 + \lambda} \mathbb{E}_\mathcal{D}[\bar{y}])^2 + \frac{1}{(1 + \lambda)^2} \text{Var}_\mathcal{D}(\bar{y})\]</span></p>
<p>What expected loss do we obtain when <span class="math inline">\(\lambda=0\)</span>, and what does that mean in terms of our model?</p>
<p><strong>Note:</strong> The subscript next to the expectation and variance lets you know what is random inside the expectation (i.e., what is the expectation taken over?). In this case, we calculate the expectation and variance of <span class="math inline">\(\bar{y}\)</span> across the dataset <span class="math inline">\(\mathcal{D}\)</span>.</p>
<details>
<summary>
<b>Answer</b>
</summary>
<p>Starting from the bias-variance decomposition presented in the lecture: <span class="math display">\[\mathbb{E}[(Y - f_{\hat\theta}(x))^2] = \sigma^2 + (g(x) - \mathbb{E}[f_{\hat\theta}(x)])^2 + \text{Var}(f_{\hat\theta}(x))\]</span></p>
<p>Recall that we are not dealing with any noise (<span class="math inline">\(\epsilon \sim \mathbb{N}(0, 0)\)</span>), so we can eliminate the <span class="math inline">\(\sigma^2\)</span> term. Then, <span class="math inline">\(g(x) = Y\)</span>, so we can replace <span class="math inline">\(g(x)\)</span> with <span class="math inline">\(Y\)</span>.</p>
<p><span class="math display">\[\mathbb{E}[(Y - f_{\hat\theta}(x))^2] = (Y - \mathbb{E}[f_{\hat\theta}(x)])^2 + \text{Var}(f_{\hat\theta}(x))\]</span></p>
<p>We can then derive <span class="math inline">\(f_{\hat\theta}(x)\)</span> using the assumptions of our constant model. We know that <span class="math inline">\(f_{\hat\theta}(x) = \frac{1}{1 + \lambda}\bar{y}\)</span>. We simplify the following expressions:</p>
<p><span class="math display">\[\mathbb{E}[(Y - f_{\hat\theta}(x))^2] = (Y - \mathbb{E}[\frac{1}{1 + \lambda}\bar{y}])^2 + \text{Var}(\frac{1}{1 + \lambda}\bar{y})\]</span></p>
<p>We can simplify:</p>
<p><span class="math display">\[\mathbb{E}[(Y - f_{\hat\theta}(x))^2] = (Y - \frac{1}{1 + \lambda} \mathbb{E}[\bar{y}])^2 + \frac{1}{(1 + \lambda)^2} \text{Var}(\bar{y})\]</span></p>
<p>Setting <span class="math inline">\(\lambda=0\)</span> is equivalent to using a constant model <strong>without</strong> L2 regularization, and we get the following equation for the expected loss:</p>
<p><span class="math display">\[\mathbb{E}[(Y - f_{\hat\theta}(x))^2] = (Y - \mathbb{E}[\bar{y}])^2 + \text{Var}(\bar{y})\]</span></p>
</details>
</section>
<section id="c-2" class="level3" data-number="10.3.3">
<h3 data-number="10.3.3" class="anchored" data-anchor-id="c-2"><span class="header-section-number">10.3.3</span> (c)</h3>
<p>Remark on how regularization has affected the model bias and variance as <span class="math inline">\(\lambda\)</span> increases. Consider what would happen to these quantities as <span class="math inline">\(\lambda \to \infty\)</span>.</p>
<details>
<summary>
<b>Answer</b>
</summary>
<p>The model bias has increased since the gap between a test point <span class="math inline">\(Y\)</span> and a scaled “best estimate” value is likely larger than between the test point and the actual best estimate. As <span class="math inline">\(\lambda \to \infty\)</span>, the model bias squared will tend towards <span class="math inline">\(Y^2\)</span>.</p>
<p>The model variance has decreased since <span class="math inline">\(\frac{1}{(1 + \lambda)^2}\)</span> if <span class="math inline">\(\lambda &gt; 0\)</span> will always be less than 1. Hence, our variance compared to the vanilla bias-variance decomposition from part (b) is reduced by a factor greater than 1. As <span class="math inline">\(\lambda \to \infty\)</span>, the model variance will become 0.</p>
</details>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../disc08/disc08.html" class="pagination-link" aria-label="Cross-Validation and Regularization">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Cross-Validation and Regularization</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> Bias and Variance</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="an">execute:</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co">  echo: true</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: true</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">    toc-title: Bias and Variance</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">    page-layout: full</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">    theme:</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">      - cosmo</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co">      - cerulean</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co">    callout-icon: false</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="fu">### [Link to Slides](https://docs.google.com/presentation/d/1gUzOsBf1IXG5Bw0G7tZprEZxii_ceZKTGb-AOyGkgjQ/edit?usp=sharing)</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="fu">## Bias-Variance Tradeoff</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>Your team would like to train a machine learning model to predict the next YouTube video a user will click on based on the videos the user has watched. We extract up to $d$ attributes (such as length of video, view count, etc.) from each video, and our model will be based on the previous $m$ videos watched by that user. Hence, the number of features for each data point for the model is $m \times d$. Currently, you're not sure how many videos to consider.</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="fu">### (a)</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>Your colleague, Lillian, generates the following plot, where the value $d$ on the $x$-axis denotes the number of features used for a particular model. However, she forgot to label the $y$-axis. Assume that the features are added to the model in decreasing levels of importance: More important features are first, and less important ones are after.</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>&lt;center&gt;</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    &lt;img src="images/bias_var_curve1.png" width = "500"&gt;</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>&lt;/center&gt;</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>Which of the following could the $y$-axis represent? Select all that apply.</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>$\Box$ **A**. Training Error</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>$\Box$ **B.** Validation error</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>$\Box$ **C.** Bias</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>$\Box$ **D.** Variance</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>&lt;details&gt;</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>&lt;summary&gt;&lt;b&gt;Answer&lt;/b&gt;&lt;/summary&gt;</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>**Training Error, Validaiton Error, Bias**</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Training Error: Can decrease as we add more features.</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Validation Error: This can be true depending on the underlying complexity of the data. </span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Bias: Typically decreases with increasing model complexity.</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Variance: Typically increases with increasing model complexity.</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>&lt;/details&gt;</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a><span class="fu">### (b)</span></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>Lillian generates the following plot, where the value $d$ is on the $x$-axis. However, she forgot to label the $y$-axis again.</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>&lt;center&gt;</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>    &lt;img src="images/bias_var_curve3.png" width = "500"&gt;</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>&lt;/center&gt;</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>Which of the following could the $y$-axis represent? Select all that apply.</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>$\Box$ **A**. Training Error</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>$\Box$ **B.** Validation error</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>$\Box$ **C.** Bias</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>$\Box$ **D.** Variance</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>&lt;details&gt;</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>&lt;summary&gt;&lt;b&gt;Answer&lt;/b&gt;&lt;/summary&gt;</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>**Validation Error**</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Training Error: Cannot increase when increasing model complexity. If a feature increases training error, its weight will be set to 0.</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Validation Error: Typically decreases up to a certain point, and then starts to increase as model becomes very complex.</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Bias: Doesn't increase with increasing model complexity.</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Variance: Doesn't decrease with increasing model complexity.</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>&lt;/details&gt;</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a><span class="fu">### (c)</span></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>Explain what happens to the error on the holdout set as we increase $d$. Why?</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>&lt;details&gt;</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>&lt;summary&gt;&lt;b&gt;Answer&lt;/b&gt;&lt;/summary&gt;</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>*Note*: The holdout set refers to a test set, not a validation set</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>For smaller $d$, we are underfitting the training data since the model is less complex. The number of features we have does not allow us to fully understand the pattern and so our holdout error is high. As we increase $d$, we have more features that allow us to pick up on some of the trends in the data and so the holdout error decreases a bit. However, there comes a point after which the features result in overfitting. Our model becomes more complex and does not seem to generalize as well to unseen data resulting in a high holdout error once again.</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>&lt;/details&gt;</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a><span class="fu">## Bias-Variance Tradeoff</span></span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a>We randomly sample $n$ data points, $(x_i, y_i)$, and use them to fit a model $f_{\hat\theta}(x)$ according to some procedure (e.g. OLS, Ridge, LASSO). Then, we sample a new data point (independent of our existing points) from the same underlying data distribution. Furthermore, assume that we have a function $g(x)$, which represents the ground truth and $\epsilon$, which represents random noise. $\epsilon$ is produced by some noise generation process such that $\mathbb{E}\left\lbrack\epsilon\right\rbrack = 0$ and $\text{Var}(\epsilon)=\sigma^2$. Whenever we query $Y$ at a given $x$, we are given $Y = g(x) + \epsilon$, a corrupted version of the real ground truth output. A new $\epsilon$ is generated each time, independent of the last. We showed in the lecture that:</span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>$$\underbrace{\mathbb{E}<span class="co">[</span><span class="ot">(Y - f_{\hat\theta}(x))^2}</span><span class="co">]</span>_{} = \underbrace{\sigma^2}_{} + \underbrace{(g(x) - \mathbb{E}[f_{\hat\theta}(x)]})^2_{} + \underbrace{\mathbb{E}[(f_{\hat\theta}(x) - \mathbb{E}[f_{\hat\theta}(x)]_{})^2}]$$</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a><span class="fu">### (a)</span></span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>Label each of the terms above using the following word bank. Not all words will be used.</span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Observation variance</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Model variance</span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>(Observation Bias) $^2$</span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>(Model Bias) $^2$</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Model Risk</span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Empirical Mean Squared Error</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a>&lt;details&gt;</span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a>&lt;summary&gt;&lt;b&gt;Answer&lt;/b&gt;&lt;/summary&gt;</span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a>$$\underbrace{\mathbb{E}{<span class="co">[</span><span class="ot">(Y - f_{\hat\theta}(x))^2}</span><span class="co">]</span>}_{\text{model risk}} </span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a>   = \underbrace{\sigma^2}_{\text{observation variance}} </span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a><span class="ss">   + </span>\underbrace{(g(x) - \mathbb{E}<span class="co">[</span><span class="ot">f_{\hat\theta}(x)</span><span class="co">]</span>)^2}_{(\text{model bias})^2} </span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a><span class="ss">   + </span>\underbrace{\mathbb{E}<span class="co">[</span><span class="ot">{ (f_{\hat\theta}(x) - \mathbb{E}[{f_{\hat\theta}(x)}]_{})^2}</span><span class="co">]</span>}_{\text{model variance}}$$</span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a>&lt;/details&gt;</span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a><span class="fu">### (b)</span></span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a>What quantities are random variables in the above equation? In our assumed data-generation process, where is the randomness in each variable coming from (i.e., which part of the assumed underlying model makes each random variable "random")?</span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a>&lt;details&gt;</span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a>&lt;summary&gt;&lt;b&gt;Answer&lt;/b&gt;&lt;/summary&gt;</span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$Y$ - this is the new observation at $x$. Its randomness comes from the random noise $\epsilon$.</span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$f_{\hat\theta}$ - this is the model fitted from the data. Its randomness comes from sampling and the random noise $\epsilon$.</span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$\hat{\theta}$ - these are the optimal theta parameters. Like the model itself, the randomness stems from the sampling and the random noise $\epsilon$.</span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a>&lt;/details&gt;</span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a><span class="fu">### (c)</span></span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a>Calculate the value of $\mathbb{E}<span class="co">[</span><span class="ot">\epsilon f_{\hat\theta}(x')</span><span class="co">]</span>$, where $f_{\hat{\theta}}(x')$ is some predicted value of the response variable at some new fixed $x'$ using a model trained on a random sample, and $\epsilon$ is the observation error for a new data point at this fixed value of $x'$.</span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a>&lt;details&gt;</span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a>&lt;summary&gt;&lt;b&gt;Answer&lt;/b&gt;&lt;/summary&gt;</span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a>Note that $f_{\hat{\theta}}(x)$ is a random variable whose randomness comes from $\hat{\theta}$ being random, which is itself random because of randomness in the training data. The data-generating mechanism for our new data point at $x$ is independent of all the other data used to train our model by assumption, so we have that the sources of randomness for $\epsilon$ and $f_{\hat{\theta}}(x)$ are independent. Since $\epsilon$ and $\hat\theta$ are independent, $$\mathbb{E}<span class="co">[</span><span class="ot">\epsilon f_{\hat\theta}(x)</span><span class="co">]</span> = \mathbb{E}<span class="co">[</span><span class="ot">\epsilon</span><span class="co">]</span>\mathbb{E}<span class="co">[</span><span class="ot">f_{\hat\theta}(x)</span><span class="co">]</span>$$</span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a>And since $\mathbb{E}<span class="co">[</span><span class="ot">\epsilon</span><span class="co">]</span> = 0$ by the definition we have given. Therefore, $$\mathbb{E}<span class="co">[</span><span class="ot">\epsilon f_{\hat\theta}(x)</span><span class="co">]</span> = \mathbb{E}<span class="co">[</span><span class="ot">\epsilon</span><span class="co">]</span>\mathbb{E}<span class="co">[</span><span class="ot">f_{\hat\theta}(x)</span><span class="co">]</span> = 0$$</span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a>Note that $\epsilon$ here refers to the noise associated with our new data point $x'$, which is independently drawn (as stated at the start of the question). This can seem a bit confusing since we've previously used $\epsilon$ to refer to the noise associated with each of the data points, but it is used to simplify the notation. It might help to think of the noise associated with each of our points $x_i$ as being $\epsilon_i$ (which are i.i.d.), with the noise associated with $x'$ as being $\epsilon'$. It then becomes clearer why $\epsilon'$ and $f_{\hat\theta}(x)$ are independent.</span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a>&lt;/details&gt;</span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a><span class="fu">## Regularization and Bias-Variance Tradeoff</span></span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a>We will use a simple constant model $f_\theta(x) = \theta$ to show the effects of regularization on bias and variance. For the sake of simplicity, we will assume that there is no noise or observational variance, so the ground truth output is equal to the observed outputs: $Y = g(x)$. </span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a><span class="fu">### (a) </span></span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a>Recall that the optimal solution for the constant model with an MSE loss and a dataset $\mathcal{D}$ with $y_1, y_2, ..., y_n$ is the mean $\bar{y}$. </span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a>We use L2 regularization with a regularization penalty of $\lambda &gt; 0$ to train another constant model. Derive the optimal solution to this new constant model \textbf{with L2 regularization} to minimize the objective function below.</span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a>R(\theta) = \arg \min_\theta \left<span class="co">[</span><span class="ot"> \left( \frac{1}{n} \sum_{i=1}^n (y_i - \theta)^2 \right)+ \lambda \theta^2 \right</span><span class="co">]</span></span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a>**Note:** As mentioned in the lecture, we do not impose a regularization penalty on the bias term and this problem only serves as a practice.</span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a>&lt;details&gt;</span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a>&lt;summary&gt;&lt;b&gt;Answer&lt;/b&gt;&lt;/summary&gt;</span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a>We use calculus to derive this result below:</span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a>$$\frac{dR}{d\theta} = \left( \frac{1}{n} \sum_i \frac{d}{d\theta} (y_i - \theta)^2 \right) + \lambda \frac{d}{d\theta}\theta^2 = \left( -\frac{2}{n} \sum_i (y_i - \theta) \right) + 2\lambda \theta$$</span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a>Set the derivative to 0, and solve for the optimal $\hat{\theta}$.</span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a>$$ \left( -\frac{2}{n}\sum_i (y_i - \hat{\theta}) \right) + 2\lambda \hat{\theta} = 0 \implies \left( -\frac{1}{n}\sum_i y_i \right) + \hat{\theta} + \lambda \hat{\theta} = 0 $$</span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a>The optimal $\hat{\theta}$ is:</span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a>$$\frac{1}{n}\sum_i y_i = \hat{\theta} (1+\lambda)$$</span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a>\hat{\theta} = \frac{1}{n(1 + \lambda)} \sum_i y_i = \frac{1}{1 + \lambda} \bar{y}</span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a>&lt;/details&gt;</span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a><span class="fu">### (b)</span></span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a>Use the bias-variance decomposition to show that for a constant model **with L2 regularization**, its optimal expected loss on a sample test point $(x, Y)$ in terms of the training data $y$ is equal to the following.</span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}_\mathcal{D}[(Y - f_{\hat\theta}(x))^2] = (Y - \frac{1}{1 + \lambda} \mathbb{E}_\mathcal{D}[\bar{y}])^2 + \frac{1}{(1 + \lambda)^2} \text{Var}_\mathcal{D}(\bar{y})$$</span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a>What expected loss do we obtain when $\lambda=0$, and what does that mean in terms of our model?</span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a>**Note:** The subscript next to the expectation and variance lets you know what is random inside the expectation (i.e., what is the expectation taken over?). In this case, we calculate the expectation and variance of $\bar{y}$ across the dataset $\mathcal{D}$.</span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a>&lt;details&gt;</span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a>&lt;summary&gt;&lt;b&gt;Answer&lt;/b&gt;&lt;/summary&gt;</span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a>Starting from the bias-variance decomposition presented in the lecture:</span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}<span class="co">[</span><span class="ot">(Y - f_{\hat\theta}(x))^2</span><span class="co">]</span> = \sigma^2 + (g(x) - \mathbb{E}<span class="co">[</span><span class="ot">f_{\hat\theta}(x)</span><span class="co">]</span>)^2 + \text{Var}(f_{\hat\theta}(x))$$</span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a>Recall that we are not dealing with any noise ($\epsilon \sim \mathbb{N}(0, 0)$), so we can eliminate the $\sigma^2$ term. Then, $g(x) = Y$, so we can replace $g(x)$ with $Y$.</span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}<span class="co">[</span><span class="ot">(Y - f_{\hat\theta}(x))^2</span><span class="co">]</span> = (Y - \mathbb{E}<span class="co">[</span><span class="ot">f_{\hat\theta}(x)</span><span class="co">]</span>)^2 + \text{Var}(f_{\hat\theta}(x))$$</span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a>We can then derive $f_{\hat\theta}(x)$ using the assumptions of our constant model. We know that $f_{\hat\theta}(x) = \frac{1}{1 + \lambda}\bar{y}$. We simplify the following expressions:</span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}<span class="co">[</span><span class="ot">(Y - f_{\hat\theta}(x))^2</span><span class="co">]</span> = (Y - \mathbb{E}<span class="co">[</span><span class="ot">\frac{1}{1 + \lambda}\bar{y}</span><span class="co">]</span>)^2 + \text{Var}(\frac{1}{1 + \lambda}\bar{y})$$</span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a>We can simplify:</span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}<span class="co">[</span><span class="ot">(Y - f_{\hat\theta}(x))^2</span><span class="co">]</span> = (Y - \frac{1}{1 + \lambda} \mathbb{E}<span class="co">[</span><span class="ot">\bar{y}</span><span class="co">]</span>)^2 + \frac{1}{(1 + \lambda)^2} \text{Var}(\bar{y})$$</span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a>Setting $\lambda=0$ is equivalent to using a constant model **without** L2 regularization, and we get the following equation for the expected loss:</span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}<span class="co">[</span><span class="ot">(Y - f_{\hat\theta}(x))^2</span><span class="co">]</span> = (Y - \mathbb{E}<span class="co">[</span><span class="ot">\bar{y}</span><span class="co">]</span>)^2 + \text{Var}(\bar{y})$$</span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a>&lt;/details&gt;</span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a><span class="fu">### (c)</span></span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a>Remark on how regularization has affected the model bias and variance as $\lambda$ increases. Consider what would happen to these quantities as $\lambda \to \infty$.</span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a>&lt;details&gt;</span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a>&lt;summary&gt;&lt;b&gt;Answer&lt;/b&gt;&lt;/summary&gt;</span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a>The model bias has increased since the gap between a test point $Y$ and a scaled "best estimate" value is likely larger than between the test point and the actual best estimate. As $\lambda \to \infty$, the model bias squared will tend towards $Y^2$.</span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a>The model variance has decreased since $\frac{1}{(1 + \lambda)^2}$ if $\lambda &gt; 0$ will always be less than 1. Hence, our variance compared to the vanilla bias-variance decomposition from part (b) is reduced by a factor greater than 1. As $\lambda \to \infty$, the model variance will become 0.</span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a>&lt;/details&gt;</span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>