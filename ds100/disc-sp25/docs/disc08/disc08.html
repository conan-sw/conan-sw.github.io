<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>9&nbsp; Cross-Validation and Regularization – Data 100 Discussion Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../disc07/disc07.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-ca5a086e270bb62b76934925835b48c3.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../disc08/disc08.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Cross-Validation and Regularization</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Data 100 Discussion Notes</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">disc-sp25</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../disc01/disc01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Math Prerequisites</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../disc02/disc02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Pandas I</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../disc03/disc03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Pandas II, EDA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../disc04/disc04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Regex, Visualization, and Transformation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../disc05/disc05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Transformations, Sampling, and SLR</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../disc06/disc06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Models, OLS</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../disc07/disc07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Gradient Descent, Feature Engineering, Housing</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../disc08/disc08.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Cross-Validation and Regularization</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Cross-Validation and Regularization</h2>
   
  <ul>
  <li><a href="#link-to-slides" id="toc-link-to-slides" class="nav-link active" data-scroll-target="#link-to-slides"><span class="header-section-number">9.0.1</span> Link to Slides</a></li>
  <li><a href="#cross-validation" id="toc-cross-validation" class="nav-link" data-scroll-target="#cross-validation"><span class="header-section-number">9.1</span> Cross Validation</a>
  <ul class="collapse">
  <li><a href="#a" id="toc-a" class="nav-link" data-scroll-target="#a"><span class="header-section-number">9.1.1</span> (a)</a></li>
  <li><a href="#b" id="toc-b" class="nav-link" data-scroll-target="#b"><span class="header-section-number">9.1.2</span> (b)</a></li>
  </ul></li>
  <li><a href="#ridge-and-lasso-regression" id="toc-ridge-and-lasso-regression" class="nav-link" data-scroll-target="#ridge-and-lasso-regression"><span class="header-section-number">9.2</span> Ridge and LASSO Regression</a>
  <ul class="collapse">
  <li><a href="#a-1" id="toc-a-1" class="nav-link" data-scroll-target="#a-1"><span class="header-section-number">9.2.1</span> (a)</a></li>
  <li><a href="#b-1" id="toc-b-1" class="nav-link" data-scroll-target="#b-1"><span class="header-section-number">9.2.2</span> (b)</a></li>
  <li><a href="#c" id="toc-c" class="nav-link" data-scroll-target="#c"><span class="header-section-number">9.2.3</span> (c)</a></li>
  <li><a href="#d" id="toc-d" class="nav-link" data-scroll-target="#d"><span class="header-section-number">9.2.4</span> (d)</a></li>
  <li><a href="#e" id="toc-e" class="nav-link" data-scroll-target="#e"><span class="header-section-number">9.2.5</span> (e)</a></li>
  </ul></li>
  <li><a href="#guessing-at-random" id="toc-guessing-at-random" class="nav-link" data-scroll-target="#guessing-at-random"><span class="header-section-number">9.3</span> Guessing at Random</a>
  <ul class="collapse">
  <li><a href="#a-2" id="toc-a-2" class="nav-link" data-scroll-target="#a-2"><span class="header-section-number">9.3.1</span> (a)</a></li>
  <li><a href="#b-2" id="toc-b-2" class="nav-link" data-scroll-target="#b-2"><span class="header-section-number">9.3.2</span> (b)</a></li>
  <li><a href="#c-1" id="toc-c-1" class="nav-link" data-scroll-target="#c-1"><span class="header-section-number">9.3.3</span> (c)</a></li>
  <li><a href="#d-1" id="toc-d-1" class="nav-link" data-scroll-target="#d-1"><span class="header-section-number">9.3.4</span> (d)</a></li>
  <li><a href="#e-1" id="toc-e-1" class="nav-link" data-scroll-target="#e-1"><span class="header-section-number">9.3.5</span> (e)</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Cross-Validation and Regularization</span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta column-body">

    
  
    
  </div>
  


</header>


<section id="link-to-slides" class="level3" data-number="9.0.1">
<h3 data-number="9.0.1" class="anchored" data-anchor-id="link-to-slides"><span class="header-section-number">9.0.1</span> <a href="https://docs.google.com/presentation/d/1F0IMB7RTXk-XbvDGfUlS13cwRPKp7pRtdv0HfZc2Irs/edit?usp=sharing">Link to Slides</a></h3>
</section>
<section id="cross-validation" class="level2" data-number="9.1">
<h2 data-number="9.1" class="anchored" data-anchor-id="cross-validation"><span class="header-section-number">9.1</span> Cross Validation</h2>
<section id="a" class="level3" data-number="9.1.1">
<h3 data-number="9.1.1" class="anchored" data-anchor-id="a"><span class="header-section-number">9.1.1</span> (a)</h3>
<p>After running <span class="math inline">\(5\)</span>-fold cross-validation, we get the following mean squared errors for each fold and value of <span class="math inline">\(\lambda\)</span> when using Ridge regularization:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Fold Num.</th>
<th style="text-align: center;"><span class="math inline">\(\lambda = 0.1\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\lambda = 0.2\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\lambda = 0.3\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\lambda = 0.4\)</span></th>
<th style="text-align: center;">Row Avg</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">1</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">70.2</td>
<td style="text-align: center;">91.2</td>
<td style="text-align: center;">91.8</td>
<td style="text-align: center;">83.4</td>
</tr>
<tr class="even">
<td style="text-align: left;">2</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">88.8</td>
<td style="text-align: center;">98.8</td>
<td style="text-align: center;">82.8</td>
</tr>
<tr class="odd">
<td style="text-align: left;">3</td>
<td style="text-align: center;">81.5</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">86.5</td>
<td style="text-align: center;">88.5</td>
<td style="text-align: center;">82.0</td>
</tr>
<tr class="even">
<td style="text-align: left;">4</td>
<td style="text-align: center;">79.4</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">92.3</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">83.1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">5</td>
<td style="text-align: center;">77.3</td>
<td style="text-align: center;">67.3</td>
<td style="text-align: center;">93.4</td>
<td style="text-align: center;">94.3</td>
<td style="text-align: center;">83.0</td>
</tr>
<tr class="even">
<td style="text-align: left;">Col Avg</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">90.4</td>
<td style="text-align: center;">93.2</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Suppose we wish to use the results of this 5-fold cross-validation to choose our hyperparameter <span class="math inline">\(\lambda\)</span>, among the following four choices in the table. Using the information in the table, which <span class="math inline">\(\lambda\)</span> would you choose? Why?</p>
<details>
<summary>
<b>Answer</b>
</summary>
<p>We should use <span class="math inline">\(\lambda = 0.2\)</span> because this value has the least average MSE across all folds.</p>
</details>
</section>
<section id="b" class="level3" data-number="9.1.2">
<h3 data-number="9.1.2" class="anchored" data-anchor-id="b"><span class="header-section-number">9.1.2</span> (b)</h3>
<p>You build a model with two hyperparameters, the coefficient for the regularization term (<span class="math inline">\(\lambda\)</span>) and our learning rate (<span class="math inline">\(\alpha\)</span>). You have 4 good candidate values for <span class="math inline">\(\lambda\)</span> and 3 possible values for <span class="math inline">\(\alpha\)</span>, and you are wondering which <span class="math inline">\(\lambda,
\alpha\)</span> pair will be the best choice. If you were to perform 5-fold cross-validation, how many validation errors would you need to calculate?</p>
<details>
<summary>
<b>Answer</b>
</summary>
<p>There are <span class="math inline">\(4 \times 3 = 12\)</span> pairs of <span class="math inline">\(\lambda, \alpha\)</span>, and each pair will have <span class="math inline">\(5\)</span> validation errors, one for each fold. So, there would be 60 validation errors in total.</p>
</details>
</section>
</section>
<section id="ridge-and-lasso-regression" class="level2" data-number="9.2">
<h2 data-number="9.2" class="anchored" data-anchor-id="ridge-and-lasso-regression"><span class="header-section-number">9.2</span> Ridge and LASSO Regression</h2>
<p>The goal of linear regression is to find the <span class="math inline">\(\theta\)</span> value that minimizes the average squared loss. In other words, we want to find <span class="math inline">\(\hat{\theta}\)</span> that satisfies the equation below:</p>
<p><span class="math display">\[\hat{\theta}
= \underset{\theta}{\operatorname{argmin}} L(\theta)
= \underset{\theta}{\operatorname{argmin}} \dfrac{1}{n}||\mathbb{Y} - \mathbb{X}{\theta}||_2^2\]</span></p>
<p>Here, <span class="math inline">\(\Bbb{X}\)</span> is a <span class="math inline">\(n \times (p + 1)\)</span> matrix, <span class="math inline">\(\theta\)</span> is a <span class="math inline">\((p + 1) \times 1\)</span> vector and <span class="math inline">\(\mathbb{Y}\)</span> is a <span class="math inline">\(n \times 1\)</span> vector. Recall that the extra <span class="math inline">\(1\)</span> in <span class="math inline">\((p+1)\)</span> comes from the intercept term. As we saw in lecture, the optimal <span class="math inline">\(\hat{\theta}\)</span> is given by the closed-form expression <span class="math inline">\(\hat{\theta} = (\Bbb{X}^T\Bbb{X})^{-1}\Bbb{X}^T \mathbb{Y}\)</span>.</p>
<p>To prevent overfitting, we saw that we can instead minimize the sum of the average squared loss plus a regularization term <span class="math inline">\(\lambda g(\theta)\)</span>. The optimization problem for such a loss function then becomes:</p>
<p><span class="math display">\[\begin{align*}
    \hat{\theta} = \underset{\theta}{\operatorname{argmin}} L(\theta) = \underset{\theta}{\operatorname{argmin}} \left[\frac{1}{n} \|\mathbb{Y} - \mathbb{X}\theta\|_{2}^{2} + \lambda g(\theta) \right]
\end{align*}\]</span></p>
<ul>
<li>If we use the function <span class="math inline">\(g(\theta) = \sum_{j=1}^p\theta_j^2 = ||{\theta}||_2^2\)</span>, we have “Ridge regression”. Recall that <span class="math inline">\(g\)</span> is the <span class="math inline">\(\ell_2\)</span> norm of <span class="math inline">\(\theta\)</span>, so this is also referred to as “<span class="math inline">\(\ell_2 / L_2\)</span> regularization”.</li>
<li>If we use the function <span class="math inline">\(g(\theta) = \sum_{j=1}^p |\theta_j| = ||{\theta}||_1\)</span>, we have “LASSO regression”. Recall that <span class="math inline">\(g\)</span> is the <span class="math inline">\(\ell_1\)</span> norm of <span class="math inline">\(\theta\)</span>, so this is also referred to as “<span class="math inline">\(\ell_1 / L_1\)</span> regularization”.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ridge_lasso.gif" class="img-fluid figure-img"></p>
<figcaption>ridge_lasso</figcaption>
</figure>
</div>
<p><strong>In this question, we intentionally choose to regularize also on the intercept term</strong> to simplify the mathematical formulation of the Ridge and LASSO regression. In practice, we would not actually want to regularize the intercept term (and you should always assume that there should not be a regularization on the intercept term).</p>
<p>For example, if we choose <span class="math inline">\(g(\theta) = ||{\theta}||_2^2\)</span>, our goal is to find <span class="math inline">\(\hat{\theta}\)</span> that satisfies the equation below:</p>
<p><span class="math display">\[\begin{align*}
\hat\theta
= \underset{\theta}{\operatorname{argmin}} L_2(\theta)
&amp;= \underset{\theta}{\operatorname{argmin}} \left[ \dfrac{1}{n}||\mathbb{Y} - \Bbb{X}{\theta}||_2^2 + \lambda ||{\theta}||_2^2
\right] \\
&amp;= \underset{\theta}{\operatorname{argmin}} \left[ \dfrac{1}{n}\sum_{i=1}^n (y_i - \Bbb{X}_{i,\cdot}^T \theta) ^2 + \lambda \sum_{j=0}^d\theta_j^2 \right]
\end{align*}\]</span></p>
<p>Recall that <span class="math inline">\(\lambda\)</span> is a hyperparameter that determines the impact of the regularization term. Like ordinary least squares, we can also find a closed-form solution to Ridge regression: <span class="math inline">\(\hat{\theta}=(\Bbb{X}^T\Bbb{X} + n \lambda \mathbf{I})^{-1} \Bbb{X}^T \mathbb{Y}\)</span>. For LASSO regression, there is no such closed-form expression.</p>
<section id="a-1" class="level3" data-number="9.2.1">
<h3 data-number="9.2.1" class="anchored" data-anchor-id="a-1"><span class="header-section-number">9.2.1</span> (a)</h3>
<p>Suppose we are dealing with the OLS case (i.e., don’t worry about regularization yet). We increase the complexity of the model until test error stops decreasing. If we continue to increase model complexity, what do we expect to happen to the training error of the model trained using OLS? What about the test error?</p>
<p><span class="math inline">\(\Box\)</span> Training error decreases</p>
<p><span class="math inline">\(\Box\)</span> Training error increases</p>
<p><span class="math inline">\(\Box\)</span> Test error decreases</p>
<p><span class="math inline">\(\Box\)</span> Test error increases</p>
<details>
<summary>
<b>Answer</b>
</summary>
<p><strong>Training error decreases, Test error increases</strong></p>
<p>The training error decreases since the model fits/recognizes more relationships between features and responses found in the training dataset. However, these relationships increasingly become specific to the training set and will not necessarily generalize to the test set, so we expect the test error to increase.</p>
<p><em>Note:</em> The above is what we expect to happen, but there may be <em>rare cases</em> where this might not be true.</p>
</details>
</section>
<section id="b-1" class="level3" data-number="9.2.2">
<h3 data-number="9.2.2" class="anchored" data-anchor-id="b-1"><span class="header-section-number">9.2.2</span> (b)</h3>
<p>Now suppose we choose one of the above regularization methods, either <span class="math inline">\(L1\)</span> or <span class="math inline">\(L2\)</span>, for some regularization parameter <span class="math inline">\(\lambda &gt; 0\)</span> then we solve for our optimum. In terms of variance, how does a regularized model compare to ordinary least squares regression (assuming the same features between both models)?</p>
<details>
<summary>
<b>Answer</b>
</summary>
<p>Regularized regression has a <strong>lower</strong> variance relative to ordinary least squares regression. This is because regularization tends to make the model “simpler” (pushing the vector of regression coefficients to be in some ball around the origin). So, upon slight changes in input variables, our predictions will vary less under regularization than under no regularization.</p>
</details>
</section>
<section id="c" class="level3" data-number="9.2.3">
<h3 data-number="9.2.3" class="anchored" data-anchor-id="c"><span class="header-section-number">9.2.3</span> (c)</h3>
<p>Suppose we have a large number of features (10,000+), and we suspect that only a handful of features are useful. Would LASSO or Ridge regression be more helpful in interpreting useful features? Why?</p>
<details>
<summary>
<b>Answer</b>
</summary>
<p>LASSO would be better as it sets many values to 0, so it would be effectively selecting useful features and “ignoring” less useful ones.</p>
<p>You can see this behavior in the GIF above with two parameters!</p>
</details>
</section>
<section id="d" class="level3" data-number="9.2.4">
<h3 data-number="9.2.4" class="anchored" data-anchor-id="d"><span class="header-section-number">9.2.4</span> (d)</h3>
<p>What are the two benefits of using Ridge regression over OLS?</p>
<details>
<summary>
<b>Answer</b>
</summary>
<ol type="1">
<li><p>If <span class="math inline">\(\mathbb{X}^T\mathbb{X}\)</span> is not full rank (not invertible), then we end up with infinitely many solutions for least squares. On the other hand, using Ridge regression guarantees invertibility of <span class="math inline">\((\mathbb{X}^T\mathbb{X} + n \lambda \mathbb{I})\)</span> and ensures that <span class="math inline">\(\hat\theta = (\mathbb{X}^T\mathbb{X} + n \lambda \mathbb{I})^{-1}\mathbb{X}^T\mathbb{Y}\)</span> always has a unique solution when <span class="math inline">\(\lambda &gt; 0\)</span>; the proof for these facts is out of scope for Data 100.</p></li>
<li><p>Ridge regression also allows for feature selection/reducing overfitting because it down weights features that are less important in predicting the response. However, it still stands that LASSO is normally better for feature selection since LASSO will actually set these unimportant coefficients to <span class="math inline">\(0\)</span> as opposed to just down-weighting them.</p></li>
</ol>
</details>
</section>
<section id="e" class="level3" data-number="9.2.5">
<h3 data-number="9.2.5" class="anchored" data-anchor-id="e"><span class="header-section-number">9.2.5</span> (e)</h3>
<p>In Ridge regression, what happens to <span class="math inline">\(\hat{\theta}\)</span> if we set <span class="math inline">\(\lambda = 0\)</span>? What happens as <span class="math inline">\(\lambda\)</span> approaches <span class="math inline">\(\infty\)</span>?</p>
<details>
<summary>
<b>Answer</b>
</summary>
<ul>
<li><p><span class="math inline">\(\lambda = 0\)</span>: <span class="math display">\[\hat{\theta}=(\Bbb{X}^T\Bbb{X} + n \lambda \mathbf{I})^{-1} \Bbb{X}^T \mathbb{Y} =(\Bbb{X}^T\Bbb{X})^{-1} \Bbb{X}^T \mathbb{Y}\]</span> Which is the normal OLS solution.</p></li>
<li><p><span class="math inline">\(\lambda \rightarrow \infty\)</span>: <span class="math display">\[ (\Bbb{X}^T\Bbb{X} + n \lambda \mathbf{I})^{-1} \rightarrow \vec{0} \text{  as  } \lambda \rightarrow \infty \]</span> Therefore, <span class="math inline">\(\hat{\theta} \rightarrow \vec{0}\)</span> as well</p>
<ul>
<li>Intuitively: As <span class="math inline">\(\lambda \rightarrow \infty\)</span>, the penalty term will dominat ethe least-squares term. After a certain point, it will be more optimal to set <span class="math inline">\(\hat{\theta} = 0\)</span> and simply incur the loss of a constant model.</li>
</ul></li>
</ul>
</details>
</section>
</section>
<section id="guessing-at-random" class="level2" data-number="9.3">
<h2 data-number="9.3" class="anchored" data-anchor-id="guessing-at-random"><span class="header-section-number">9.3</span> Guessing at Random</h2>
<p>A multiple choice test has 100 questions, each with five answer choices. Assume for each question that there is only one correct choice. The grading scheme is as follows:</p>
<ul>
<li>4 points are awarded for each right answer.</li>
<li>For each other answer (wrong, missing, etc), one point is taken off; that is, -1 point is awarded.</li>
</ul>
<p>A student hasn’t studied at all and therefore selects each question’s answer uniformly at random, independently of all the other questions.</p>
<p>Define the following random variables:</p>
<ul>
<li><span class="math inline">\(R\)</span>: The number of answers the student gets right.</li>
<li><span class="math inline">\(W\)</span>: The number of answers the student does not get right.</li>
<li><span class="math inline">\(S\)</span>: The student’s score on the test.</li>
</ul>
<section id="a-2" class="level3" data-number="9.3.1">
<h3 data-number="9.3.1" class="anchored" data-anchor-id="a-2"><span class="header-section-number">9.3.1</span> (a)</h3>
<p>What is the distribution of <span class="math inline">\(R\)</span>? Provide the name and parameters of the appropriate distribution. Explain your answer.</p>
<details>
<summary>
<b>Answer</b>
</summary>
<p><span class="math inline">\(R\)</span> is counting the number of “successes” (or 1s) out of <span class="math inline">\(100\)</span> total independent Bernoulli trials, where a “success” is defined as answering the question correctly, and each question is a trial. The trials are independent because the student selects a random answer with the same probability distribution, no matter whether the other answers are chosen. The probability of “success” on any single trial is <span class="math inline">\(1/5 = 0.2\)</span>, so, <span class="math inline">\(R\)</span> must follow a binomial distribution with <span class="math inline">\(n = 100\)</span> and <span class="math inline">\(p = 0.2\)</span>.</p>
</details>
</section>
<section id="b-2" class="level3" data-number="9.3.2">
<h3 data-number="9.3.2" class="anchored" data-anchor-id="b-2"><span class="header-section-number">9.3.2</span> (b)</h3>
<p>Find <span class="math inline">\(\mathbb{E}[R]\)</span></p>
<details>
<summary>
<b>Answer</b>
</summary>
<p>From class, the expectation of a <span class="math inline">\(\text{Binomial}(n,p)\)</span> random variable is always <span class="math inline">\(np\)</span>. So, we obtain: <span class="math display">\[\mathbb{E}[R] = n \cdot p = 100 \cdot 0.2 = 20\]</span></p>
</details>
</section>
<section id="c-1" class="level3" data-number="9.3.3">
<h3 data-number="9.3.3" class="anchored" data-anchor-id="c-1"><span class="header-section-number">9.3.3</span> (c)</h3>
<p>True or False: <span class="math inline">\(\text{SD}(R) = \text{SD}(W)\)</span>? Remember that <span class="math inline">\(\text{Var}(X) = \text{SD}(X)^2\)</span>.</p>
<details>
<summary>
<b>Answer</b>
</summary>
<p>True. Note that <span class="math inline">\(R + W = 100\)</span>. Hence, <span class="math display">\[\begin{align*}
    \text{Var}(R) &amp;= \text{Var}(100 - W) \\
     &amp;= (-1)^2\text{Var}(W)\\
     &amp;= \text{Var}(W)
\end{align*}\]</span></p>
<p>We use the non-linearity of variance, <span class="math inline">\(\text{Var}(aX+b) = a^2\text{Var}(X)\)</span>, to simplify our expression.</p>
</details>
</section>
<section id="d-1" class="level3" data-number="9.3.4">
<h3 data-number="9.3.4" class="anchored" data-anchor-id="d-1"><span class="header-section-number">9.3.4</span> (d)</h3>
<p>Find <span class="math inline">\(\mathbb{E}[S]\)</span>, the student’s expected score on the test.</p>
<details>
<summary>
<b>Answer</b>
</summary>
<p>The student’s score on the test is a function of how many they get correct and how many they get incorrect. Using the point scheme given in the question, we can write this score as <span class="math inline">\(S = 4R - W\)</span> since each correct answer is awarded <span class="math inline">\(4\)</span> points, and each wrong answer is penalized by <span class="math inline">\(1\)</span> point. Note that <span class="math inline">\(S\)</span> is also a random variable since it is a function of random variables <span class="math inline">\(R\)</span> and <span class="math inline">\(W\)</span>. Note that <span class="math inline">\(R + W = 100\)</span>, since there are <span class="math inline">\(100\)</span> questions. Substituting <span class="math inline">\(W = 100 - R\)</span> and using linearity of expectations, we see:</p>
<p><span class="math display">\[\begin{align*}
    \mathbb{E}[S] &amp;= \mathbb{E}[4R - W] \\
    &amp;= \mathbb{E}[4R - 100 + R] \\
    &amp;= \mathbb{E}[5R - 100] \\
    &amp;= 5\mathbb{E}[R] - 100 \\
\end{align*}\]</span></p>
<p>Substituting <span class="math inline">\(\mathbb{E}[R] = 20\)</span> from part (b), we see the students expected score on the exam using this guessing strategy is <span class="math inline">\(0\)</span>.</p>
</details>
</section>
<section id="e-1" class="level3" data-number="9.3.5">
<h3 data-number="9.3.5" class="anchored" data-anchor-id="e-1"><span class="header-section-number">9.3.5</span> (e)</h3>
<p>Find <span class="math inline">\(\text{SD}(S)\)</span></p>
<details>
<summary>
<b>Answer</b>
</summary>
<p>We know from the question above that we can write <span class="math inline">\(4R - W\)</span> as <span class="math inline">\(5R - 100\)</span>. Since the variance of a random variable plus a constant is just the variance of the original random variable:</p>
<p><span class="math display">\[\begin{align*}
    \text{Var}(S) &amp;= \text{Var}(5R - 100) \\
    &amp;= 5^{2}\text{Var}(R) \\
    &amp;= 25\text{Var}(R)
\end{align*}\]</span></p>
<p>We know that the variance of a <span class="math inline">\(\text{Binomial}(n,p)\)</span> variable is <span class="math inline">\(np(1-p)\)</span>. Plugging in the values of <span class="math inline">\(n, p\)</span> from part (a), we see <span class="math inline">\(\text{Var}(R) = 16\)</span>, giving us <span class="math inline">\(\text{Var}(S) = 400\)</span>. Hence, <span class="math inline">\(SD(S) = \sqrt{400} = 20\)</span>.</p>
</details>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../disc07/disc07.html" class="pagination-link" aria-label="Gradient Descent, Feature Engineering, Housing">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Gradient Descent, Feature Engineering, Housing</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> Cross-Validation and Regularization</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="an">execute:</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co">  echo: true</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: true</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">    toc-title: Cross-Validation and Regularization</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">    page-layout: full</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">    theme:</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">      - cosmo</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co">      - cerulean</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co">    callout-icon: false</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="fu">### [Link to Slides](https://docs.google.com/presentation/d/1F0IMB7RTXk-XbvDGfUlS13cwRPKp7pRtdv0HfZc2Irs/edit?usp=sharing)</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="fu">## Cross Validation</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="fu">### (a)</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>After running $5$-fold cross-validation, we get the following mean squared errors for each fold and value of $\lambda$ when using Ridge regularization:</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>| Fold Num. | $\lambda = 0.1$ | $\lambda = 0.2$ | $\lambda = 0.3$ | $\lambda = 0.4$ | Row Avg |</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>| :-- | :-: | :-: | :-: | :-: | :-: |</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>| 1 | 80.2 | 70.2 | 91.2 | 91.8 | 83.4 |</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>| 2 | 76.8 | 66.8 | 88.8 | 98.8 | 82.8 |</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>| 3 | 81.5 | 71.5 | 86.5 | 88.5 | 82.0 |</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>| 4 | 79.4 | 68.4 | 92.3 | 92.4 | 83.1 |</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>| 5 | 77.3 | 67.3 | 93.4 | 94.3 | 83.0 |</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>| Col Avg | 79.0 | 68.8 | 90.4 | 93.2 | |</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>Suppose we wish to use the results of this 5-fold cross-validation to choose our hyperparameter $\lambda$, among the following four choices in the table. Using the information in the table, which $\lambda$ would you choose? Why?</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>&lt;details&gt;</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>&lt;summary&gt;&lt;b&gt;Answer&lt;/b&gt;&lt;/summary&gt;</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>We should use $\lambda = 0.2$ because this value has the least average MSE across all folds.</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>&lt;/details&gt;</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a><span class="fu">### (b)</span></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>You build a model with two hyperparameters, the coefficient for the regularization term ($\lambda$) and our learning rate ($\alpha$). You have 4 good candidate values for $\lambda$ and 3 possible values for $\alpha$, and you are wondering which $\lambda,</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>\alpha$ pair will be the best choice. If you were to perform 5-fold cross-validation, how many validation</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a> errors would you need to calculate?</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a> &lt;details&gt;</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a> &lt;summary&gt;&lt;b&gt;Answer&lt;/b&gt;&lt;/summary&gt;</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>There are $4 \times 3 = 12$ pairs of $\lambda, \alpha$, and each pair will have $5$ validation errors, one for each fold. So, there would be 60 validation errors in total.</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a> &lt;/details&gt;</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a><span class="fu">## Ridge and LASSO Regression</span></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>The goal of linear regression is to find the $\theta$ value that minimizes the average squared loss. In other words, we want to find $\hat{\theta}$ that satisfies the equation below:</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>$$\hat{\theta}</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>= \underset{\theta}{\operatorname{argmin}} L(\theta) </span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>= \underset{\theta}{\operatorname{argmin}} \dfrac{1}{n}||\mathbb{Y} - \mathbb{X}{\theta}||_2^2$$</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>Here, $\Bbb{X}$ is a $n \times (p + 1)$ matrix, $\theta$ is a $(p + 1) \times 1$ vector and $\mathbb{Y}$ is a $n \times 1$ vector. Recall that the extra $1$ in $(p+1)$ comes from the intercept term. As we saw in lecture, the optimal $\hat{\theta}$ is given by the closed-form expression $\hat{\theta} = (\Bbb{X}^T\Bbb{X})^{-1}\Bbb{X}^T \mathbb{Y}$.</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>To prevent overfitting, we saw that we can instead minimize the sum of the average squared loss plus a regularization term $\lambda g(\theta)$.</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>The optimization problem for such a \textit{regularized} loss function then becomes:</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>    \hat{\theta} = \underset{\theta}{\operatorname{argmin}} L(\theta) = \underset{\theta}{\operatorname{argmin}} \left<span class="co">[</span><span class="ot">\frac{1}{n} \|\mathbb{Y} - \mathbb{X}\theta\|_{2}^{2} + \lambda g(\theta) \right</span><span class="co">]</span></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>If we use the function $g(\theta) = \sum_{j=1}^p\theta_j^2 = ||{\theta}||_2^2$, we have "Ridge regression". Recall that $g$ is the $\ell_2$ norm of $\theta$, so this is also referred to as "$\ell_2 / L_2$ regularization".</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>If we use the function $g(\theta) = \sum_{j=1}^p |\theta_j| = ||{\theta}||_1$, we have "LASSO regression". Recall that $g$ is the $\ell_1$ norm of $\theta$, so this is also referred to as "$\ell_1 / L_1$ regularization".</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a><span class="al">![ridge_lasso](images/ridge_lasso.gif)</span></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>**In this question, we intentionally choose to regularize also on the intercept term** to simplify the mathematical formulation of the Ridge and LASSO regression. In practice, we would not actually want to regularize the intercept term (and you should always assume that there should not be a regularization on the intercept term).</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>For example, if we choose  $g(\theta) = ||{\theta}||_2^2$, our goal is to find $\hat{\theta}$ that satisfies the equation below:</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>\hat\theta</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>= \underset{\theta}{\operatorname{argmin}} L_2(\theta) </span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>&amp;= \underset{\theta}{\operatorname{argmin}} \left[ \dfrac{1}{n}||\mathbb{Y} - \Bbb{X}{\theta}||_2^2 + \lambda ||{\theta}||_2^2 </span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>\right] <span class="sc">\\</span></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>&amp;= \underset{\theta}{\operatorname{argmin}} \left<span class="co">[</span><span class="ot"> \dfrac{1}{n}\sum_{i=1}^n (y_i - \Bbb{X}_{i,\cdot}^T \theta) ^2 + \lambda \sum_{j=0}^d\theta_j^2 \right</span><span class="co">]</span></span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>Recall that $\lambda$ is a hyperparameter that determines the impact of the regularization term. Like ordinary least squares, we can also find a closed-form solution to Ridge regression: $\hat{\theta}=(\Bbb{X}^T\Bbb{X} + n \lambda \mathbf{I})^{-1} \Bbb{X}^T \mathbb{Y}$. For LASSO regression, there is no such closed-form expression.</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a><span class="fu">### (a)</span></span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>Suppose we are dealing with the OLS case (i.e., don't worry about regularization yet). We increase the complexity of the model until test error stops decreasing. If we continue to increase model complexity, what do we expect to happen to the training error of the model trained using OLS? What about the test error?</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>$\Box$ Training error decreases</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>$\Box$ Training error increases</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>$\Box$ Test error decreases</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>$\Box$ Test error increases</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>&lt;details&gt;</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>&lt;summary&gt;&lt;b&gt;Answer&lt;/b&gt;&lt;/summary&gt;</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>**Training error decreases, Test error increases**</span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>The training error decreases since the model fits/recognizes more relationships between features and responses found in the training dataset. However, these relationships increasingly become specific to the training set and will not necessarily generalize to the test set, so we expect the test error to increase.</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a>*Note:* The above is what we expect to happen, but there may be *rare cases* where this might not be true.</span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a>&lt;/details&gt;</span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a><span class="fu">### (b)</span></span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a>Now suppose we choose one of the above regularization methods, either $L1$ or $L2$, for some regularization parameter $\lambda &gt; 0$ then we solve for our optimum. In terms of variance, how does a regularized model compare to ordinary least squares regression (assuming the same features between both models)?</span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a>&lt;details&gt;</span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a>&lt;summary&gt;&lt;b&gt;Answer&lt;/b&gt;&lt;/summary&gt;</span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a>Regularized regression has a **lower** variance relative to ordinary least squares regression. This is because regularization tends to make the model "simpler" (pushing the vector of regression coefficients to be in some ball around the origin). So, upon slight changes in input variables, our predictions will vary less under regularization than under no regularization.</span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a>&lt;/details&gt;</span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a><span class="fu">### (c)</span></span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a>Suppose we have a large number of features (10,000+), and we suspect that only a handful of features are useful. Would LASSO or Ridge regression be more helpful in interpreting useful features? Why?</span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a>&lt;details&gt;</span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a>&lt;summary&gt;&lt;b&gt;Answer&lt;/b&gt;&lt;/summary&gt;</span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a>LASSO would be better as it sets many values to 0, so it would be effectively selecting useful features and "ignoring" less useful ones.</span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a>You can see this behavior in the GIF above with two parameters!</span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a>&lt;/details&gt;</span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a><span class="fu">### (d)</span></span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a>What are the two benefits of using Ridge regression over OLS?</span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>&lt;details&gt;</span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a>&lt;summary&gt;&lt;b&gt;Answer&lt;/b&gt;&lt;/summary&gt;</span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>If $\mathbb{X}^T\mathbb{X}$ is not full rank (not invertible), then we end up with infinitely many solutions for least squares. On the other hand, using Ridge regression guarantees invertibility of $(\mathbb{X}^T\mathbb{X} + n \lambda \mathbb{I})$ and ensures that $\hat\theta = (\mathbb{X}^T\mathbb{X} + n \lambda \mathbb{I})^{-1}\mathbb{X}^T\mathbb{Y}$ always has a unique solution when $\lambda &gt; 0$; the proof for these facts is out of scope for Data 100.</span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Ridge regression also allows for feature selection/reducing overfitting because it down weights features that are less important in predicting the response. However, it still stands that LASSO is normally better for feature selection since LASSO will actually set these unimportant coefficients to $0$ as opposed to just down-weighting them.</span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a>&lt;/details&gt;</span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a><span class="fu">### (e)</span></span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a>In Ridge regression, what happens to $\hat{\theta}$ if we set $\lambda = 0$? What happens as $\lambda$ approaches $\infty$?</span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a>&lt;details&gt;</span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a>&lt;summary&gt;&lt;b&gt;Answer&lt;/b&gt;&lt;/summary&gt;</span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$\lambda = 0$: $$\hat{\theta}=(\Bbb{X}^T\Bbb{X} + n \lambda \mathbf{I})^{-1} \Bbb{X}^T \mathbb{Y} =(\Bbb{X}^T\Bbb{X})^{-1} \Bbb{X}^T \mathbb{Y}$$ Which is the normal OLS solution.</span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$\lambda \rightarrow \infty$: $$ (\Bbb{X}^T\Bbb{X} + n \lambda \mathbf{I})^{-1} \rightarrow \vec{0} \text{  as  } \lambda \rightarrow \infty $$ Therefore, $\hat{\theta} \rightarrow \vec{0}$ as well</span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a><span class="ss">    * </span>Intuitively: As $\lambda \rightarrow \infty$, the penalty term will dominat ethe least-squares term. After a certain point, it will be more optimal to set $\hat{\theta} = 0$ and simply incur the loss of a constant model.</span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a>&lt;/details&gt;</span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a><span class="fu">## Guessing at Random</span></span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a>A multiple choice test has 100 questions, each with five answer choices. Assume for each question that there is only one correct choice. The grading scheme is as follows:</span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>4 points are awarded for each right answer.</span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>For each other answer (wrong, missing, etc), one point is taken off; that is, -1 point is awarded.</span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a>A student hasn't studied at all and therefore selects each question's answer uniformly at random, independently of all the other questions. </span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a>Define the following random variables:</span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$R$: The number of answers the student gets right.</span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$W$: The number of answers the student does not get right.</span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$S$: The student's score on the test.</span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a><span class="fu">### (a)</span></span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a>What is the distribution of $R$? Provide the name and parameters of the appropriate distribution. Explain your answer.</span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a>&lt;details&gt;</span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a>&lt;summary&gt;&lt;b&gt;Answer&lt;/b&gt;&lt;/summary&gt;</span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a>$R$ is counting the number of "successes" (or 1s) out of $100$ total independent Bernoulli trials, where a "success" is defined as answering the question correctly, and each question is a trial. The trials are independent because the student selects a random answer with the same probability distribution, no matter whether the other answers are chosen. The probability of "success" on any single trial is $1/5 = 0.2$, so, $R$ must follow a binomial distribution with $n = 100$ and $p = 0.2$.</span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a>&lt;/details&gt;</span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a><span class="fu">### (b)</span></span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a>Find $\mathbb{E}<span class="co">[</span><span class="ot">R</span><span class="co">]</span>$</span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a>&lt;details&gt;</span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a>&lt;summary&gt;&lt;b&gt;Answer&lt;/b&gt;&lt;/summary&gt;</span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a>From class, the expectation of a $\text{Binomial}(n,p)$ random variable is always $np$. So, we obtain:</span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}<span class="co">[</span><span class="ot">R</span><span class="co">]</span> = n \cdot p = 100 \cdot 0.2 = 20$$</span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a>&lt;/details&gt;</span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a><span class="fu">### (c)</span></span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a>True or False: $\text{SD}(R) = \text{SD}(W)$? Remember that $\text{Var}(X) = \text{SD}(X)^2$.</span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a>&lt;details&gt;</span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a>&lt;summary&gt;&lt;b&gt;Answer&lt;/b&gt;&lt;/summary&gt;</span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a>True. Note that $R + W = 100$. Hence,</span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a>    \text{Var}(R) &amp;= \text{Var}(100 - W) <span class="sc">\\</span></span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a>     &amp;= (-1)^2\text{Var}(W)<span class="sc">\\</span></span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a>     &amp;= \text{Var}(W)</span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a>We use the non-linearity of variance, $\text{Var}(aX+b) = a^2\text{Var}(X)$, to simplify our expression. </span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a>&lt;/details&gt;</span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a><span class="fu">### (d)</span></span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a>Find $\mathbb{E}<span class="co">[</span><span class="ot">S</span><span class="co">]</span>$, the student's expected score on the test.</span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a>&lt;details&gt;</span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a>&lt;summary&gt;&lt;b&gt;Answer&lt;/b&gt;&lt;/summary&gt;</span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a>The student's score on the test is a function of how many they get correct and how many they get incorrect. Using the point scheme given in the question, we can write this score as $S = 4R - W$ since each correct answer is awarded $4$ points, and each wrong answer is penalized by $1$ point. Note that $S$ is also a random variable since it is a function of random variables $R$ and $W$. Note that $R + W = 100$, since there are $100$ questions. Substituting $W = 100 - R$ and using linearity of expectations, we see:</span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a>    \mathbb{E}<span class="co">[</span><span class="ot">S</span><span class="co">]</span> &amp;= \mathbb{E}<span class="co">[</span><span class="ot">4R - W</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a>    &amp;= \mathbb{E}<span class="co">[</span><span class="ot">4R - 100 + R</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a>    &amp;= \mathbb{E}<span class="co">[</span><span class="ot">5R - 100</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a>    &amp;= 5\mathbb{E}<span class="co">[</span><span class="ot">R</span><span class="co">]</span> - 100 <span class="sc">\\</span></span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a>Substituting $\mathbb{E}<span class="co">[</span><span class="ot">R</span><span class="co">]</span> = 20$ from part (b), we see the students expected score on the exam using this guessing strategy is $0$.</span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a>&lt;/details&gt;</span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a><span class="fu">### (e)</span></span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a>Find $\text{SD}(S)$</span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a>&lt;details&gt;</span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a>&lt;summary&gt;&lt;b&gt;Answer&lt;/b&gt;&lt;/summary&gt;</span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-242"><a href="#cb1-242" aria-hidden="true" tabindex="-1"></a>We know from the question above that we can write $4R - W$ as $5R - 100$. Since the variance of a random variable plus a constant is just the variance of the original random variable:</span>
<span id="cb1-243"><a href="#cb1-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-244"><a href="#cb1-244" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-245"><a href="#cb1-245" aria-hidden="true" tabindex="-1"></a>    \text{Var}(S) &amp;= \text{Var}(5R - 100) <span class="sc">\\</span></span>
<span id="cb1-246"><a href="#cb1-246" aria-hidden="true" tabindex="-1"></a>    &amp;= 5^{2}\text{Var}(R) <span class="sc">\\</span></span>
<span id="cb1-247"><a href="#cb1-247" aria-hidden="true" tabindex="-1"></a>    &amp;= 25\text{Var}(R)</span>
<span id="cb1-248"><a href="#cb1-248" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-249"><a href="#cb1-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-250"><a href="#cb1-250" aria-hidden="true" tabindex="-1"></a>We know that the variance of a $\text{Binomial}(n,p)$ variable is $np(1-p)$. Plugging in the values of $n, p$ from part (a), we see $\text{Var}(R) = 16$, giving us $\text{Var}(S) = 400$. Hence, $SD(S) = \sqrt{400} = 20$. </span>
<span id="cb1-251"><a href="#cb1-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-252"><a href="#cb1-252" aria-hidden="true" tabindex="-1"></a>&lt;/details&gt;</span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>