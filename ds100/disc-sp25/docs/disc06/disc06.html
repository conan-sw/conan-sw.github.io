<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>7&nbsp; Models, OLS – Data 100 Discussion Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../disc05/disc05.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-ca5a086e270bb62b76934925835b48c3.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../disc06/disc06.html"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Models, OLS</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Data 100 Discussion Notes</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">disc-sp25</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../disc01/disc01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Math Prerequisites</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../disc02/disc02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Pandas I</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../disc03/disc03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Pandas II, EDA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../disc04/disc04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Regex, Visualization, and Transformation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../disc05/disc05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Transformations, Sampling, and SLR</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../disc06/disc06.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Models, OLS</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Models, OLS</h2>
   
  <ul>
  <li><a href="#driving-with-a-constant-model" id="toc-driving-with-a-constant-model" class="nav-link active" data-scroll-target="#driving-with-a-constant-model"><span class="header-section-number">7.1</span> Driving with a Constant Model</a>
  <ul class="collapse">
  <li><a href="#a" id="toc-a" class="nav-link" data-scroll-target="#a"><span class="header-section-number">7.1.1</span> (a)</a></li>
  <li><a href="#b" id="toc-b" class="nav-link" data-scroll-target="#b"><span class="header-section-number">7.1.2</span> (b)</a></li>
  <li><a href="#c" id="toc-c" class="nav-link" data-scroll-target="#c"><span class="header-section-number">7.1.3</span> (c)</a></li>
  <li><a href="#d" id="toc-d" class="nav-link" data-scroll-target="#d"><span class="header-section-number">7.1.4</span> (d)</a></li>
  </ul></li>
  <li><a href="#geometry-of-least-squares" id="toc-geometry-of-least-squares" class="nav-link" data-scroll-target="#geometry-of-least-squares"><span class="header-section-number">7.2</span> Geometry of Least Squares</a></li>
  <li><a href="#multiple-choice-questions" id="toc-multiple-choice-questions" class="nav-link" data-scroll-target="#multiple-choice-questions"><span class="header-section-number">7.3</span> Multiple Choice Questions</a>
  <ul class="collapse">
  <li><a href="#a-1" id="toc-a-1" class="nav-link" data-scroll-target="#a-1"><span class="header-section-number">7.3.1</span> (a)</a></li>
  <li><a href="#b-1" id="toc-b-1" class="nav-link" data-scroll-target="#b-1"><span class="header-section-number">7.3.2</span> (b)</a></li>
  <li><a href="#c-1" id="toc-c-1" class="nav-link" data-scroll-target="#c-1"><span class="header-section-number">7.3.3</span> (c)</a></li>
  <li><a href="#d-1" id="toc-d-1" class="nav-link" data-scroll-target="#d-1"><span class="header-section-number">7.3.4</span> (d)</a></li>
  <li><a href="#e" id="toc-e" class="nav-link" data-scroll-target="#e"><span class="header-section-number">7.3.5</span> (e)</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Models, OLS</span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta column-body">

    
  
    
  </div>
  


</header>


<section id="driving-with-a-constant-model" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="driving-with-a-constant-model"><span class="header-section-number">7.1</span> Driving with a Constant Model</h2>
<p>Lillian is trying to use modeling to drive her car autonomously. To do this, she collects a lot of data from driving around her neighborhood and stores it in <code>drive</code>. She wants your help to design a model that can drive on her behalf in the future using the outputs of the models you design. First, she wants to tackle two aspects of this autonomous car modeling framework: going forward and turning. Some statistics from the collected dataset are shown below using <code>drive.describe()</code>, which returns the mean, standard deviation, quartiles, minimum, and maximum for the two columns in the dataset: <code>target_speed</code> and <code>degree_turn</code>.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">target_speed</th>
<th style="text-align: center;">degree_turn</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">count</td>
<td style="text-align: center;">500.000000</td>
<td style="text-align: center;">500.000000</td>
</tr>
<tr class="even">
<td style="text-align: center;">mean</td>
<td style="text-align: center;">32.923408</td>
<td style="text-align: center;">143.721153</td>
</tr>
<tr class="odd">
<td style="text-align: center;">std</td>
<td style="text-align: center;">46.678744</td>
<td style="text-align: center;">153.641504</td>
</tr>
<tr class="even">
<td style="text-align: center;">min</td>
<td style="text-align: center;">0.231601</td>
<td style="text-align: center;">0.000000</td>
</tr>
<tr class="odd">
<td style="text-align: center;">25%</td>
<td style="text-align: center;">12.350025</td>
<td style="text-align: center;">6.916210</td>
</tr>
<tr class="even">
<td style="text-align: center;">50%</td>
<td style="text-align: center;">25.820689</td>
<td style="text-align: center;">45.490086</td>
</tr>
<tr class="odd">
<td style="text-align: center;">75%</td>
<td style="text-align: center;">39.788716</td>
<td style="text-align: center;">323.197168</td>
</tr>
<tr class="even">
<td style="text-align: center;">max</td>
<td style="text-align: center;">379.919965</td>
<td style="text-align: center;">359.430309</td>
</tr>
</tbody>
</table>
<section id="a" class="level3" data-number="7.1.1">
<h3 data-number="7.1.1" class="anchored" data-anchor-id="a"><span class="header-section-number">7.1.1</span> (a)</h3>
<p>Suppose the first part of the model predicts the target speed of the car. Using constant models trained on the speeds of the collected data shown above with <span class="math inline">\(L_1\)</span> and <span class="math inline">\(L_2\)</span> loss functions, which of the following is true?</p>
<p>A. The model trained with the <span class="math inline">\(L_1\)</span> loss will always drive slower than the model trained with <span class="math inline">\(L_2\)</span> loss.</p>
<p>B. The model trained with the <span class="math inline">\(L_2\)</span> loss will always drive slower than the model trained with <span class="math inline">\(L_1\)</span> loss.</p>
<p>C. The mode trained with the <span class="math inline">\(L_1\)</span> loss will sometimes drive slower than the model trained with <span class="math inline">\(L_2\)</span> loss.</p>
<p>D. The model trained with the <span class="math inline">\(L_2\)</span> loss will somtimes drive slower than the model trained with <span class="math inline">\(L_1\)</span> loss.</p>
<details>
<summary>
<b>Answer</b>
</summary>
<p>A. The model trained with the <span class="math inline">\(L_1\)</span> loss will always drive slower than the model trained with <span class="math inline">\(L_2\)</span> loss.</p>
<p>Remember that when we have a constant model, using <span class="math inline">\(L_1\)</span> will always predict the <em>median</em> while <span class="math inline">\(L_2\)</span> loss will always predict the <em>mean</em>. Since the median (<span class="math inline">\(25.82\)</span>) is lower than the mean (<span class="math inline">\(32.92\)</span>), we know that the model trained on <span class="math inline">\(L_1\)</span> loss will always drive slower.</p>
</details>
</section>
<section id="b" class="level3" data-number="7.1.2">
<h3 data-number="7.1.2" class="anchored" data-anchor-id="b"><span class="header-section-number">7.1.2</span> (b)</h3>
<p>Finding that the model trained with the <span class="math inline">\(L_2\)</span> loss drives too slowly, Lillian changes the loss function for the constant model where the loss is penalized <strong>more</strong> if the true speed is higher. That way, in order to minimize loss, the model would have to output predictions closer to the true value, particularly as speeds get faster, the end result being a higher constant speed. Lillian writes this as <span class="math inline">\(L(y, \hat{y}) = y(y-\hat{y})^2\)</span>.</p>
<p>Find the optimal <span class="math inline">\(\hat{\theta}_0\)</span> for the constant model using the new empirical risk function <span class="math inline">\(R(\theta_0)\)</span> below:</p>
<p><span class="math display">\[\begin{align*}
R(\theta_0) = \frac{1}{n}\sum_i y_i(y_i-\theta_0)^2
\end{align*}\]</span></p>
<details>
<summary>
<b>Answer</b>
</summary>
<p>Take the derivative: <span class="math display">\[\begin{align}
\frac{dR}{d\theta_0} &amp;= \frac{1}{n} \sum_i \frac{d}{d\theta_0}y_i (y_i - \theta_0)^2 \\
&amp;= \frac{1}{n} \sum_i -2y_i (y_i - \theta_0) = -\frac{2}{n} \sum_i y_i^2 - y_i\theta_0
\end{align}\]</span> Set the derivative to 0: <span class="math display">\[\begin{align}
-\frac{2}{n} \sum_i y_i^2 - y_i\theta_0 = 0 \\
\theta_0 \sum_i y_i = \sum_i y_i^2\\
\theta_0 = \frac{\sum_i y_i^2}{ \sum_i y_i }
\end{align}\]</span></p>
<p>Note that the empirical risk function is convex, which you can show by computing the second derivative of <span class="math inline">\(R(\theta_0)\)</span> and noting that it is positive for all values of <span class="math inline">\(\theta_0\)</span>.</p>
<p><span class="math display">\[\frac{d^2R}{d\theta_0^2} = \frac{2}{n} \sum_i y_i &gt; 0\]</span> since <span class="math inline">\(y_i\)</span> represents speed, also validated by the fact that the minimum value is positive.</p>
<p>Therefore, any critical point must be the global minimum and so the optimal value we found minimizes empirical risk.</p>
</details>
</section>
<section id="c" class="level3" data-number="7.1.3">
<h3 data-number="7.1.3" class="anchored" data-anchor-id="c"><span class="header-section-number">7.1.3</span> (c)</h3>
<p>Lillian’s friend, Yash, also begins working on a model that predicts the degree of turning at a particular time between 0 and 359 degrees using the data in the <code>degree_turn</code> column. Explain why a constant model is likely inappropriate in this use case.</p>
<p><em>Extra</em>: If you’ve studied some physics, you may recognize the behavior of our constant model!</p>
<details>
<summary>
<b>Answer</b>
</summary>
<p>Any constant model will essentially be always turning at an angle and will be unable to turn either direction or go straight (i.e., it’ll essentially go in a circle forever).</p>
</details>
</section>
<section id="d" class="level3" data-number="7.1.4">
<h3 data-number="7.1.4" class="anchored" data-anchor-id="d"><span class="header-section-number">7.1.4</span> (d)</h3>
<p>Suppose we finally expand our modeling framework to use simple linear regression (i.e., <span class="math inline">\(f_\theta(x) = \theta_{w,0}+\theta_{w,1}x\)</span>) For our first simple linear regression model, we predict the turn angle (<span class="math inline">\(y\)</span>) using target speed (<span class="math inline">\(x\)</span>). Our optimal parameters are: <span class="math inline">\(\hat{\theta}_{w, 1}=0.019\)</span> and <span class="math inline">\(\hat{\theta}_{w,0}=143.1\)</span></p>
<p>However, we realize that we actually want a model that predicts target speed (our new <span class="math inline">\(y\)</span>) using turn angle, our new <span class="math inline">\(x\)</span> (instead of the other way around)! What are our new optimal parameters for this new model?</p>
<details>
<summary>
<b>Answer</b>
</summary>
<p>To predict target speed (new <span class="math inline">\(y\)</span>) from turn angle (new <span class="math inline">\(x\)</span>), we need to compute <span class="math inline">\(\hat{\theta}_1=r*\frac{\sigma_{\text{speed}}}{\sigma_{\text{turn}}}\)</span></p>
<p>When we predicted the turn angle from target speed, we computed <span class="math inline">\(\hat{\theta}_{w,1}=r*\frac{\sigma_{\text{turn}}}{\sigma_{\text{speed}}} = 0.019\)</span></p>
<p>Therefore, <span class="math inline">\(\hat{\theta}_1 = r * \frac{\sigma_{\text{speed}}}{\sigma_{\text{turn}}} = (r*\frac{\sigma_{\text{turn}}}{\sigma_{\text{speed}}})\frac{\sigma_{\text{speed}}^2}{\sigma_{\text{turn}}^2} = 0.019 * \frac{46.678744^2}{153.641504^2} \approx 0.00175\)</span></p>
<p>Then, <span class="math inline">\(\hat{\theta}_0 = \bar{y} - \hat{\theta}_1\bar{x} = 32.92 - 0.00175*143.72 = 32.67\)</span></p>
<p>Note that <span class="math inline">\(\bar{y}\)</span> and <span class="math inline">\(\bar{x}\)</span> are the means of target speed and turn angle respectively.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>We can’t use the inverse function <span class="math inline">\(f_\theta^{-1}(x)\)</span> since minimizing the sum of squared vertical residuals is not the inverse problem of minimizing the sum of squared horizontal residuals.</p>
<ul>
<li>The slope of the line that minimizes <span class="math inline">\(MSE(y, \hat{y})\)</span> is <span class="math inline">\(r*\frac{\sigma_y}{\sigma_x}\)</span></li>
<li>The slope of the line that minimizes <span class="math inline">\(MSE(x, \hat{x})\)</span> is <span class="math inline">\(r*\frac{\sigma_x}{\sigma_y}\)</span></li>
</ul>
<p>They are not inverses! The visualization below shows the difference between minimizing horizontal and vertical residuals.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/slr_residuals.png" class="img-fluid figure-img"></p>
<figcaption>Horizontal and Vertical Residuals</figcaption>
</figure>
</div>
</div>
</div>
</details>
</section>
</section>
<section id="geometry-of-least-squares" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="geometry-of-least-squares"><span class="header-section-number">7.2</span> Geometry of Least Squares</h2>
<p>Suppose we have a dataset represented with the design matrix span(<span class="math inline">\(\mathbb{X}\)</span>) and response vector <span class="math inline">\(\mathbb{Y}\)</span>. We use linear regression to solve for this and obtain optimal weights as <span class="math inline">\(\hat{\theta}\)</span>. Label the following terms on the geometric interpretation of ordinary least squares:</p>
<ul>
<li><span class="math inline">\(\mathbb{X}\)</span> (i.e., span(<span class="math inline">\(\mathbb{X}\)</span>))</li>
<li>The response vector <span class="math inline">\(\mathbb{Y}\)</span></li>
<li>The residual vector $ - <span class="math inline">\(\mathbb{X}\hat{\theta}\)</span></li>
<li>The prediction vector <span class="math inline">\(\mathbb{X}\hat{\theta}\)</span> (using optimal parameters)</li>
<li>A prediction vector <span class="math inline">\(\mathbb{X}\alpha\)</span> (using an arbitrary vector <span class="math inline">\(\alpha\)</span>)</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/blank.jpg" class="img-fluid figure-img"></p>
<figcaption>Geometric Empty</figcaption>
</figure>
</div>
<details>
<summary>
<b>Answer</b>
</summary>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/filled.jpg" class="img-fluid figure-img"></p>
<figcaption>Geometric Answer</figcaption>
</figure>
</div>
<p>A couple things to note:</p>
<ul>
<li>The rectangle represents span(<span class="math inline">\(\mathbb{X}\)</span>), or everywhere that you’re able to get to with a linear combination of the columns of <span class="math inline">\(\mathbb{X}\)</span>. Therefore, <span class="math inline">\(\mathbb{X}\hat{\alpha}\)</span> and <span class="math inline">\(\mathbb{X}\hat{\theta}\)</span> must be the arrows in the rectangle.</li>
<li>We know that the residual vector <span class="math inline">\(\mathbb{Y} - \mathbb{X}\hat{\theta}\)</span> is orthogonal to span(<span class="math inline">\(\mathbb{X}\)</span>)</li>
<li><span class="math inline">\(\mathbb{X}\hat{\theta}\)</span> is the best/closest to <span class="math inline">\(\mathbb{Y}\)</span> in span(<span class="math inline">\(\mathbb{X}\)</span>), so we know that it is the one that looks like it is the shadow of the bolded arrow representing <span class="math inline">\(\mathbb{Y}\)</span></li>
</ul>
</details>
</section>
<section id="multiple-choice-questions" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="multiple-choice-questions"><span class="header-section-number">7.3</span> Multiple Choice Questions</h2>
<p>Using the geometry of least squares, let’s answer a few questions about Ordinary Least Squares (OLS)!</p>
<section id="a-1" class="level3" data-number="7.3.1">
<h3 data-number="7.3.1" class="anchored" data-anchor-id="a-1"><span class="header-section-number">7.3.1</span> (a)</h3>
<p>Which of the following are true about the optimal solution <span class="math inline">\(\hat{\theta}\)</span> to OLS? Recall that the least squares estimate <span class="math inline">\(\hat{\theta}\)</span> solves the normal equation <span class="math inline">\((\mathbb{X}^T\mathbb{X})\theta = \mathbb{X}^T\mathbb{Y}\)</span></p>
<p><span class="math display">\[\begin{align*}
\hat{\theta} = (\mathbb{X}^T\mathbb{X})^{-1}\mathbb{X}^T\mathbb{Y}
\end{align*}\]</span></p>
<p><span class="math inline">\(\Box\)</span> A. Using the normal equation, we can derive an optimal solution for simple linear regression with an <span class="math inline">\(L_2\)</span> loss.</p>
<p><span class="math inline">\(\Box\)</span> B. Using the normal equation, we can derive an optimal solution for simple linear regression with an <span class="math inline">\(L_1\)</span> loss.</p>
<p><span class="math inline">\(\Box\)</span> C. Using the normal equation, we can derive an optimal solution for a constant model with an <span class="math inline">\(L_2\)</span> loss.</p>
<p><span class="math inline">\(\Box\)</span> D. Using the normal equation, we can derive an optimal solution for a constant model with an <span class="math inline">\(L_1\)</span> loss.</p>
<p><span class="math inline">\(\Box\)</span> E. Using the normal equation, we can derive an optimal solution for the model <span class="math inline">\(\hat{y} = \theta_1x + \theta_2sin(x^2)\)</span></p>
<p><span class="math inline">\(\Box\)</span> F. Using the normal equation, we can derive an optimal solution for the model <span class="math inline">\(\hat{y} = \theta_1\theta_2 + \theta_2x^2\)</span></p>
<details>
<summary>
<b>Answer</b>
</summary>
<p>We can derive solutions to both simple linear regression and constant model with an <span class="math inline">\(L_2\)</span> loss since they can be represented in the form <span class="math inline">\(y = x^T\theta\)</span> in some way. Specifically, one of the two entries of <span class="math inline">\(x\)</span> would be <span class="math inline">\(1\)</span> for SLR (and the other would be the explanatory variable). The only entry for the constant model would be <span class="math inline">\(1\)</span>.</p>
<p>We cannot derive solutions for anything with the <span class="math inline">\(L_1\)</span> loss since the normal equation optimizes for MSE.</p>
<p>Since option E is linear with respect to <span class="math inline">\(\theta\)</span>, we can use the normal equation. A good rule of thumb to determine if an equation is linear in <span class="math inline">\(\theta\)</span> is checking if an expression can be separated into a matrix product of two terms:</p>
<p><span class="math display">\[\begin{align*}
\hat{y} = \begin{bmatrix}x &amp; sin(x^2)\end{bmatrix}\begin{bmatrix}\theta_1 \\ \theta_2\end{bmatrix}
\end{align*}\]</span></p>
<p>Conversely, option F is not linear in <span class="math inline">\(\theta\)</span> so we cannot use the normal equations.</p>
</details>
</section>
<section id="b-1" class="level3" data-number="7.3.2">
<h3 data-number="7.3.2" class="anchored" data-anchor-id="b-1"><span class="header-section-number">7.3.2</span> (b)</h3>
<p>Which of the following conditions are required for the least squares estimate in the previous subpart?</p>
<p><span class="math inline">\(\Box\)</span> A. <span class="math inline">\(\mathbb{X}\)</span> must be full column rank</p>
<p><span class="math inline">\(\Box\)</span> B. <span class="math inline">\(\mathbb{Y}\)</span> must be full column rank</p>
<p><span class="math inline">\(\Box\)</span> C. <span class="math inline">\(\mathbb{X}\)</span> must be invertible</p>
<p><span class="math inline">\(\Box\)</span> D. <span class="math inline">\(\mathbb{X}^T\)</span> must be invertible</p>
<details>
<summary>
<b>Answer</b>
</summary>
<p>A. <span class="math inline">\(\mathbb{X}\)</span> must be full column rank</p>
<p><span class="math inline">\(\mathbb{X}\)</span> must be full column rank in order for the normal equation to have a unique solution. Otherwise, <span class="math inline">\(\mathbb{X}^T\mathbb{X}\)</span> will not be invertible, and there will be infinite least squares estimates. Noth that invertibility is required of <span class="math inline">\(\mathbb{X}^T\mathbb{X}\)</span>: neither <span class="math inline">\(\mathbb{X}\)</span> nor <span class="math inline">\(\mathbb{X}^T\)</span> need to be invertible. <span class="math inline">\(\mathbb{X}\)</span> and <span class="math inline">\(\mathbb{X}^T\)</span> don’t even need to be square matrices! <span class="math inline">\(\mathbb{Y}\)</span> is a vector so the matrix notion of “full column rank” does not really apply here (a vector will always span one dimension unless it is a vector of zeros).</p>
</details>
</section>
<section id="c-1" class="level3" data-number="7.3.3">
<h3 data-number="7.3.3" class="anchored" data-anchor-id="c-1"><span class="header-section-number">7.3.3</span> (c)</h3>
<p>What is always true about the residuals in the least squares regression? Select all that apply.</p>
<p><span class="math inline">\(\Box\)</span> A. They are orthogonal to the column space of the design matrix.</p>
<p><span class="math inline">\(\Box\)</span> B. They represent the errors of the predictions.</p>
<p><span class="math inline">\(\Box\)</span> C. Their sum is equal to the mean squared error.</p>
<p><span class="math inline">\(\Box\)</span> D. Their sum is equal to zero.</p>
<p><span class="math inline">\(\Box\)</span> E. None of the above.</p>
<details>
<summary>
<b>Answer</b>
</summary>
<p><strong>A</strong>, <strong>B</strong></p>
<p><strong>(C)</strong> is wrong because the mean squared error is the <em>mean</em> of the sum of the <em>squares</em> of the residuals.</p>
<p><strong>(D)</strong> A counter-example is:</p>
<p><span class="math display">\[\begin{align*}
\mathbb{X} = \begin{bmatrix}2 &amp; 3 \\ 1 &amp; 5 \\ 2 &amp; 4 \end{bmatrix}, \mathbb{Y} = \begin{bmatrix} 1 \\ 3 \\ 2 \end{bmatrix}
\end{align*}\]</span></p>
<p>After solving the least squares problem, the sum of the residuals is <span class="math inline">\(-0.0247\)</span>, which is not equal to zero. However, note that this statement is, in general, true if the design matrix <span class="math inline">\(\mathbb{X}\)</span> contains a column for the intercept!</p>
</details>
</section>
<section id="d-1" class="level3" data-number="7.3.4">
<h3 data-number="7.3.4" class="anchored" data-anchor-id="d-1"><span class="header-section-number">7.3.4</span> (d)</h3>
<p>Which of the following are true about the predictions made by OLS? Select all that apply.</p>
<p><span class="math inline">\(\Box\)</span> A. They are projections of the observations onto the column space of the design matrix.</p>
<p><span class="math inline">\(\Box\)</span> B. They are linear combinations of the features.</p>
<p><span class="math inline">\(\Box\)</span> C. They are orthogonal to the residuals.</p>
<p><span class="math inline">\(\Box\)</span> D. They are orthogonal to the column space of the features.</p>
<p><span class="math inline">\(\Box\)</span> E. None of the above.</p>
<details>
<summary>
<b>Answer</b>
</summary>
<p><strong>(A)</strong>, <strong>(B)</strong>, <strong>(C)</strong></p>
<p><strong>(A)</strong> is correct because the predictions made by OLS is the vector in the column space of <span class="math inline">\(\mathbb{X}\)</span> closest to the response vector <span class="math inline">\(\mathbb{Y}\)</span>, which is the orthogonal projection of <span class="math inline">\(\mathbb{Y}\)</span> onto <span class="math inline">\(\mathbb{X}\)</span>.</p>
<p><strong>(B)</strong> is correct because the predictions <span class="math inline">\(\mathbb{X}\hat{\theta}\)</span> is a linear combination of the columns of <span class="math inline">\(\mathbb{X}\)</span> (the features).</p>
<p><strong>(C)</strong> is correct because we know that the predictions <span class="math inline">\(\mathbb{X}\hat{\theta}\)</span> is the vector in span(<span class="math inline">\(\mathbb{X}\)</span>) closest to <span class="math inline">\(\mathbb{Y}\)</span>. Therefore, <span class="math inline">\(\mathbb{Y} - \mathbb{X}\hat{\theta}\)</span> (the residuals) must be orthogonal to the predictions.</p>
<p><strong>(D)</strong> is not correct because the predictions are in the column space of the features.</p>
</details>
</section>
<section id="e" class="level3" data-number="7.3.5">
<h3 data-number="7.3.5" class="anchored" data-anchor-id="e"><span class="header-section-number">7.3.5</span> (e)</h3>
<p>Which of the following is true of the mystery quantity <span class="math inline">\(\vec{v} = (I - \mathbb{X}(\mathbb{X}^T\mathbb{X})^{-1}\mathbb{X}^T)\mathbb{Y}\)</span>?</p>
<p><span class="math inline">\(\Box\)</span> A. The vector <span class="math inline">\(\vec{v}\)</span> represents the residuals for any linear model.</p>
<p><span class="math inline">\(\Box\)</span> B. If the matrix <span class="math inline">\(\mathbb{X}\)</span> contains the <span class="math inline">\(\vec{1}\)</span> vector, then the sum of the elements in vector <span class="math inline">\(\vec{v}\)</span> is 0 (i.e., <span class="math inline">\(\sum_iv_i = 0\)</span>)</p>
<p><span class="math inline">\(\Box\)</span> C. All the column vectors <span class="math inline">\(x_i\)</span> of <span class="math inline">\(\mathbb{X}\)</span> are orthogonal to <span class="math inline">\(\vec{v}\)</span>.</p>
<p><span class="math inline">\(\Box\)</span> D. If <span class="math inline">\(\mathbb{X}\)</span> is of shape <span class="math inline">\(n\)</span> by <span class="math inline">\(p\)</span>, there are <span class="math inline">\(p\)</span> elements in vector <span class="math inline">\(\vec{v}\)</span>.</p>
<p><span class="math inline">\(\Box\)</span> E. For any <span class="math inline">\(\vec{\alpha}\)</span>, <span class="math inline">\(\mathbb{X}\vec{\alpha}\)</span> is orthogonal to <span class="math inline">\(\vec{v}\)</span>.</p>
<details>
<summary>
<b>Answer</b>
</summary>
<p><strong>(B)</strong>, <strong>(C)</strong>, <strong>(E)</strong></p>
<p>The trick is to recognize the following:</p>
<p><span class="math display">\[\begin{align*}
\vec{v} &amp;= (I - \mathbb{X}(\mathbb{X}^T\mathbb{X})^{-1}\mathbb{X}^T)\mathbb{Y}\\
&amp;= \mathbb{Y} - \mathbb{X}(\mathbb{X}^T\mathbb{X})^{-1}\mathbb{X}^T\mathbb{Y}\\
&amp;= \mathbb{Y} - \mathbb{X}\hat{\theta}
\end{align*}\]</span></p>
<p>which is the residual vector for an OLS model.</p>
<p><strong>(A)</strong> is incorrect because any linear model does not create the residual vector <span class="math inline">\(v\)</span>; only the optimal linear model with weights <span class="math inline">\(\hat{\theta}\)</span> does.</p>
<p><strong>(D)</strong> is incorrect because the vector <span class="math inline">\(v\)</span> is of size <span class="math inline">\(n\)</span> since there are <span class="math inline">\(n\)</span> data points.</p>
<p>The rest are correct by properties of orthogonality as given by the geometry of least squares.</p>


<!-- -->

</details></section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../disc05/disc05.html" class="pagination-link" aria-label="Transformations, Sampling, and SLR">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Transformations, Sampling, and SLR</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> Models, OLS</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="an">execute:</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co">  echo: true</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: true</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">    toc-title: Models, OLS</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">    page-layout: full</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">    theme:</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">      - cosmo</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co">      - cerulean</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co">    callout-icon: false</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="fu">## Driving with a Constant Model</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>Lillian is trying to use modeling to drive her car autonomously. To do this, she collects a lot</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>of data from driving around her neighborhood and stores it in <span class="in">`drive`</span>. She wants your help to</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>design a model that can drive on her behalf in the future using the outputs of the models you</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>design. First, she wants to tackle two aspects of this autonomous car modeling framework:</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>going forward and turning.</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>Some statistics from the collected dataset are shown below using <span class="in">`drive.describe()`</span>,</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>which returns the mean, standard deviation, quartiles, minimum, and maximum for the two</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>columns in the dataset: <span class="in">`target_speed`</span> and <span class="in">`degree_turn`</span>.</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>| | target_speed | degree_turn |</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>| :-: | :-: | :-: |</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>| count | 500.000000 | 500.000000 |</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>| mean | 32.923408 | 143.721153 |</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>| std | 46.678744 | 153.641504 |</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>| min | 0.231601 | 0.000000|</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>| 25% | 12.350025 | 6.916210 |</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>| 50% | 25.820689 | 45.490086 |</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>| 75% | 39.788716 | 323.197168 |</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>| max | 379.919965 | 359.430309 |</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="fu">### (a)</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>Suppose the first part of the model predicts the target speed of the car. Using constant</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>models trained on the speeds of the collected data shown above with $L_1$ and $L_2$ loss</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>functions, which of the following is true?</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>A. The model trained with the $L_1$ loss will always drive slower than the model trained with $L_2$ loss.</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>B. The model trained with the $L_2$ loss will always drive slower than the model trained with $L_1$ loss.</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>C. The mode trained with the $L_1$ loss will sometimes drive slower than the model trained with $L_2$ loss.</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>D. The model trained with the $L_2$ loss will somtimes drive slower than the model trained with $L_1$ loss.</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>&lt;details&gt;</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>&lt;summary&gt;&lt;b&gt;Answer&lt;/b&gt;&lt;/summary&gt;</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>A. The model trained with the $L_1$ loss will always drive slower than the model trained with $L_2$ loss.</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>Remember that when we have a constant model, using $L_1$ will always predict the *median* while $L_2$ loss will always predict the *mean*. Since the median ($25.82$) is lower than the mean ($32.92$), we know that the model trained on $L_1$ loss will always drive slower.</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>&lt;/details&gt;</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a><span class="fu">### (b)</span></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>Finding that the model trained with the $L_2$ loss drives too slowly, Lillian changes the</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>loss function for the constant model where the loss is penalized **more** if the true speed is</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>higher. That way, in order to minimize loss, the model would have to output predictions</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>closer to the true value, particularly as speeds get faster, the end result being a higher</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>constant speed. Lillian writes this as $L(y, \hat{y}) = y(y-\hat{y})^2$.</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>Find the optimal $\hat{\theta}_0$ for the constant model using the new empirical risk function $R(\theta_0)$</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>below:</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>R(\theta_0) = \frac{1}{n}\sum_i y_i(y_i-\theta_0)^2</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>&lt;details&gt;</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>&lt;summary&gt;&lt;b&gt;Answer&lt;/b&gt;&lt;/summary&gt;</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>Take the derivative:</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>\frac{dR}{d\theta_0} &amp;= \frac{1}{n} \sum_i \frac{d}{d\theta_0}y_i (y_i - \theta_0)^2 <span class="sc">\\</span></span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{1}{n} \sum_i -2y_i (y_i - \theta_0) = -\frac{2}{n} \sum_i y_i^2 - y_i\theta_0 </span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>Set the derivative to 0:</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>-\frac{2}{n} \sum_i y_i^2 - y_i\theta_0 = 0 <span class="sc">\\</span></span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>\theta_0 \sum_i y_i = \sum_i y_i^2<span class="sc">\\</span></span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>\theta_0 = \frac{\sum_i y_i^2}{ \sum_i y_i }</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>Note that the empirical risk function is convex, which you can show by computing the second derivative of $R(\theta_0)$ and noting that it is positive for all values of $\theta_0$.</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>$$\frac{d^2R}{d\theta_0^2} = \frac{2}{n} \sum_i y_i &gt; 0$$</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>since $y_i$ represents speed, also validated by the fact that the minimum value is positive.</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>Therefore, any critical point must be the global minimum and so the optimal value we found minimizes empirical risk.</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>&lt;/details&gt;</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a><span class="fu">### (c)</span></span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>Lillian’s friend, Yash, also begins working on a model that predicts the degree of turning</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>at a particular time between 0 and 359 degrees using the data in the <span class="in">`degree_turn`</span></span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>column. Explain why a constant model is likely inappropriate in this use case.</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>*Extra*: If you’ve studied some physics, you may recognize the behavior of our constant</span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a>model!</span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a>&lt;details&gt;</span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a>&lt;summary&gt;&lt;b&gt;Answer&lt;/b&gt;&lt;/summary&gt;</span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a>Any constant model will essentially be always turning at an angle and will</span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>be unable to turn either direction or go straight (i.e., it’ll essentially go in a circle</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a>forever).</span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a>&lt;/details&gt;</span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a><span class="fu">### (d)</span></span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a>Suppose we finally expand our modeling framework to use simple linear regression (i.e., $f_\theta(x) = \theta_{w,0}+\theta_{w,1}x$) For our first simple linear regression model, we predict the turn angle ($y$) using target speed ($x$). Our optimal parameters are: $\hat{\theta}_{w, 1}=0.019$ and $\hat{\theta}_{w,0}=143.1$</span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a>However, we realize that we actually want a model that predicts target speed (our new $y$)</span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a>using turn angle, our new $x$ (instead of the other way around)! What are our new optimal</span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a>parameters for this new model?</span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a>&lt;details&gt;</span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a>&lt;summary&gt;&lt;b&gt;Answer&lt;/b&gt;&lt;/summary&gt;</span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a>To predict target speed (new $y$) from turn angle (new $x$), we need to compute $\hat{\theta}_1=r*\frac{\sigma_{\text{speed}}}{\sigma_{\text{turn}}}$</span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a>When we predicted the turn angle from target speed, we computed $\hat{\theta}_{w,1}=r*\frac{\sigma_{\text{turn}}}{\sigma_{\text{speed}}} = 0.019$</span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a>Therefore, $\hat{\theta}_1 = r * \frac{\sigma_{\text{speed}}}{\sigma_{\text{turn}}} = (r*\frac{\sigma_{\text{turn}}}{\sigma_{\text{speed}}})\frac{\sigma_{\text{speed}}^2}{\sigma_{\text{turn}}^2} = 0.019 * \frac{46.678744^2}{153.641504^2} \approx 0.00175$</span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a>Then, $\hat{\theta}_0 = \bar{y} - \hat{\theta}_1\bar{x} = 32.92 - 0.00175*143.72 = 32.67$</span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a>Note that $\bar{y}$ and $\bar{x}$ are the means of target speed and turn angle respectively.</span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a>We can't use the inverse function $f_\theta^{-1}(x)$ since minimizing the sum of squared vertical residuals is not the inverse problem of minimizing the sum of squared horizontal residuals.</span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The slope of the line that minimizes $MSE(y, \hat{y})$ is $r*\frac{\sigma_y}{\sigma_x}$</span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The slope of the line that minimizes $MSE(x, \hat{x})$ is $r*\frac{\sigma_x}{\sigma_y}$</span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a>They are not inverses! The visualization below shows the difference between minimizing horizontal and vertical residuals.</span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a><span class="al">![Horizontal and Vertical Residuals](images/slr_residuals.png)</span></span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a>&lt;/details&gt;</span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a><span class="fu">## Geometry of Least Squares</span></span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a>Suppose we have a dataset represented with the design matrix span($\mathbb{X}$) and response vector $\mathbb{Y}$. We use linear regression to solve for this and obtain optimal weights as $\hat{\theta}$. Label the following terms on the geometric interpretation of ordinary least squares:</span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$\mathbb{X}$ (i.e., span($\mathbb{X}$))</span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The response vector $\mathbb{Y}$</span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The residual vector $\mathbb{Y} - $\mathbb{X}\hat{\theta}$</span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The prediction vector $\mathbb{X}\hat{\theta}$ (using optimal parameters)</span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>A prediction vector $\mathbb{X}\alpha$ (using an arbitrary vector $\alpha$)</span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a><span class="al">![Geometric Empty](images/blank.jpg)</span></span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a>&lt;details&gt;</span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a>&lt;summary&gt;&lt;b&gt;Answer&lt;/b&gt;&lt;/summary&gt;</span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a><span class="al">![Geometric Answer](images/filled.jpg)</span></span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a>A couple things to note:</span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The rectangle represents span($\mathbb{X}$), or everywhere that you're able to get to with a linear combination of the columns of $\mathbb{X}$. Therefore, $\mathbb{X}\hat{\alpha}$ and $\mathbb{X}\hat{\theta}$ must be the arrows in the rectangle.</span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>We know that the residual vector $\mathbb{Y} - \mathbb{X}\hat{\theta}$ is orthogonal to span($\mathbb{X}$)</span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$\mathbb{X}\hat{\theta}$ is the best/closest to $\mathbb{Y}$ in span($\mathbb{X}$), so we know that it is the one that looks like it is the shadow of the bolded arrow representing $\mathbb{Y}$</span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a>&lt;/details&gt;</span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a><span class="fu">## Multiple Choice Questions</span></span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a>Using the geometry of least squares, let's answer a few questions about Ordinary Least Squares (OLS)!</span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a><span class="fu">### (a)</span></span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a>Which of the following are true about the optimal solution $\hat{\theta}$ to OLS? Recall that the least squares estimate $\hat{\theta}$ solves the normal equation $(\mathbb{X}^T\mathbb{X})\theta = \mathbb{X}^T\mathbb{Y}$</span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a>\hat{\theta} = (\mathbb{X}^T\mathbb{X})^{-1}\mathbb{X}^T\mathbb{Y}</span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a>$\Box$ A. Using the normal equation, we can derive an optimal solution for simple linear regression with an $L_2$ loss.</span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a>$\Box$ B. Using the normal equation, we can derive an optimal solution for simple linear regression with an $L_1$ loss.</span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a>$\Box$ C. Using the normal equation, we can derive an optimal solution for a constant model with an $L_2$ loss.</span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a>$\Box$ D. Using the normal equation, we can derive an optimal solution for a constant model with an $L_1$ loss.</span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a>$\Box$ E. Using the normal equation, we can derive an optimal solution for the model $\hat{y} = \theta_1x + \theta_2sin(x^2)$</span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a>$\Box$ F. Using the normal equation, we can derive an optimal solution for the model $\hat{y} = \theta_1\theta_2 + \theta_2x^2$</span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a>&lt;details&gt;</span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a>&lt;summary&gt;&lt;b&gt;Answer&lt;/b&gt;&lt;/summary&gt;</span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a>We can derive solutions to both simple linear regression and constant model with an $L_2$ loss since they can be represented in the form $y = x^T\theta$ in some way. Specifically, one of the two entries of $x$ would be $1$ for SLR (and the other would be the explanatory variable). The only entry for the constant model would be $1$.</span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a>We cannot derive solutions for anything with the $L_1$ loss since the normal equation optimizes for MSE.</span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a>Since option E is linear with respect to $\theta$, we can use the normal equation. A good rule of thumb to determine if an equation is linear in $\theta$ is checking if an expression can be separated into a matrix product of two terms:</span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a>\hat{y} = \begin{bmatrix}x &amp; sin(x^2)\end{bmatrix}\begin{bmatrix}\theta_1 <span class="sc">\\</span> \theta_2\end{bmatrix}</span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a>Conversely, option F is not linear in $\theta$ so we cannot use the normal equations.</span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a>&lt;/details&gt;</span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a><span class="fu">### (b)</span></span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a>Which of the following conditions are required for the least squares estimate in the previous subpart?</span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a>$\Box$ A. $\mathbb{X}$ must be full column rank</span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a>$\Box$ B. $\mathbb{Y}$ must be full column rank</span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a>$\Box$ C. $\mathbb{X}$ must be invertible</span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a>$\Box$ D. $\mathbb{X}^T$ must be invertible</span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a>&lt;details&gt;</span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a>&lt;summary&gt;&lt;b&gt;Answer&lt;/b&gt;&lt;/summary&gt;</span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a>A. $\mathbb{X}$ must be full column rank</span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a>$\mathbb{X}$ must be full column rank in order for the normal equation to have a unique solution. Otherwise, $\mathbb{X}^T\mathbb{X}$ will not be invertible, and there will be infinite least squares estimates. Noth that invertibility is required of $\mathbb{X}^T\mathbb{X}$: neither $\mathbb{X}$ nor $\mathbb{X}^T$ need to be invertible. $\mathbb{X}$ and $\mathbb{X}^T$ don't even need to be square matrices! $\mathbb{Y}$ is a vector so the matrix notion of "full column rank" does not really apply here (a vector will always span one dimension unless it is a vector of zeros).</span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a>&lt;/details&gt;</span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a><span class="fu">### (c)</span></span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a>What is always true about the residuals in the least squares regression? Select all that apply.</span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a>$\Box$ A. They are orthogonal to the column space of the design matrix.</span>
<span id="cb1-242"><a href="#cb1-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-243"><a href="#cb1-243" aria-hidden="true" tabindex="-1"></a>$\Box$ B. They represent the errors of the predictions.</span>
<span id="cb1-244"><a href="#cb1-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-245"><a href="#cb1-245" aria-hidden="true" tabindex="-1"></a>$\Box$ C. Their sum is equal to the mean squared error.</span>
<span id="cb1-246"><a href="#cb1-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-247"><a href="#cb1-247" aria-hidden="true" tabindex="-1"></a>$\Box$ D. Their sum is equal to zero.</span>
<span id="cb1-248"><a href="#cb1-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-249"><a href="#cb1-249" aria-hidden="true" tabindex="-1"></a>$\Box$ E. None of the above.</span>
<span id="cb1-250"><a href="#cb1-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-251"><a href="#cb1-251" aria-hidden="true" tabindex="-1"></a>&lt;details&gt;</span>
<span id="cb1-252"><a href="#cb1-252" aria-hidden="true" tabindex="-1"></a>&lt;summary&gt;&lt;b&gt;Answer&lt;/b&gt;&lt;/summary&gt;</span>
<span id="cb1-253"><a href="#cb1-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-254"><a href="#cb1-254" aria-hidden="true" tabindex="-1"></a>**A**, **B**</span>
<span id="cb1-255"><a href="#cb1-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-256"><a href="#cb1-256" aria-hidden="true" tabindex="-1"></a>**(C)** is wrong because the mean squared error is the *mean* of the sum of the *squares* of the residuals.</span>
<span id="cb1-257"><a href="#cb1-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-258"><a href="#cb1-258" aria-hidden="true" tabindex="-1"></a>**(D)** A counter-example is: </span>
<span id="cb1-259"><a href="#cb1-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-260"><a href="#cb1-260" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-261"><a href="#cb1-261" aria-hidden="true" tabindex="-1"></a>\mathbb{X} = \begin{bmatrix}2 &amp; 3 <span class="sc">\\</span> 1 &amp; 5 <span class="sc">\\</span> 2 &amp; 4 \end{bmatrix}, \mathbb{Y} = \begin{bmatrix} 1 <span class="sc">\\</span> 3 <span class="sc">\\</span> 2 \end{bmatrix}</span>
<span id="cb1-262"><a href="#cb1-262" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-263"><a href="#cb1-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-264"><a href="#cb1-264" aria-hidden="true" tabindex="-1"></a>After solving the least squares problem, the sum of the residuals is $-0.0247$, which is not equal to zero. However, note that this statement is, in general, true if the design matrix $\mathbb{X}$ contains a column for the intercept!</span>
<span id="cb1-265"><a href="#cb1-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-266"><a href="#cb1-266" aria-hidden="true" tabindex="-1"></a>&lt;/details&gt;</span>
<span id="cb1-267"><a href="#cb1-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-268"><a href="#cb1-268" aria-hidden="true" tabindex="-1"></a><span class="fu">### (d)</span></span>
<span id="cb1-269"><a href="#cb1-269" aria-hidden="true" tabindex="-1"></a>Which of the following are true about the predictions made by OLS? Select all that apply.</span>
<span id="cb1-270"><a href="#cb1-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-271"><a href="#cb1-271" aria-hidden="true" tabindex="-1"></a>$\Box$ A. They are projections of the observations onto the column space of the design matrix.</span>
<span id="cb1-272"><a href="#cb1-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-273"><a href="#cb1-273" aria-hidden="true" tabindex="-1"></a>$\Box$ B. They are linear combinations of the features.</span>
<span id="cb1-274"><a href="#cb1-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-275"><a href="#cb1-275" aria-hidden="true" tabindex="-1"></a>$\Box$ C. They are orthogonal to the residuals.</span>
<span id="cb1-276"><a href="#cb1-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-277"><a href="#cb1-277" aria-hidden="true" tabindex="-1"></a>$\Box$ D. They are orthogonal to the column space of the features.</span>
<span id="cb1-278"><a href="#cb1-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-279"><a href="#cb1-279" aria-hidden="true" tabindex="-1"></a>$\Box$ E. None of the above.</span>
<span id="cb1-280"><a href="#cb1-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-281"><a href="#cb1-281" aria-hidden="true" tabindex="-1"></a>&lt;details&gt;</span>
<span id="cb1-282"><a href="#cb1-282" aria-hidden="true" tabindex="-1"></a>&lt;summary&gt;&lt;b&gt;Answer&lt;/b&gt;&lt;/summary&gt;</span>
<span id="cb1-283"><a href="#cb1-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-284"><a href="#cb1-284" aria-hidden="true" tabindex="-1"></a>**(A)**, **(B)**, **(C)**</span>
<span id="cb1-285"><a href="#cb1-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-286"><a href="#cb1-286" aria-hidden="true" tabindex="-1"></a>**(A)** is correct because the predictions made by OLS is the vector in the column space of $\mathbb{X}$ closest to the response vector $\mathbb{Y}$, which is the orthogonal projection of $\mathbb{Y}$ onto $\mathbb{X}$.</span>
<span id="cb1-287"><a href="#cb1-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-288"><a href="#cb1-288" aria-hidden="true" tabindex="-1"></a>**(B)** is correct because the predictions $\mathbb{X}\hat{\theta}$ is a linear combination of the columns of $\mathbb{X}$ (the features).</span>
<span id="cb1-289"><a href="#cb1-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-290"><a href="#cb1-290" aria-hidden="true" tabindex="-1"></a>**(C)** is correct because we know that the predictions $\mathbb{X}\hat{\theta}$ is the vector in span($\mathbb{X}$) closest to $\mathbb{Y}$. Therefore, $\mathbb{Y} - \mathbb{X}\hat{\theta}$ (the residuals) must be orthogonal to the predictions.</span>
<span id="cb1-291"><a href="#cb1-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-292"><a href="#cb1-292" aria-hidden="true" tabindex="-1"></a>**(D)** is not correct because the predictions are in the column space of the features.</span>
<span id="cb1-293"><a href="#cb1-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-294"><a href="#cb1-294" aria-hidden="true" tabindex="-1"></a>&lt;/details&gt;</span>
<span id="cb1-295"><a href="#cb1-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-296"><a href="#cb1-296" aria-hidden="true" tabindex="-1"></a><span class="fu">### (e)</span></span>
<span id="cb1-297"><a href="#cb1-297" aria-hidden="true" tabindex="-1"></a>Which of the following is true of the mystery quantity $\vec{v} = (I - \mathbb{X}(\mathbb{X}^T\mathbb{X})^{-1}\mathbb{X}^T)\mathbb{Y}$?</span>
<span id="cb1-298"><a href="#cb1-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-299"><a href="#cb1-299" aria-hidden="true" tabindex="-1"></a>$\Box$ A. The vector $\vec{v}$ represents the residuals for any linear model.</span>
<span id="cb1-300"><a href="#cb1-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-301"><a href="#cb1-301" aria-hidden="true" tabindex="-1"></a>$\Box$ B. If the matrix $\mathbb{X}$ contains the $\vec{1}$ vector, then the sum of the elements in vector $\vec{v}$ is 0 (i.e., $\sum_iv_i = 0$)</span>
<span id="cb1-302"><a href="#cb1-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-303"><a href="#cb1-303" aria-hidden="true" tabindex="-1"></a>$\Box$ C. All the column vectors $x_i$ of $\mathbb{X}$ are orthogonal to $\vec{v}$.</span>
<span id="cb1-304"><a href="#cb1-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-305"><a href="#cb1-305" aria-hidden="true" tabindex="-1"></a>$\Box$ D. If $\mathbb{X}$ is of shape $n$ by $p$, there are $p$ elements in vector $\vec{v}$.</span>
<span id="cb1-306"><a href="#cb1-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-307"><a href="#cb1-307" aria-hidden="true" tabindex="-1"></a>$\Box$ E. For any $\vec{\alpha}$, $\mathbb{X}\vec{\alpha}$ is orthogonal to $\vec{v}$.</span>
<span id="cb1-308"><a href="#cb1-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-309"><a href="#cb1-309" aria-hidden="true" tabindex="-1"></a>&lt;details&gt;</span>
<span id="cb1-310"><a href="#cb1-310" aria-hidden="true" tabindex="-1"></a>&lt;summary&gt;&lt;b&gt;Answer&lt;/b&gt;&lt;/summary&gt;</span>
<span id="cb1-311"><a href="#cb1-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-312"><a href="#cb1-312" aria-hidden="true" tabindex="-1"></a>**(B)**, **(C)**, **(E)**</span>
<span id="cb1-313"><a href="#cb1-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-314"><a href="#cb1-314" aria-hidden="true" tabindex="-1"></a>The trick is to recognize the following:</span>
<span id="cb1-315"><a href="#cb1-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-316"><a href="#cb1-316" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-317"><a href="#cb1-317" aria-hidden="true" tabindex="-1"></a>\vec{v} &amp;= (I - \mathbb{X}(\mathbb{X}^T\mathbb{X})^{-1}\mathbb{X}^T)\mathbb{Y}<span class="sc">\\</span></span>
<span id="cb1-318"><a href="#cb1-318" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbb{Y} - \mathbb{X}(\mathbb{X}^T\mathbb{X})^{-1}\mathbb{X}^T\mathbb{Y}<span class="sc">\\</span></span>
<span id="cb1-319"><a href="#cb1-319" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbb{Y} - \mathbb{X}\hat{\theta}</span>
<span id="cb1-320"><a href="#cb1-320" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-321"><a href="#cb1-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-322"><a href="#cb1-322" aria-hidden="true" tabindex="-1"></a>which is the residual vector for an OLS model.</span>
<span id="cb1-323"><a href="#cb1-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-324"><a href="#cb1-324" aria-hidden="true" tabindex="-1"></a>**(A)** is incorrect because any linear model does not create the residual vector $v$; only the optimal linear model with weights $\hat{\theta}$ does.</span>
<span id="cb1-325"><a href="#cb1-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-326"><a href="#cb1-326" aria-hidden="true" tabindex="-1"></a>**(D)** is incorrect because the vector $v$ is of size $n$ since there are $n$ data points.</span>
<span id="cb1-327"><a href="#cb1-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-328"><a href="#cb1-328" aria-hidden="true" tabindex="-1"></a>The rest are correct by properties of orthogonality as given by the geometry of least squares.</span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>