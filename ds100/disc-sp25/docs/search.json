[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data 100 Discussion Notes",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>disc-sp25</span>"
    ]
  },
  {
    "objectID": "disc02/disc02.html",
    "href": "disc02/disc02.html",
    "title": "2  Pandas I",
    "section": "",
    "text": "This discussion is all about practicing using pandas, and testing your knowledge about its various functionalities to accomplish small tasks.\nWe will be using the elections dataset from lecture.\n\n# import packages\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\nelections = pd.read_csv('elections.csv')\nelections.head(10)\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\n\n\n1\n1824\nJohn Quincy Adams\nDemocratic-Republican\n113142\nwin\n42.789878\n\n\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\n\n\n3\n1828\nJohn Quincy Adams\nNational Republican\n500897\nloss\n43.796073\n\n\n4\n1832\nAndrew Jackson\nDemocratic\n702735\nwin\n54.574789\n\n\n5\n1832\nHenry Clay\nNational Republican\n484205\nloss\n37.603628\n\n\n6\n1832\nWilliam Wirt\nAnti-Masonic\n100715\nloss\n7.821583\n\n\n7\n1836\nHugh Lawson White\nWhig\n146109\nloss\n10.005985\n\n\n8\n1836\nMartin Van Buren\nDemocratic\n763291\nwin\n52.272472\n\n\n9\n1836\nWilliam Henry Harrison\nWhig\n550816\nloss\n37.721543\n\n\n\n\n\n\n\nWrite a line of code that returns the elections table sorted in descending order by \"Popular vote\". Store your result in a variable named sorted. Would calling sorted.iloc[[0], :] give the same result as sorted.loc[[0], :]?\n\n\nCode\nsorted = elections.sort_values(\"Popular vote\", ascending = False)\nsorted\n\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n178\n2020\nJoseph Biden\nDemocratic\n81268924\nwin\n51.311515\n\n\n182\n2024\nDonald Trump\nRepublican\n77303568\nwin\n49.808629\n\n\n183\n2024\nKamala Harris\nDemocratic\n75019230\nloss\n48.336772\n\n\n179\n2020\nDonald Trump\nRepublican\n74216154\nloss\n46.858542\n\n\n162\n2008\nBarack Obama\nDemocratic\n69498516\nwin\n53.023510\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n110\n1956\nT. Coleman Andrews\nStates' Rights\n107929\nloss\n0.174883\n\n\n141\n1992\nBo Gritz\nPopulist\n106152\nloss\n0.101918\n\n\n99\n1948\nClaude A. Watson\nProhibition\n103708\nloss\n0.212747\n\n\n89\n1932\nWilliam Z. Foster\nCommunist\n103307\nloss\n0.261069\n\n\n6\n1832\nWilliam Wirt\nAnti-Masonic\n100715\nloss\n7.821583\n\n\n\n\n187 rows × 6 columns\n\n\n\n\n\nExplanation\n\n\n\nWe can sort a DataFrame by a column using the .sort_values() function! Remember to specify ascending = False, or else it will sort in increasing order.\n\n\n\nsorted.iloc[[0], :]\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n178\n2020\nJoseph Biden\nDemocratic\n81268924\nwin\n51.311515\n\n\n\n\n\n\n\n\nsorted.loc[[0], :]\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\n\n\n\n\n\n\n\n\n\nExplanation\n\n \n\nThe difference is that .loc[] uses label-based indexing, while .iloc[] uses integer position-based indexing. Using .loc[] will simply grab the row with the label 0 regardless of where it is, while .iloc[] will grab the first row of the sorted DataFrame.\n\n\n\nUsing Boolean slicing, write one line of pandas code that returns a DataFrame that only contains election results from the 1900s.\n\n\nCode\nelections[(elections[\"Year\"] &gt;= 1900) & (elections[\"Year\"] &lt; 2000)]\n\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n54\n1900\nJohn G. Woolley\nProhibition\n210864\nloss\n1.526821\n\n\n55\n1900\nWilliam Jennings Bryan\nDemocratic\n6370932\nloss\n46.130540\n\n\n56\n1900\nWilliam McKinley\nRepublican\n7228864\nwin\n52.342640\n\n\n57\n1904\nAlton B. Parker\nDemocratic\n5083880\nloss\n37.685116\n\n\n58\n1904\nEugene V. Debs\nSocialist\n402810\nloss\n2.985897\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n146\n1996\nHarry Browne\nLibertarian\n485759\nloss\n0.505198\n\n\n147\n1996\nHoward Phillips\nTaxpayers\n184656\nloss\n0.192045\n\n\n148\n1996\nJohn Hagelin\nNatural Law\n113670\nloss\n0.118219\n\n\n149\n1996\nRalph Nader\nGreen\n685297\nloss\n0.712721\n\n\n150\n1996\nRoss Perot\nReform\n8085294\nloss\n8.408844\n\n\n\n\n97 rows × 6 columns\n\n\n\n\n\nExplanation\n\n \n\nWe can “filter” DataFrames by using boolean slicing! 1. Construct a boolean Series that is True if a row contains election results from the 1900s, and False otherwise. * We can use the & (and) logical operator! 1900 or after and before 2000. 2. Use the boolean Series to slice the DataFrame * df[boolean_array]\n\n\n\nWrite one line of pandas code that returns a Series, where the index is the \"Party\", and the values are how many times that party won an election. Only include parties that have won an election.\n\n\nCode\nelections[elections[\"Result\"] == \"win\"][\"Party\"].value_counts()\n\n\nParty\nRepublican               24\nDemocratic               23\nWhig                      2\nDemocratic-Republican     1\nNational Union            1\nName: count, dtype: int64\n\n\n\n\nCode\nelections[elections[\"Result\"] == \"win\"].groupby(\"Party\").size()\n\n\nParty\nDemocratic               23\nDemocratic-Republican     1\nNational Union            1\nRepublican               24\nWhig                      2\ndtype: int64\n\n\n\n\nExplanation\n\n \n\nTwo parts to this! 1. Filter DataFrame to only include winners. * Use boolean slicing again! Construct a boolean Series that has True if the row contains a winner, and False otherwise * elections[elections[\"Result\"] == \"win\"] 2. Within filtered DataFrame (let’s call this winners), count the number of times each party won an election. Two ways to do this. * Extract the Party column from winners, and call value_counts(). * winners[\"Party\"].value_counts() * Group by the Party column, and aggregate by the number of rows in each sub-DataFrame. * winners.groupby(\"Party\").size() * The two methods above return the same thing, except .value_counts() sorts by the values in decreasing order, while .groupby() sort by the index in increasing order!\n\n\n\nWrite a line of pandas code that returns a Series whose index is the years and whose values are the number of candidates that participated in those years’ elections.\n\n\nCode\nelections[\"Year\"].value_counts().head() #.head() to limit output\n\n\nYear\n1996    7\n1948    6\n1976    6\n2004    6\n2008    6\nName: count, dtype: int64\n\n\n\n\nCode\nelections.groupby(\"Year\").size().head() #.head() to limit output\n\n\nYear\n1824    2\n1828    2\n1832    3\n1836    3\n1840    2\ndtype: int64\n\n\n\n\nExplanation\n\n \n\nVery similar to Problem 3! Might even be easier, actually. Each row corresponds to one candidate per election cycle, so we simply need to count the number of times each Year appears in the elections DataFrame. Just like in Problem 3, two ways to do this.\n\nExtract the Year column as a Series, call .value_counts() on it.\n\nelections[\"Year\"].value_counts()\n\n\nGroup by the Year column, creating a sub-DataFrame for each unique Year. Aggregate by .size(), counting the number of rows in each sub-DataFrame.\n\nelections.groupby(\"Year\").size()\n\n\n\n\n\nWrite a line of pandas code that creates a filtered DataFrame named filtered_parties from the elections dataset and keeps only the parties that have at least one election % more than 50%.\n\n\nCode\nfiltered_parties = elections.groupby(\"Party\").filter(lambda df: df[\"%\"].max() &gt; 50)\nfiltered_parties\n\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\n\n\n1\n1824\nJohn Quincy Adams\nDemocratic-Republican\n113142\nwin\n42.789878\n\n\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\n\n\n4\n1832\nAndrew Jackson\nDemocratic\n702735\nwin\n54.574789\n\n\n7\n1836\nHugh Lawson White\nWhig\n146109\nloss\n10.005985\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n176\n2016\nHillary Clinton\nDemocratic\n65853514\nloss\n48.521539\n\n\n178\n2020\nJoseph Biden\nDemocratic\n81268924\nwin\n51.311515\n\n\n179\n2020\nDonald Trump\nRepublican\n74216154\nloss\n46.858542\n\n\n182\n2024\nDonald Trump\nRepublican\n77303568\nwin\n49.808629\n\n\n183\n2024\nKamala Harris\nDemocratic\n75019230\nloss\n48.336772\n\n\n\n\n99 rows × 6 columns\n\n\n\n\n\nExplanation\n\n \n\nThis filtering is different from boolean slicing! Boolean slicing considers rows individually, while .filter() considers groups of rows. Rows of a sub-DataFrame either all make it, or none make it.\n\nGroup by the Party column, creating one sub-DataFrame for each party.\n\nelections.groupby(\"Party\")\n\nFilter using .filter()\n\nPass in a function into .filter() that takes in a DataFrame and returns True or False. Can be a lambda function!\n.filter(lambda df: df[\"%\"].max() &gt; 50)\n\nIf the lambda function returns True, it means you keep the entire sub-DataFrame. False means you exclude it entirely!\n\n\n\n\n\n\nWrite a line of pandas code that uses the filtered_parties DataFrame to return a new DataFrame with row indices that correspond to the year and columns that correspond to each party. Each entry should be the total percentage of votes for all the candidates that ran during that particular year for the specified party. Missing values from the dataset (the cases where a party did not have a candidate in a particular year) should be entered as 0. Below is an example.\n\n\n\nCode\nelections_pivot = filtered_parties.pivot_table(\n    index = \"Year\",\n    columns = \"Party\",\n    values = \"%\",\n    aggfunc = np.sum,\n    fill_value = 0)\nelections_pivot.head(10)\n\n\n\n\n\n\n\n\nParty\nDemocratic\nDemocratic-Republican\nNational Union\nRepublican\nWhig\n\n\nYear\n\n\n\n\n\n\n\n\n\n1824\n0.000000\n100.0\n0.0\n0.000000\n0.000000\n\n\n1828\n56.203927\n0.0\n0.0\n0.000000\n0.000000\n\n\n1832\n54.574789\n0.0\n0.0\n0.000000\n0.000000\n\n\n1836\n52.272472\n0.0\n0.0\n0.000000\n47.727528\n\n\n1840\n46.948787\n0.0\n0.0\n0.000000\n53.051213\n\n\n1844\n50.749477\n0.0\n0.0\n0.000000\n49.250523\n\n\n1848\n42.552229\n0.0\n0.0\n0.000000\n47.309296\n\n\n1852\n51.013168\n0.0\n0.0\n0.000000\n44.056548\n\n\n1856\n45.306080\n0.0\n0.0\n33.139919\n0.000000\n\n\n1860\n0.000000\n0.0\n0.0\n39.699408\n0.000000\n\n\n\n\n\n\n\n\n\nExplanation\n\n \n\nFirst thing to notice is that the columns are values of the Party column! This tells us that what we see is a pivot table.\n\nUse the .pivot_table() function on filtered_parties\n\nindex = \"Year\" and columns = \"Party\", saying that the unique values of Year should make up the row indices, and the unique values of Party should make up the columns.\nvalues = \"%\" indicates that we populate the cells with the % values for each combination of Year, Party\naggfunc = np.sum describes how to aggregate the values in a cell\nfill_value = 0 says to impute 0 in case there is no % value for a specific Year, Party combination",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Pandas I</span>"
    ]
  },
  {
    "objectID": "disc03/disc03.html",
    "href": "disc03/disc03.html",
    "title": "3  Pandas II, EDA",
    "section": "",
    "text": "3.1 Dealing with Missing Data\nWhile exploring a Berkeley dataset (separate from babynames) with a million records, you realize that a portion of measurements in different fields are NaN values! You decide to impute these missing values before continuing your EDA. Given the empirical distribution of each of the below variables, determine how to solve the missing data problem. (Note that the data in these graphs are fictional).\nSuppose that you plot “cups of coffee sold at V&A Cafe per day” versus “inches of rain per day” across a period of 2 months, shown below. V&A Cafe is not missing any data, but 30% of the data in “inches of rain” are NaN values that have been represented with “-2”, an impossible amount of rain. Which of the following techniques would be most effective in solving the issue of missing data? (Select all that apply)\nA. Using the mean to impute the missing values\nB. Using the mode(s) to impute the missing values\nC. Using the median to impute the missing values\nD. Dropping any rows with missing values\nE. Imputing missing values through interpolation\nSuppose we examine the amount of money lost/gained in a game of poker and see that this variable is missing 1% of its values. Its distribution, shown below, is constructed from all valid (non-NaN) values. Which of the following techniques would be reasonably effective in solving this issue of missing data? (Select all that apply)\nA. Using the mean to impute the missing values\nB. Using the mode(s) to impute the missing values\nC. Using the median to impute the missing values\nD. Dropping any rows with missing values\nE. Imputing missing values through interpolation\nSuppose that the relationship between students’ time asleep (in hours) and the amount of extra credit they received in Data 100 is shown below. There is no missing data for “hours asleep”, but 0.5% of “extra credit score” is missing. Like in part a, the missing NaN values were replaced with an impossible score of -0.002, making the graph look funky. Which of the following techniques would be most effective in solving this issue of missing data? (Select all that apply)\nA. Using the mean to impute the missing values\nB. Using the mode(s) to impute the missing values\nC. Using the median to impute the missing values\nD. Dropping any rows with missing values\nE. Imputing missing values through interpolation",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Pandas II, EDA</span>"
    ]
  },
  {
    "objectID": "disc03/disc03.html#dealing-with-missing-data",
    "href": "disc03/disc03.html#dealing-with-missing-data",
    "title": "3  Pandas II, EDA",
    "section": "",
    "text": "Question 1a\n\n\n\n\n\n\n\n\n\nAnswer\n\nCorrect Options: A, C, E\n\n\n\n\n\nQuestion 1b graph\n\n\n\n\n\n\n\n\n\nAnswer\n\nCorrect Options: B, D\n\n\n\n\n\nQuestion 1c graph\n\n\n\n\n\n\n\n\n\nAnswer\n\nCorrect Options: D, E",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Pandas II, EDA</span>"
    ]
  },
  {
    "objectID": "disc03/disc03.html#pandas-eda-exam-prep-modeled-after-fa22-midterm-q1",
    "href": "disc03/disc03.html#pandas-eda-exam-prep-modeled-after-fa22-midterm-q1",
    "title": "3  Pandas II, EDA",
    "section": "3.2 Pandas + EDA exam prep (modeled after Fa22 Midterm Q1)",
    "text": "3.2 Pandas + EDA exam prep (modeled after Fa22 Midterm Q1)\nIt’s the annual Monopoly World Championship! The finalists: Shawn, Amanda, Neil, and Annie are playing Monopoly, a board game where players pay a price to buy properties, which can then generate income for them. Each property can be owned by only one player at a time. At the end of the game, the player with the most money wins.\nShawn wants to figure out which properties are most worth buying. He creates a DataFrame income with data on the current game state, shown on the left. He also finds a DataFrame properties with data on Monopoly properties, shown on the right.\nBoth tables have 28 rows. For brevity, only the first few rows of each DataFrame are shown.\n\n\nCode\n# First DataFrame: income\ndata_income = {\n    'Player': ['Shawn', 'Amanda', 'Neil', np.nan, 'Shawn', 'Annie', 'Amanda'],\n    'Property': ['Boardwalk', 'Park Place', 'Marvin Gardens', 'Kentucky Ave', 'Pennsylvania Ave', 'Oriental Ave', 'Baltic Ave'],\n    'Income Generated': ['$425', '$375', '$200', np.nan, '$150', '$50', '$60']\n}\n\nincome = pd.DataFrame(data_income)\nincome\n\n\n\n\n\n\n\n\n\nPlayer\nProperty\nIncome Generated\n\n\n\n\n0\nShawn\nBoardwalk\n$425\n\n\n1\nAmanda\nPark Place\n$375\n\n\n2\nNeil\nMarvin Gardens\n$200\n\n\n3\nNaN\nKentucky Ave\nNaN\n\n\n4\nShawn\nPennsylvania Ave\n$150\n\n\n5\nAnnie\nOriental Ave\n$50\n\n\n6\nAmanda\nBaltic Ave\n$60\n\n\n\n\n\n\n\nincome\n\nPlayer is the name of the player, as a str.\nProperty is a property currently owned by the player, as a str.\nIncome Generated is the amount of income a player has earned from that property so far, as a str.\n\n\n\nCode\n# Second DataFrame: properties\ndata_properties = {\n    'Property': ['Park Place', 'Oriental Ave', 'Vermont Ave', 'Pacific Ave', 'Boardwalk', 'Illinois Ave', 'Atlantic Ave'],\n    'Property Color': ['Dark Blue', 'Light Blue', 'Light Blue', 'Green', 'Dark Blue', 'Red', 'Yellow'],\n    'Purchase Price': [350.0, 100.0, 100.0, 300.0, 400.0, 240.0, 260.0]\n}\n\nproperties = pd.DataFrame(data_properties)\nproperties\n\n\n\n\n\n\n\n\n\nProperty\nProperty Color\nPurchase Price\n\n\n\n\n0\nPark Place\nDark Blue\n350.0\n\n\n1\nOriental Ave\nLight Blue\n100.0\n\n\n2\nVermont Ave\nLight Blue\n100.0\n\n\n3\nPacific Ave\nGreen\n300.0\n\n\n4\nBoardwalk\nDark Blue\n400.0\n\n\n5\nIllinois Ave\nRed\n240.0\n\n\n6\nAtlantic Ave\nYellow\n260.0\n\n\n\n\n\n\n\nproperties\n\nProperty is the name of the property, as a str. There are 28 unique properties.\nProperty Color is a color group that the property belongs to, as a str. There are 10 unique color groups, and each property belongs to a single group.\nPurchase Price is the price to buy the property, as a float.\n\nNote: For the properties that are not currently owned by any player, the Player and Income Generated columns in the income table have a NaN value.\n(a) What is the granularity of the income table?\n\n\nAnswer\n\n\nProperty\n\nEach unique property has its own row\nNotice how one player can have own multiple properties and can appear in multiple rows! This tells us that the granularity of this table is not Player.\n\n\n(b) Consider the Player and Purchase Price variables. What type of variable is each one? (quantitative, qualitative nominal, qualitative ordinal)\n\n\nAnswer\n\n\nPlayer: Qualitative nominal\nPurchase Price: Quantitative\n\n\n(c) Which of the following line(s) of code successfully returns a Series with the number of properties each player owns? Select all that apply.\n\n\nAnswer\n\n\nincome[\"Player\"].value_counts()\nincome.groupby(\"Player\").size()\n\n\n\n\nCode\nincome.groupby(\"Player\").agg(pd.value_counts)\n\n\n\n\n\n\n\n\n\nProperty\nIncome Generated\n\n\nPlayer\n\n\n\n\n\n\nAmanda\n[1, 1]\n[1, 1]\n\n\nAnnie\n1\n1\n\n\nNeil\n1\n1\n\n\nShawn\n[1, 1]\n[1, 1]\n\n\n\n\n\n\n\n\n\nCode\nincome[\"Player\"].value_counts()\n\n\nPlayer\nShawn     2\nAmanda    2\nNeil      1\nAnnie     1\nName: count, dtype: int64\n\n\n\n\nCode\n# income[\"Player\", \"Property\"].groupby(\"Player\").size()\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe above code will error! Make sure to use double brackets when selecting columns.\n\n\n\n\nCode\nincome.groupby(\"Player\")[[\"Player\"]].count()\n\n\n\n\n\n\n\n\n\nPlayer\n\n\nPlayer\n\n\n\n\n\nAmanda\n2\n\n\nAnnie\n1\n\n\nNeil\n1\n\n\nShawn\n2\n\n\n\n\n\n\n\n(d) He now decides to calculate the amount of profit from each property. He wants to store this in a column called Profit in the income DataFrame. To do this, he first has to transform the Income Generated column to be of a float datatype.\nWrite one line of code to replace the old column with a new column, also called Income Generated, with the datatype modification described above. You may assume that each entry in Income Generated consists of a dollar sign ($) followed by a number, except for the NaN values.\n\n\nCode\nincome[\"Income Generated\"] = income[\"Income Generated\"].str[1:].astype(float)\n\n\n(e) Assuming that the answer to (d) is correct, let’s add a Profit column to the income DataFrame. Fill in the following blanks to do this, and please add arguments to function class as you see appropriate.\nNote: Profit is calculated by subtracting the purchase price from generated income.\ncombined_df = income._____A_____(_______B_______)\nincome[\"Profit\"] = _______C_______\n\n\nCode\ncombined_df = income.merge(properties, on = \"Property\")\nincome[\"Profit\"] = combined_df[\"Income Generated\"] - combined_df[\"Purchase Price\"]\n\n\nShawn realizes he’s lost more money than he’s made. To solve this problem, he begins by writing some Pandas code to merge the Property Color column into the income DataFrame and drops all rows with NaN values. He calls this DataFrame merged_df. Shown below are the first few rows.\n\n\nCode\nmerged_df = pd.DataFrame({\"Player\": [\"Shawn\", \"Amanda\", \"Neil\", \"Shawn\", \"Annie\", \"Amanda\"],\n                          \"Property\": [\"Boardwalk\", \"Park Place\", \"Marvin Gardens\", \"Pennsylvania Ave\", \"Oriental Ave\", \"Baltic Ave\"],\n                          \"Income Generated\": [425., 375., 200., 150., 50., 60.],\n                          \"Profit\": [-25., 25., 50., -100., 0., 0.],\n                          \"Property Color\": [\"Dark Blue\", \"Dark Blue\", \"Yellow\", \"Green\", \"Light Blue\", \"Purple\"]})\nmerged_df\n\n\n\n\n\n\n\n\n\nPlayer\nProperty\nIncome Generated\nProfit\nProperty Color\n\n\n\n\n0\nShawn\nBoardwalk\n425.0\n-25.0\nDark Blue\n\n\n1\nAmanda\nPark Place\n375.0\n25.0\nDark Blue\n\n\n2\nNeil\nMarvin Gardens\n200.0\n50.0\nYellow\n\n\n3\nShawn\nPennsylvania Ave\n150.0\n-100.0\nGreen\n\n\n4\nAnnie\nOriental Ave\n50.0\n0.0\nLight Blue\n\n\n5\nAmanda\nBaltic Ave\n60.0\n0.0\nPurple\n\n\n\n\n\n\n\nShawn decides he will now only buy properties from a color group that he deems “profitable.” He deems a color group “profitable” if at least 50% of the properties in the group that are currently owned by players have made a positive (non-zero) profit for those players.\nFill in the following lines of code to help him display a DataFrame with a subset of the rows in merged_df: the rows with properties that belong to profitable color groups. Your solution may use fewer lines of code than we provide.\n\n\nCode\ndef func(group):\n    if np.mean(group[\"Profit\"] &gt; 0) &gt;= 0.5:\n        return True\n    return False\n\nmerged_df.groupby(\"Property Color\").filter(func)\n\n\n\n\n\n\n\n\n\nPlayer\nProperty\nIncome Generated\nProfit\nProperty Color\n\n\n\n\n0\nShawn\nBoardwalk\n425.0\n-25.0\nDark Blue\n\n\n1\nAmanda\nPark Place\n375.0\n25.0\nDark Blue\n\n\n2\nNeil\nMarvin Gardens\n200.0\n50.0\nYellow",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Pandas II, EDA</span>"
    ]
  },
  {
    "objectID": "disc04/disc04.html",
    "href": "disc04/disc04.html",
    "title": "4  Regex, Visualization, and Transformation",
    "section": "",
    "text": "4.1 Regular Expressions\nRegular Expressions (RegEx for short) are an immensely powerful tool for parsing strings. However, it’s many rules make RegEx very confusing, even for veteran users, so please don’t hesitate to ask questions! Here’s a snippet of the RegEx portion of the Fall 2023 Midterm\nReference Sheet:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regex, Visualization, and Transformation</span>"
    ]
  },
  {
    "objectID": "disc04/disc04.html#regular-expressions",
    "href": "disc04/disc04.html#regular-expressions",
    "title": "4  Regex, Visualization, and Transformation",
    "section": "",
    "text": "Regex Reference\n\n\n\n4.1.1 (a)\nWhich string contains a match for the following regular expression, “\\(\\texttt{1+1\\$}\\)”?\n\n\\(\\texttt{What is 1+1}\\)\n\\(\\texttt{Make a wish at 11:11}\\)\n\\(\\texttt{111 Ways to Succeed}\\)\n\n\n\nAnswer\n\n\n\\(\\texttt{Make a wish at 11:11}\\)\n\nNote that there are two operators here, the + and the $. The $ operator states that our match must occur at the end of the string, so that already rules out the third option. Furthermore, the + operator indicates that we need “one or more 1s” which is followed by another 1 (so two or more 1s in total). The only string ending with two or more consecutive 1s is the second option.\n\n\n\n4.1.2 (b)\nWrite a regular expression that matches a string which contains only one word containing only lowercase letters and numbers (including the empty string).\n\n\nAnswer\n\n\\(\\texttt{\\^{}[a-z0-9]*\\$}\\)\n\n\\(\\texttt{[a-z0-9]}\\) to indicate that the word only include lowercase letters and/or numbers\n\\(\\texttt{*}\\) operator to state that the word can contain “0 or more” of \\(\\texttt{[a-z0-9]}\\). Note that we also want to match empty strings, which is why \\(\\texttt{*}\\) is preferred over \\(\\texttt{+}\\) here.\n\\(\\texttt{\\^{}}\\) and \\(\\texttt{\\$}\\) to indicate that the pattern must match the string at the beginning of the string, and at the end of the string respectively. This ensures that the pattern only matches strings with one word (not two or more words).\n\n\n\n\n4.1.3 (c)\nGiven \\(\\texttt{sometext = \"I've got 10 eggs, 20 gooses, and 30 giants.\"}\\), use \\(\\texttt{re.findall}\\) to extract all items and quantities from the string. The result should look like \\(\\texttt{[\"10 eggs\", \"20 gooses\", \"30 giants\"]}\\). You may assume that a space separates quantity and type, and that each item ends in s.\n\n\nAnswer\n\n\\(\\texttt{re.findall(r\"\\\\d+\\\\s\\\\w+\", sometext)}\\)\nThe strings we want to match begin with a number, followed by a space, followed by a word. We can use this to construct our pattern.\n\n\\(\\texttt{\\\\d+}\\) to match one or more digits\n\\(\\texttt{\\\\s}\\) to match a single space\n\\(\\texttt{\\\\w+}\\) to match one or more word characters \\(\\texttt{[A-Za-z0-9\\_]}\\)\n\n\n\n\n4.1.4 (d)\nFor each pattern specify the starting and ending position of the first match in the string. The index starts at zero and we are using closed intervals (both endpoints are included).\n\n\n\n\n\n\n\n\n\n\n\n\\(\\texttt{abcdefg}\\)\n\\(\\texttt{abcs!}\\)\n\\(\\texttt{ab abc}\\)\n\\(\\texttt{abc, 123}\\)\n\n\n\n\n\\(\\texttt{abc*}\\)\n[0,2]\n\n\n\n\n\n\\(\\texttt{[ \\^{} \\\\s]+}\\)\n\n\n\n\n\n\n\\(\\texttt{ab.*c}\\)\n\n\n\n\n\n\n\\(\\texttt{[a-z1,9]+}\\)\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\texttt{abcdefg}\\)\n\\(\\texttt{abcs!}\\)\n\\(\\texttt{ab abc}\\)\n\\(\\texttt{abc, 123}\\)\n\n\n\n\n\\(\\texttt{abc*}\\)\n[0,2]\n[0,2]\n[0,1]\n[0,2]\n\n\n\\(\\texttt{[ \\^{} \\\\s]+}\\)\n[0,6]\n[0,4]\n[0,1]\n[0,3]\n\n\n\\(\\texttt{ab.*c}\\)\n[0,2]\n[0,2]\n[0,5]\n[0,2]\n\n\n\\(\\texttt{[a-z1,9]+}\\)\n[0,6]\n[0,3]\n[0,1]\n[0,3]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regex, Visualization, and Transformation</span>"
    ]
  },
  {
    "objectID": "disc04/disc04.html#visualizing-bigfoot",
    "href": "disc04/disc04.html#visualizing-bigfoot",
    "title": "4  Regex, Visualization, and Transformation",
    "section": "4.2 Visualizing Bigfoot",
    "text": "4.2 Visualizing Bigfoot\nMany of you have probably heard of Bigfoot before. It’s a mysterious ape-like creature that is said to live in North American forests. Most doubt its existence, but a passionate few swear that Bigfoot is real. In this discussion, you will be working with a dataset on Bigfoot sightings, visualizing variable distributions and combinations thereof to better understand how/when/where Bigfoot is reportedly spotted, and possibly either confirm or cast doubt on its existence. The Bigfoot data contains a ton of variables about each reported Bigfoot spotting, including location information, weather, and moon phase.\n\n\nCode\n# Importing packages\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n\nCode\n# Loading bigfoot data\n\nurl = 'https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-09-13/bigfoot.csv'\ndf = pd.read_csv(url)\n\n\nThis dataset is extremely messy, with observations missing many values across multiple columns. This is normally the case with data based on citizen reports (many do not fill out all required fields). For the purposes of this discussion, we will drop all observations with any missing values and some unneeded columns. However, note this is not a good practice and you should almost never do this in real life!\n\n\nCode\n# Drop unneeded rows and observations with missing values\n\nbigfoot = df.dropna().rename({'temperature_high':'temp_high' ,'temperature_low':'temp_low'},axis = 1)\nbigfoot = bigfoot.drop(['observed', 'location_details', 'county', 'state', 'title',\n       'latitude', 'longitude', 'number', 'classification', 'geohash',\n       'temperature_mid', 'dew_point','precip_probability', 'precip_type','summary', \n       'wind_bearing'], axis = 1)\n\n\nHere are the first few entries of the bigfoot table:\n\n\nCode\nbigfoot.head(5)\n\n\n\n\n\n\n\n\n\nseason\ndate\ntemp_high\ntemp_low\nhumidity\ncloud_cover\nmoon_phase\nprecip_intensity\npressure\nuv_index\nvisibility\nwind_speed\n\n\n\n\n10\nSummer\n2016-06-07\n74.69\n53.80\n0.79\n0.61\n0.10\n0.0010\n998.87\n6.0\n9.70\n0.49\n\n\n21\nSummer\n2015-10-02\n49.06\n44.24\n0.87\n0.93\n0.67\n0.0092\n1022.92\n3.0\n9.16\n2.87\n\n\n32\nFall\n2009-10-31\n69.01\n34.42\n0.77\n0.81\n0.42\n0.0158\n1011.48\n3.0\n1.97\n3.94\n\n\n34\nSummer\n1978-07-15\n68.56\n63.05\n0.88\n0.80\n0.33\n0.0285\n1014.70\n5.0\n5.71\n5.47\n\n\n55\nSummer\n2015-11-26\n20.49\n5.35\n0.65\n0.08\n0.54\n0.0002\n1037.98\n1.0\n10.00\n0.40\n\n\n\n\n\n\n\nLet’s first look at distributions of individual quantitative variables. Let’s say we’re interested in wind_speed.\n\n4.2.1 (a)\nWhich of the following are appropriate visualizations for plotting the distribution of a quantitative variable? (Select all that apply.)\nA. Pie charts\nB. Kernel Density Plot\nC. Scatter plot\nD. Box plot\nE. Histogram\nF. Hex plot\n\n\n\nAnswer\n\nKernel Density Plot, Box plot, Histogram\nA Pie chart would not be appropriate because they are used to visualize the distribution of categories, or of a single qualitative variables. Scatter plots and Hex plots are also not appropriate as they visualize the relationship between two quantitative variables.\n\n\n4.2.2 (b)\nWrite a line of code that produces a visualization that depicts the variable’s distribution (example shown below).\n\n\n\nwindspeed\n\n\n\n\nCode\nsns.histplot(data = bigfoot, x = \"wind_speed\", kde = True);\n\n\n\n\n\n\n\n\n\nNote that kde = True is required to overlay the KDE curve over the actual histogram!\n\n\n4.2.3 (c)\nNow, let’s look at some qualitative variables. Write a line of code that produces a visualization that shows the distribution of bigfoot sightings across the variable season (example shown below).\nHint: Use seaborn’s sns.countplot or matplotlib’s plt.bar.\n\n\n\nCode\nsns.countplot(data = bigfoot, x = \"season\");\n\n\n\n\n\n\n\n\n\nIn order to also replicate the colors of the bars, you would need to manually specify them.\n\n\nCode\nsns.countplot(data = bigfoot, x = \"season\", palette = [\"blue\", \"orange\", \"green\", \"red\", \"purple\"]);\n\n\n\n\n\n\n\n\n\nYou could have alternatively used Matplotlib!\n\n\nCode\nseason_counts = bigfoot[\"season\"].value_counts()\nplt.bar(season_counts.index, season_counts.values, color = [\"blue\", \"orange\", \"green\", \"red\", \"purple\"]);\n\n\n\n\n\n\n\n\n\n\n\n4.2.4 (d)\nFinally, produce a single visualization that showcases how the prevalence of bigfoot sightings at particular combinations of moon_phase and wind_speed vary across each season.\nHint: Think about color as the third information channel in the plot.\n\n\n\nCode\nsns.scatterplot(data = bigfoot,\n                x = \"moon_phase\",\n                y = \"wind_speed\",\n                hue = \"season\",\n                alpha = 0.2);\n\n\n\n\n\n\n\n\n\nNote the two rather unfamiliar arguments:\n\nhue specifies which column of bigfoot we want to color the points according to\nalpha specifies how transparent the points should be. Higher values of alpha lead to more opaque points.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regex, Visualization, and Transformation</span>"
    ]
  },
  {
    "objectID": "disc04/disc04.html#kernel-density-estimation-kde",
    "href": "disc04/disc04.html#kernel-density-estimation-kde",
    "title": "4  Regex, Visualization, and Transformation",
    "section": "4.3 Kernel Density Estimation (KDE)",
    "text": "4.3 Kernel Density Estimation (KDE)\nKernel Density Estimation is used to estimate a probability density function (or density curve) from a set of data. A kernel with a bandwidth parameter α is placed on data observations \\(x_i\\) with \\(i ∈ \\{1, ..., n\\}\\), and the density estimation is calculated by averaging all kernels. Below, Gaussian and Boxcar kernel equations are listed:\n\nGaussian Kernel: \\(K_{\\alpha}(x, x_i) = \\frac{1}{\\sqrt{2\\pi\\alpha^2}}\\exp({-\\frac{(x-x_i)^2}{2\\alpha^2}})\\)\nBoxcar Kernel: \\(B_{\\alpha}(x, x_i) = \\begin{cases}\n    \\frac{1}{\\alpha} & \\text{ if }-\\frac{\\alpha}{2} \\leq x - x_i \\leq \\frac{\\alpha}{2} \\\\\n    0 & \\text{ else}\n\\end{cases}\\)\n\nThe KDE is calculated as follows: \\(f_\\alpha(x) = \\frac{1}{n}\\sum_{i = 1}^{n} K_\\alpha(x, x_i)\\).\n\n4.3.1 (a)\nDraw a KDE plot (by hand is fine) for data points [1, 4, 8, 9] using Gaussian Kernel and \\(\\alpha = 1\\). On the plot show \\(x\\), \\(x_i\\), \\(\\alpha\\), and the KDE.\n\n\nAnswer\n\nWith \\(\\alpha = 1\\), we get a Gaussian Kernel of \\(K_{1}(x, x_i) = \\frac{1}{\\sqrt{2 \\pi}} \\exp\\left(-\\frac{(x - x_i)^2}{2} \\right)\\).\nThis kernel is greatest when \\(x = x_i\\), giving us maximum point at \\[K_{1}(x, x) = \\frac{1}{\\sqrt{2 \\pi}} = 0.3989 \\approx 0.4\\]\nEach individual kernel is a Gaussian centered, respectively, at . Since we have 4 kernels, each with an area of 1, we normalize by dividing each kernel by 4. This gives us a maximum height of \\(0.1\\). We then sum those kernels together to obtain the final KDE plot:\n\n\n\nKDE\n\n\n\n\n\n4.3.2 (b)\nWe wish to compare the results of KDE using a Gaussian kernel and a boxcar kernel. For \\(\\alpha &gt; 0\\), which of the following statements is true? Choose all that apply.\nA. Decreasing \\(\\alpha\\) for a Gaussian kernel decreases the smoothness of the KDE.\nB. The Gaussian kernel is always better than the boxcar kernel for KDEs.\nC. Because the Gaussian kernel is smooth, we can safely use large \\(\\alpha\\) values for kernel density estimation without worrying about the actual distribution of data.\nD. The area under the boxcar kernel is 1, regardless of the value of \\(\\alpha\\).\nE. None of the above.\n\n\nAnswer\n\nCorrect options: A, D\nB is false because a boxcar kernel can perform better is \\(\\alpha\\) is not chosen properly for the Gaussian kernel\nC is false because an \\(\\alpha\\) that is too high risks including too many points in the estimate, resulting in a flatter curve.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regex, Visualization, and Transformation</span>"
    ]
  },
  {
    "objectID": "disc05/disc05.html",
    "href": "disc05/disc05.html",
    "title": "5  Transformations, Sampling, and SLR",
    "section": "",
    "text": "5.1 Logarithmic Transformations\nIshani is a development economist interested in studying the relationship between literacy rates and gross national income in countries across the world. Originally, she plotted the data on a linear (absolute) scale, shown on the left. She noticed that the non-linear relationship between the variables with a lot of points clustered towards the larger values of literacy rate, so she consults the Tukey-Mosteller Bulge diagram and decides to do a \\(\\log_{10}\\) transformation of the \\(y\\)-axis, shown on the right. The solid blue line is a “line of best fit” (we’ll formalize this later in the course).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transformations, Sampling, and SLR</span>"
    ]
  },
  {
    "objectID": "disc05/disc05.html#logarithmic-transformations",
    "href": "disc05/disc05.html#logarithmic-transformations",
    "title": "5  Transformations, Sampling, and SLR",
    "section": "",
    "text": "log transform plot\n\n\n\n5.1.1 (a)\nInstead of using the \\(\\log_{10}\\) transformation of the \\(y\\)-axis, what other transformations could Ishani have used to attempt to linearize the relationship between literacy rate (\\(x\\)) and gross national income per capita (\\(y\\)). Select all that apply.\nA. \\(\\log_e(y)\\)\nB. \\(10^y\\)\nC. \\(\\sqrt{x}\\)\nD. \\(x^2\\)\nE. \\(y^2\\)\n\n\nAnswer\n\n\n\\(\\log_e(y)\\), \\(x^2\\)\n\nThe original plot displays a very strong non-linear relationship that looks exponential. For large values of \\(x\\), and increase in \\(x\\) is matched with a very significant increase in \\(y\\). Therefore, we would want to apply a transformation that makes \\(x\\) values larger or \\(y\\) values smaller.\n\\(x^3\\), \\(\\sqrt{y}\\), and \\(\\log(y)\\) are also valid as suggested by the Tukey-Mosteller Bulge Diagram.\n\n\n\n5.1.2 (b)\nLet \\(C\\) and \\(k\\) be some constant values and \\(x\\) and \\(y\\) represent literacy rate and gross national income per capita, respectively. Based on the plots, which of the following best describes the pattern seen in the data?\nA. \\(y = C + kx\\)\nB. \\(y = C*10^{kx}\\)\nC. \\(y = C + k\\log_{10}(x)\\)\nD. \\(y = Cx^k\\)\n\n\nAnswer\n\n\n\\(y = C*10^{kx}\\)\n\nThe basic format of a regression line is \\(y = kx + b\\). Noticed that the plot on the right applied a log transformation to the \\(y\\) axis, so we can apply that same transformation to the equation to derive the true relationship.\n\\[\\begin{align}\n\\log_{10}(y) &= kx + b\\\\\ny &= 10^{kx + b}\\\\\ny &= 10^b*10^{kx}\n\\end{align}\\]\nwhere \\(C = 10^b\\)\n\n\n\n5.1.3 (c)\nWhat parts of the plot could you use to make initial guesses on \\(C\\) and \\(k\\)?\n\n\nAnswer\n\n\n\\(C\\): \\(b\\) is the y-intercept of the transformed plot, and \\(C = 10^b\\)\n\\(k\\): Slope of the regression line in the transformed plot\n\n\n\n\n5.1.4 (d)\nIshani’s friend, Yash, points to the solid line on the transformed plot and says “since this line is going up and to the right, we can say that, in general, the higher the literacy rate, the greater the gross national income per capita”. Is this a reasonable interpretation of the plot?\n\n\nAnswer\n\nYes, the observation is equivalent to saying that the slope is positive, which means increases in \\(x\\) correspond to increases in \\(y\\). This does not mean higher literacy rates cause higher gross national incomes, just that they are positively correlated.\n\n\n\n5.1.5 (e)\nSuppose that instead of plotting positive quantities, our data contained some zero and negative values. How can we reasonably apply a logarithmic transform to this data?\n\n\nAnswer\n\nRecall that logarithms are not defined for 0 or negative values. Thus, we must first make all of our values positive. Suppose our data consists of three points, \\([-3, -2, 4]\\).\n\nAdd the magnitude of the smallest number to each value to make all values non-negative: \\([0, 1, 7]\\)\nAdd a small positive number to each value (e.g., \\(1\\)) to make all values positive: \\([1, 2, 8]\\)\n\nNow, it is possible to take the logarithm of each value! Note that the steps above are merely shifting the data, they are not changing any underlying linear or non-linear relationships.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transformations, Sampling, and SLR</span>"
    ]
  },
  {
    "objectID": "disc05/disc05.html#data-collection-through-sampling",
    "href": "disc05/disc05.html#data-collection-through-sampling",
    "title": "5  Transformations, Sampling, and SLR",
    "section": "5.2 Data Collection through Sampling",
    "text": "5.2 Data Collection through Sampling\nIt’s time for the Data 100 midterm, and the professors want to estimate the difficulty of the exam. They decided to survey students on the exam’s difficulty with a 10-point scale and then use the mean of the student’s responses as the estimate.\n\n5.2.1 (a)\nWhat is the population the professors are interested in trying to understand?\nA. Students in Data 100\nB. Students enrolled in the Data 100 Ed\nC. Students who attend the Data 100 lectures\nD. Students who took the Data 100 midterm\n\n\nAnswer\n\nD. Students who took the Data 100 midterm\nThe professors are only interested in the students who actually took the midterm. Some students in the first three options might not have taken the midterm exam!\n\n\n\n5.2.2 (b)\nThe professors consider a few different methods for collecting the survey data about the midterm. Which of the following methods is best? (think through which considerations go into “best”)\nA. The professors add one Slido poll in the first lecture following the exam and only consider synchronous responses.\nB. The professors add a question to the homework assignments of a simple random sample of students within every discussion section.\nC. The professors make a post on Ed asking students to submit a Google Form containing the survey question.\nD. The professors choose a simple random sample of discussion sections, go to each selected section and ask each student in the group as part of the final discussion question.\n\n\nAnswer\n\nB. The professors add a question to the homework assignments of a simple random sample of students within every discussion section.\nAlthough not perfect, B describes the best method out of the four! This method samples randomly from uniformly from students across discussion times, as well as providing strong incentive for students to answer. The only issue would be the fact that the sampling frame does not include students who don’t have a discussion section.\nHere are reasons why the other options are not as good:\n\nA: Sample only from students who attend synchronously, introducing selection bias.\nC: Not all students check Ed periodically and the survey is optional, introducing selection bias and non-response bias.\nD: Room for social pressure, which can bias the results of the survey. Also, there may be systematic or inherent differences between discussion sections.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transformations, Sampling, and SLR</span>"
    ]
  },
  {
    "objectID": "disc05/disc05.html#simple-linear-regression",
    "href": "disc05/disc05.html#simple-linear-regression",
    "title": "5  Transformations, Sampling, and SLR",
    "section": "5.3 Simple Linear Regression",
    "text": "5.3 Simple Linear Regression\nLillian and Prabhleen were watching their favorite chemistry Youtuber NileRed experimenting with turning gloves into grape soda and wanted to try it themselves. The experiment was done at various temperatures and yielded various amounts of grape soda. Since this reaction is very costly, they were only able to do it 10 times. This data set of size \\(n = 10\\) (Yield data) contains measurements of yield from an experiment done at five different temperature levels. The variables are \\(y\\) = yield in liters and \\(x\\) = temperature in degrees Fahrenheit. Below is a scatter plot of our data. \n\n\n\n\\(\\sigma_x\\)\n\\(\\sigma_y\\)\n\\(r\\)\n\\(\\bar{x}\\)\n\\(\\bar{y}\\)\n\n\n\n\n15\n0.3\n0.50\n75.00\n3\n\n\n\n\n5.3.1 (a)\nGiven the above statistics, calculate the slope (\\(\\hat{\\theta_1}\\)) and y-intercept (\\(\\hat{\\theta_0}\\)) of the line of best fit using Mean Squared Error (MSE) as our loss function and plot the line on the graph above:\n\\[\\begin{align*}\ny = \\hat{\\theta_0} + \\hat{\\theta_1}x\n\\end{align*}\\]\n\n\nAnswer\n\n\\[\\begin{align*}\n\\hat{\\theta_1} &= r*\\frac{\\sigma_y}{\\sigma_x}\\\\\n\\hat{\\theta_1} &= 0.5*\\frac{0.3}{15}\\\\\n&= 0.01\\\\\\\\\n\\hat{\\theta_0} &= \\bar{y} - \\hat{\\theta_1}\\bar{x}\\\\\n\\hat{\\theta_0} &= 3 - 75.00 * 0.01\\\\\n&= 3 - 0.75\\\\\n&= 2.25\n\\end{align*}\\]\n\n\n\nYield Temp Regression Line\n\n\nNote that the \\(\\sigma_x\\) and \\(\\sigma_y\\) values in the table are slightly different from what can be seen in the plot in order to make the calculations easier! As a result, the slope and intercept values you obtained may also be slightly different from what you see in the plot.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transformations, Sampling, and SLR</span>"
    ]
  }
]