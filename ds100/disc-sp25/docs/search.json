[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data 100 Discussion Notes",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>disc-sp25</span>"
    ]
  },
  {
    "objectID": "disc02/disc02.html",
    "href": "disc02/disc02.html",
    "title": "2  Pandas I",
    "section": "",
    "text": "This discussion is all about practicing using pandas, and testing your knowledge about its various functionalities to accomplish small tasks.\nWe will be using the elections dataset from lecture.\n\n# import packages\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\nelections = pd.read_csv('elections.csv')\nelections.head(10)\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\n\n\n1\n1824\nJohn Quincy Adams\nDemocratic-Republican\n113142\nwin\n42.789878\n\n\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\n\n\n3\n1828\nJohn Quincy Adams\nNational Republican\n500897\nloss\n43.796073\n\n\n4\n1832\nAndrew Jackson\nDemocratic\n702735\nwin\n54.574789\n\n\n5\n1832\nHenry Clay\nNational Republican\n484205\nloss\n37.603628\n\n\n6\n1832\nWilliam Wirt\nAnti-Masonic\n100715\nloss\n7.821583\n\n\n7\n1836\nHugh Lawson White\nWhig\n146109\nloss\n10.005985\n\n\n8\n1836\nMartin Van Buren\nDemocratic\n763291\nwin\n52.272472\n\n\n9\n1836\nWilliam Henry Harrison\nWhig\n550816\nloss\n37.721543\n\n\n\n\n\n\n\nWrite a line of code that returns the elections table sorted in descending order by \"Popular vote\". Store your result in a variable named sorted. Would calling sorted.iloc[[0], :] give the same result as sorted.loc[[0], :]?\n\n\nCode\nsorted = elections.sort_values(\"Popular vote\", ascending = False)\nsorted\n\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n178\n2020\nJoseph Biden\nDemocratic\n81268924\nwin\n51.311515\n\n\n182\n2024\nDonald Trump\nRepublican\n77303568\nwin\n49.808629\n\n\n183\n2024\nKamala Harris\nDemocratic\n75019230\nloss\n48.336772\n\n\n179\n2020\nDonald Trump\nRepublican\n74216154\nloss\n46.858542\n\n\n162\n2008\nBarack Obama\nDemocratic\n69498516\nwin\n53.023510\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n110\n1956\nT. Coleman Andrews\nStates' Rights\n107929\nloss\n0.174883\n\n\n141\n1992\nBo Gritz\nPopulist\n106152\nloss\n0.101918\n\n\n99\n1948\nClaude A. Watson\nProhibition\n103708\nloss\n0.212747\n\n\n89\n1932\nWilliam Z. Foster\nCommunist\n103307\nloss\n0.261069\n\n\n6\n1832\nWilliam Wirt\nAnti-Masonic\n100715\nloss\n7.821583\n\n\n\n\n187 rows × 6 columns\n\n\n\n\n\nExplanation\n\n\n\nWe can sort a DataFrame by a column using the .sort_values() function! Remember to specify ascending = False, or else it will sort in increasing order.\n\n\n\nsorted.iloc[[0], :]\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n178\n2020\nJoseph Biden\nDemocratic\n81268924\nwin\n51.311515\n\n\n\n\n\n\n\n\nsorted.loc[[0], :]\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\n\n\n\n\n\n\n\n\n\nExplanation\n\n \n\nThe difference is that .loc[] uses label-based indexing, while .iloc[] uses integer position-based indexing. Using .loc[] will simply grab the row with the label 0 regardless of where it is, while .iloc[] will grab the first row of the sorted DataFrame.\n\n\n\nUsing Boolean slicing, write one line of pandas code that returns a DataFrame that only contains election results from the 1900s.\n\n\nCode\nelections[(elections[\"Year\"] &gt;= 1900) & (elections[\"Year\"] &lt; 2000)]\n\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n54\n1900\nJohn G. Woolley\nProhibition\n210864\nloss\n1.526821\n\n\n55\n1900\nWilliam Jennings Bryan\nDemocratic\n6370932\nloss\n46.130540\n\n\n56\n1900\nWilliam McKinley\nRepublican\n7228864\nwin\n52.342640\n\n\n57\n1904\nAlton B. Parker\nDemocratic\n5083880\nloss\n37.685116\n\n\n58\n1904\nEugene V. Debs\nSocialist\n402810\nloss\n2.985897\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n146\n1996\nHarry Browne\nLibertarian\n485759\nloss\n0.505198\n\n\n147\n1996\nHoward Phillips\nTaxpayers\n184656\nloss\n0.192045\n\n\n148\n1996\nJohn Hagelin\nNatural Law\n113670\nloss\n0.118219\n\n\n149\n1996\nRalph Nader\nGreen\n685297\nloss\n0.712721\n\n\n150\n1996\nRoss Perot\nReform\n8085294\nloss\n8.408844\n\n\n\n\n97 rows × 6 columns\n\n\n\n\n\nExplanation\n\n \n\nWe can “filter” DataFrames by using boolean slicing! 1. Construct a boolean Series that is True if a row contains election results from the 1900s, and False otherwise. * We can use the & (and) logical operator! 1900 or after and before 2000. 2. Use the boolean Series to slice the DataFrame * df[boolean_array]\n\n\n\nWrite one line of pandas code that returns a Series, where the index is the \"Party\", and the values are how many times that party won an election. Only include parties that have won an election.\n\n\nCode\nelections[elections[\"Result\"] == \"win\"][\"Party\"].value_counts()\n\n\nParty\nRepublican               24\nDemocratic               23\nWhig                      2\nDemocratic-Republican     1\nNational Union            1\nName: count, dtype: int64\n\n\n\n\nCode\nelections[elections[\"Result\"] == \"win\"].groupby(\"Party\").size()\n\n\nParty\nDemocratic               23\nDemocratic-Republican     1\nNational Union            1\nRepublican               24\nWhig                      2\ndtype: int64\n\n\n\n\nExplanation\n\n \n\nTwo parts to this! 1. Filter DataFrame to only include winners. * Use boolean slicing again! Construct a boolean Series that has True if the row contains a winner, and False otherwise * elections[elections[\"Result\"] == \"win\"] 2. Within filtered DataFrame (let’s call this winners), count the number of times each party won an election. Two ways to do this. * Extract the Party column from winners, and call value_counts(). * winners[\"Party\"].value_counts() * Group by the Party column, and aggregate by the number of rows in each sub-DataFrame. * winners.groupby(\"Party\").size() * The two methods above return the same thing, except .value_counts() sorts by the values in decreasing order, while .groupby() sort by the index in increasing order!\n\n\n\nWrite a line of pandas code that returns a Series whose index is the years and whose values are the number of candidates that participated in those years’ elections.\n\n\nCode\nelections[\"Year\"].value_counts().head() #.head() to limit output\n\n\nYear\n1996    7\n1948    6\n2016    6\n1976    6\n2008    6\nName: count, dtype: int64\n\n\n\n\nCode\nelections.groupby(\"Year\").size().head() #.head() to limit output\n\n\nYear\n1824    2\n1828    2\n1832    3\n1836    3\n1840    2\ndtype: int64\n\n\n\n\nExplanation\n\n \n\nVery similar to Problem 3! Might even be easier, actually. Each row corresponds to one candidate per election cycle, so we simply need to count the number of times each Year appears in the elections DataFrame. Just like in Problem 3, two ways to do this.\n\nExtract the Year column as a Series, call .value_counts() on it.\n\nelections[\"Year\"].value_counts()\n\n\nGroup by the Year column, creating a sub-DataFrame for each unique Year. Aggregate by .size(), counting the number of rows in each sub-DataFrame.\n\nelections.groupby(\"Year\").size()\n\n\n\n\n\nWrite a line of pandas code that creates a filtered DataFrame named filtered_parties from the elections dataset and keeps only the parties that have at least one election % more than 50%.\n\n\nCode\nfiltered_parties = elections.groupby(\"Party\").filter(lambda df: df[\"%\"].max() &gt; 50)\nfiltered_parties\n\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\n\n\n1\n1824\nJohn Quincy Adams\nDemocratic-Republican\n113142\nwin\n42.789878\n\n\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\n\n\n4\n1832\nAndrew Jackson\nDemocratic\n702735\nwin\n54.574789\n\n\n7\n1836\nHugh Lawson White\nWhig\n146109\nloss\n10.005985\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n176\n2016\nHillary Clinton\nDemocratic\n65853514\nloss\n48.521539\n\n\n178\n2020\nJoseph Biden\nDemocratic\n81268924\nwin\n51.311515\n\n\n179\n2020\nDonald Trump\nRepublican\n74216154\nloss\n46.858542\n\n\n182\n2024\nDonald Trump\nRepublican\n77303568\nwin\n49.808629\n\n\n183\n2024\nKamala Harris\nDemocratic\n75019230\nloss\n48.336772\n\n\n\n\n99 rows × 6 columns\n\n\n\n\n\nExplanation\n\n \n\nThis filtering is different from boolean slicing! Boolean slicing considers rows individually, while .filter() considers groups of rows. Rows of a sub-DataFrame either all make it, or none make it.\n\nGroup by the Party column, creating one sub-DataFrame for each party.\n\nelections.groupby(\"Party\")\n\nFilter using .filter()\n\nPass in a function into .filter() that takes in a DataFrame and returns True or False. Can be a lambda function!\n.filter(lambda df: df[\"%\"].max() &gt; 50)\n\nIf the lambda function returns True, it means you keep the entire sub-DataFrame. False means you exclude it entirely!\n\n\n\n\n\n\nWrite a line of pandas code that uses the filtered_parties DataFrame to return a new DataFrame with row indices that correspond to the year and columns that correspond to each party. Each entry should be the total percentage of votes for all the candidates that ran during that particular year for the specified party. Missing values from the dataset (the cases where a party did not have a candidate in a particular year) should be entered as 0. Below is an example.\n\n\n\nCode\nelections_pivot = filtered_parties.pivot_table(\n    index = \"Year\",\n    columns = \"Party\",\n    values = \"%\",\n    aggfunc = np.sum,\n    fill_value = 0)\nelections_pivot.head(10)\n\n\n\n\n\n\n\n\nParty\nDemocratic\nDemocratic-Republican\nNational Union\nRepublican\nWhig\n\n\nYear\n\n\n\n\n\n\n\n\n\n1824\n0.000000\n100.0\n0.0\n0.000000\n0.000000\n\n\n1828\n56.203927\n0.0\n0.0\n0.000000\n0.000000\n\n\n1832\n54.574789\n0.0\n0.0\n0.000000\n0.000000\n\n\n1836\n52.272472\n0.0\n0.0\n0.000000\n47.727528\n\n\n1840\n46.948787\n0.0\n0.0\n0.000000\n53.051213\n\n\n1844\n50.749477\n0.0\n0.0\n0.000000\n49.250523\n\n\n1848\n42.552229\n0.0\n0.0\n0.000000\n47.309296\n\n\n1852\n51.013168\n0.0\n0.0\n0.000000\n44.056548\n\n\n1856\n45.306080\n0.0\n0.0\n33.139919\n0.000000\n\n\n1860\n0.000000\n0.0\n0.0\n39.699408\n0.000000\n\n\n\n\n\n\n\n\n\nExplanation\n\n \n\nFirst thing to notice is that the columns are values of the Party column! This tells us that what we see is a pivot table.\n\nUse the .pivot_table() function on filtered_parties\n\nindex = \"Year\" and columns = \"Party\", saying that the unique values of Year should make up the row indices, and the unique values of Party should make up the columns.\nvalues = \"%\" indicates that we populate the cells with the % values for each combination of Year, Party\naggfunc = np.sum describes how to aggregate the values in a cell\nfill_value = 0 says to impute 0 in case there is no % value for a specific Year, Party combination",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Pandas I</span>"
    ]
  },
  {
    "objectID": "disc03/disc03.html",
    "href": "disc03/disc03.html",
    "title": "3  Pandas II, EDA",
    "section": "",
    "text": "3.1 Dealing with Missing Data\nWhile exploring a Berkeley dataset (separate from babynames) with a million records, you realize that a portion of measurements in different fields are NaN values! You decide to impute these missing values before continuing your EDA. Given the empirical distribution of each of the below variables, determine how to solve the missing data problem. (Note that the data in these graphs are fictional).\nSuppose that you plot “cups of coffee sold at V&A Cafe per day” versus “inches of rain per day” across a period of 2 months, shown below. V&A Cafe is not missing any data, but 30% of the data in “inches of rain” are NaN values that have been represented with “-2”, an impossible amount of rain. Which of the following techniques would be most effective in solving the issue of missing data? (Select all that apply)\nA. Using the mean to impute the missing values\nB. Using the mode(s) to impute the missing values\nC. Using the median to impute the missing values\nD. Dropping any rows with missing values\nE. Imputing missing values through interpolation\nSuppose we examine the amount of money lost/gained in a game of poker and see that this variable is missing 1% of its values. Its distribution, shown below, is constructed from all valid (non-NaN) values. Which of the following techniques would be reasonably effective in solving this issue of missing data? (Select all that apply)\nA. Using the mean to impute the missing values\nB. Using the mode(s) to impute the missing values\nC. Using the median to impute the missing values\nD. Dropping any rows with missing values\nE. Imputing missing values through interpolation\nSuppose that the relationship between students’ time asleep (in hours) and the amount of extra credit they received in Data 100 is shown below. There is no missing data for “hours asleep”, but 0.5% of “extra credit score” is missing. Like in part a, the missing NaN values were replaced with an impossible score of -0.002, making the graph look funky. Which of the following techniques would be most effective in solving this issue of missing data? (Select all that apply)\nA. Using the mean to impute the missing values\nB. Using the mode(s) to impute the missing values\nC. Using the median to impute the missing values\nD. Dropping any rows with missing values\nE. Imputing missing values through interpolation",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Pandas II, EDA</span>"
    ]
  },
  {
    "objectID": "disc03/disc03.html#dealing-with-missing-data",
    "href": "disc03/disc03.html#dealing-with-missing-data",
    "title": "3  Pandas II, EDA",
    "section": "",
    "text": "Question 1a\n\n\n\n\n\n\n\n\n\nAnswer\n\nCorrect Options: A, C, E\n\n\n\n\n\nQuestion 1b graph\n\n\n\n\n\n\n\n\n\nAnswer\n\nCorrect Options: B, D\n\n\n\n\n\nQuestion 1c graph\n\n\n\n\n\n\n\n\n\nAnswer\n\nCorrect Options: D, E",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Pandas II, EDA</span>"
    ]
  },
  {
    "objectID": "disc03/disc03.html#pandas-eda-exam-prep-modeled-after-fa22-midterm-q1",
    "href": "disc03/disc03.html#pandas-eda-exam-prep-modeled-after-fa22-midterm-q1",
    "title": "3  Pandas II, EDA",
    "section": "3.2 Pandas + EDA exam prep (modeled after Fa22 Midterm Q1)",
    "text": "3.2 Pandas + EDA exam prep (modeled after Fa22 Midterm Q1)\nIt’s the annual Monopoly World Championship! The finalists: Shawn, Amanda, Neil, and Annie are playing Monopoly, a board game where players pay a price to buy properties, which can then generate income for them. Each property can be owned by only one player at a time. At the end of the game, the player with the most money wins.\nShawn wants to figure out which properties are most worth buying. He creates a DataFrame income with data on the current game state, shown on the left. He also finds a DataFrame properties with data on Monopoly properties, shown on the right.\nBoth tables have 28 rows. For brevity, only the first few rows of each DataFrame are shown.\n\n\nCode\n# First DataFrame: income\ndata_income = {\n    'Player': ['Shawn', 'Amanda', 'Neil', np.nan, 'Shawn', 'Annie', 'Amanda'],\n    'Property': ['Boardwalk', 'Park Place', 'Marvin Gardens', 'Kentucky Ave', 'Pennsylvania Ave', 'Oriental Ave', 'Baltic Ave'],\n    'Income Generated': ['$425', '$375', '$200', np.nan, '$150', '$50', '$60']\n}\n\nincome = pd.DataFrame(data_income)\nincome\n\n\n\n\n\n\n\n\n\nPlayer\nProperty\nIncome Generated\n\n\n\n\n0\nShawn\nBoardwalk\n$425\n\n\n1\nAmanda\nPark Place\n$375\n\n\n2\nNeil\nMarvin Gardens\n$200\n\n\n3\nNaN\nKentucky Ave\nNaN\n\n\n4\nShawn\nPennsylvania Ave\n$150\n\n\n5\nAnnie\nOriental Ave\n$50\n\n\n6\nAmanda\nBaltic Ave\n$60\n\n\n\n\n\n\n\nincome\n\nPlayer is the name of the player, as a str.\nProperty is a property currently owned by the player, as a str.\nIncome Generated is the amount of income a player has earned from that property so far, as a str.\n\n\n\nCode\n# Second DataFrame: properties\ndata_properties = {\n    'Property': ['Park Place', 'Oriental Ave', 'Vermont Ave', 'Pacific Ave', 'Boardwalk', 'Illinois Ave', 'Atlantic Ave'],\n    'Property Color': ['Dark Blue', 'Light Blue', 'Light Blue', 'Green', 'Dark Blue', 'Red', 'Yellow'],\n    'Purchase Price': [350.0, 100.0, 100.0, 300.0, 400.0, 240.0, 260.0]\n}\n\nproperties = pd.DataFrame(data_properties)\nproperties\n\n\n\n\n\n\n\n\n\nProperty\nProperty Color\nPurchase Price\n\n\n\n\n0\nPark Place\nDark Blue\n350.0\n\n\n1\nOriental Ave\nLight Blue\n100.0\n\n\n2\nVermont Ave\nLight Blue\n100.0\n\n\n3\nPacific Ave\nGreen\n300.0\n\n\n4\nBoardwalk\nDark Blue\n400.0\n\n\n5\nIllinois Ave\nRed\n240.0\n\n\n6\nAtlantic Ave\nYellow\n260.0\n\n\n\n\n\n\n\nproperties\n\nProperty is the name of the property, as a str. There are 28 unique properties.\nProperty Color is a color group that the property belongs to, as a str. There are 10 unique color groups, and each property belongs to a single group.\nPurchase Price is the price to buy the property, as a float.\n\nNote: For the properties that are not currently owned by any player, the Player and Income Generated columns in the income table have a NaN value.\n(a) What is the granularity of the income table?\n\n\nAnswer\n\n\nProperty\n\nEach unique property has its own row\nNotice how one player can have own multiple properties and can appear in multiple rows! This tells us that the granularity of this table is not Player.\n\n\n(b) Consider the Player and Purchase Price variables. What type of variable is each one? (quantitative, qualitative nominal, qualitative ordinal)\n\n\nAnswer\n\n\nPlayer: Qualitative nominal\nPurchase Price: Quantitative\n\n\n(c) Which of the following line(s) of code successfully returns a Series with the number of properties each player owns? Select all that apply.\n\n\nAnswer\n\n\nincome[\"Player\"].value_counts()\nincome.groupby(\"Player\").size()\n\n\n\n\nCode\nincome.groupby(\"Player\").agg(pd.value_counts)\n\n\n\n\n\n\n\n\n\nProperty\nIncome Generated\n\n\nPlayer\n\n\n\n\n\n\nAmanda\n[1, 1]\n[1, 1]\n\n\nAnnie\n1\n1\n\n\nNeil\n1\n1\n\n\nShawn\n[1, 1]\n[1, 1]\n\n\n\n\n\n\n\n\n\nCode\nincome[\"Player\"].value_counts()\n\n\nPlayer\nShawn     2\nAmanda    2\nNeil      1\nAnnie     1\nName: count, dtype: int64\n\n\n\n\nCode\n# income[\"Player\", \"Property\"].groupby(\"Player\").size()\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe above code will error! Make sure to use double brackets when selecting columns.\n\n\n\n\nCode\nincome.groupby(\"Player\")[[\"Player\"]].count()\n\n\n\n\n\n\n\n\n\nPlayer\n\n\nPlayer\n\n\n\n\n\nAmanda\n2\n\n\nAnnie\n1\n\n\nNeil\n1\n\n\nShawn\n2\n\n\n\n\n\n\n\n(d) He now decides to calculate the amount of profit from each property. He wants to store this in a column called Profit in the income DataFrame. To do this, he first has to transform the Income Generated column to be of a float datatype.\nWrite one line of code to replace the old column with a new column, also called Income Generated, with the datatype modification described above. You may assume that each entry in Income Generated consists of a dollar sign ($) followed by a number, except for the NaN values.\n\n\nCode\nincome[\"Income Generated\"] = income[\"Income Generated\"].str[1:].astype(float)\n\n\n(e) Assuming that the answer to (d) is correct, let’s add a Profit column to the income DataFrame. Fill in the following blanks to do this, and please add arguments to function class as you see appropriate.\nNote: Profit is calculated by subtracting the purchase price from generated income.\ncombined_df = income._____A_____(_______B_______)\nincome[\"Profit\"] = _______C_______\n\n\nCode\ncombined_df = income.merge(properties, on = \"Property\")\nincome[\"Profit\"] = combined_df[\"Income Generated\"] - combined_df[\"Purchase Price\"]\n\n\nShawn realizes he’s lost more money than he’s made. To solve this problem, he begins by writing some Pandas code to merge the Property Color column into the income DataFrame and drops all rows with NaN values. He calls this DataFrame merged_df. Shown below are the first few rows.\n\n\nCode\nmerged_df = pd.DataFrame({\"Player\": [\"Shawn\", \"Amanda\", \"Neil\", \"Shawn\", \"Annie\", \"Amanda\"],\n                          \"Property\": [\"Boardwalk\", \"Park Place\", \"Marvin Gardens\", \"Pennsylvania Ave\", \"Oriental Ave\", \"Baltic Ave\"],\n                          \"Income Generated\": [425., 375., 200., 150., 50., 60.],\n                          \"Profit\": [-25., 25., 50., -100., 0., 0.],\n                          \"Property Color\": [\"Dark Blue\", \"Dark Blue\", \"Yellow\", \"Green\", \"Light Blue\", \"Purple\"]})\nmerged_df\n\n\n\n\n\n\n\n\n\nPlayer\nProperty\nIncome Generated\nProfit\nProperty Color\n\n\n\n\n0\nShawn\nBoardwalk\n425.0\n-25.0\nDark Blue\n\n\n1\nAmanda\nPark Place\n375.0\n25.0\nDark Blue\n\n\n2\nNeil\nMarvin Gardens\n200.0\n50.0\nYellow\n\n\n3\nShawn\nPennsylvania Ave\n150.0\n-100.0\nGreen\n\n\n4\nAnnie\nOriental Ave\n50.0\n0.0\nLight Blue\n\n\n5\nAmanda\nBaltic Ave\n60.0\n0.0\nPurple\n\n\n\n\n\n\n\nShawn decides he will now only buy properties from a color group that he deems “profitable.” He deems a color group “profitable” if at least 50% of the properties in the group that are currently owned by players have made a positive (non-zero) profit for those players.\nFill in the following lines of code to help him display a DataFrame with a subset of the rows in merged_df: the rows with properties that belong to profitable color groups. Your solution may use fewer lines of code than we provide.\n\n\nCode\ndef func(group):\n    if np.mean(group[\"Profit\"] &gt; 0) &gt;= 0.5:\n        return True\n    return False\n\nmerged_df.groupby(\"Property Color\").filter(func)\n\n\n\n\n\n\n\n\n\nPlayer\nProperty\nIncome Generated\nProfit\nProperty Color\n\n\n\n\n0\nShawn\nBoardwalk\n425.0\n-25.0\nDark Blue\n\n\n1\nAmanda\nPark Place\n375.0\n25.0\nDark Blue\n\n\n2\nNeil\nMarvin Gardens\n200.0\n50.0\nYellow",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Pandas II, EDA</span>"
    ]
  }
]