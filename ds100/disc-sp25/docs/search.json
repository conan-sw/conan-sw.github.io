[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data 100 Discussion Notes",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>disc-sp25</span>"
    ]
  },
  {
    "objectID": "disc01/disc01.html",
    "href": "disc01/disc01.html",
    "title": "2  Math Prerequisites",
    "section": "",
    "text": "2.0.1 Link to Slides",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Math Prerequisites</span>"
    ]
  },
  {
    "objectID": "disc01/disc01.html#linear-algebra-fundamentals",
    "href": "disc01/disc01.html#linear-algebra-fundamentals",
    "title": "2  Math Prerequisites",
    "section": "2.1 Linear Algebra Fundamentals",
    "text": "2.1 Linear Algebra Fundamentals\nLinear algebra is what powers linear regression, logistic regression, and PCA (concepts that we will be studying in this course). This question aims to build an understanding of how matrix-vector operations work.\nConsider yourself starstruck: it’s 2016, and you just spotted the first family of music, Beyonce, husband Jay-Z, and their four year-old daughter Blue, shopping for fruit bowls at Berkeley Bowl. Each bowl contains some fruit and the price of a fruit bowl is simply the total price of all of its individual fruit.\nBerkeley Bowl has apples for $2, bananas for $1, and cantaloupes for $4 (expensive!). The price of each of these can be written in a vector:\n\\[\\begin{align*}\n    \\vec{v} =\n        \\begin{bmatrix}\n        2\\\\\n        1\\\\\n        4\n        \\end{bmatrix}\n\\end{align*}\\]\nBerkeley Bowl sells the following fruit bowls: 1. 2 of each fruit 2. 5 apples and 8 bananas 3. 2 bananas and 3 cantaloupes 4. 10 cantaloupes\n\n2.1.1 (a)\nDefine a matrix \\(B\\) such that \\(B\\vec{v}\\) evaluates to a length 4 column vector containing the price of each fruit bowl. The first entry of the result should be the cost of fruit bowl #1, the second entry the cost of fruit bowl #2, etc.\n\n\nAnswer\n\nRecognize that each of the costs can be expressed as a linear combination of quantities (\\(B\\)) and prices (\\(\\vec{v}\\)). The matrix \\(B\\) of quantities should have each row representing the unique fruit bowls, and each column representing the unique fruits.\n\\[\\begin{align*}\nB = \\begin{bmatrix}\n2 & 2 & 2\\\\\n5 & 8 & 0\\\\\n0 & 2 & 3\\\\\n0 & 0 & 10\n\\end{bmatrix}\n\\end{align*}\\]\n\n\n\n2.1.2 (b)\nBeyonce, Jay-Z, and Blue make the following purchases:\n\nBeyonce buys 2 fruit bowl #1s and 1 fruit bowl #2\nJay-Z buys 1 of each fruit bowl\nBlue buys 10 fruit bowl #4s\n\nDefine a matrix \\(A\\) such that the matrix expression \\(AB\\vec{v}\\) evaluates to a length 3 column vector containing how much each of them spent. The first entry of the result should be the total amount spent by Beyonce, the second entry the amount spend by Jay-Z, etc.\n\n\nAnswer\n\nWe know that \\(B\\vec{v}\\) represent the price of each fruit bowl. Therefore, in order to figure out how much each of them spent, we can multiply \\(A\\), a matrix containing the fruit bowls bought by each of the individuals, by the vector of prices \\(B\\vec{v}\\). Each row in \\(A\\) should represent one individual, and each column should represent one fruit (\\(B\\vec{v}\\) being \\(4\\times1\\) tells us that \\(A\\) should be \\(3\\times4\\) instead of the other way around)\n\\[\\begin{align*}\nA = \\begin{bmatrix}\n2 & 1 & 0 & 0\\\\\n1 & 1 & 1 & 1\\\\\n0 & 0 & 0 & 10\n\\end{bmatrix}\n\\end{align*}\\]\n\n\n\n2.1.3 (c)\nLet’s suppose Berkeley Bowl changes their fruit prices, but you don’t know what they changed their prices to. Beyonce, Jay-Z and Blue buy the same quantity of fruit bowls and the number of fruit in each bowl is the same, but now they each spent these amounts:\n\\[\\begin{align*}\n\\vec{x} = \\begin{bmatrix}80 \\\\ 80 \\\\ 100\\end{bmatrix}\n\\end{align*}\\]\nLet \\(\\vec{v_2}\\) be a vector containing the new prices of each fruit. Express \\(\\vec{v_2}\\) in terms of \\(A\\), \\(B\\), and \\(\\vec{x}\\).\n\n\nAnswer\n\nWe know from previous parts that \\(\\vec{x} = AB\\vec{v_2}\\). We can multiply both sides by \\((AB)^{-1}\\), resulting in \\(\\vec{v_2} = (AB)^{-1}\\vec{x}\\). Note that this assumes that \\(AB\\) is invertible!\n\n\n\n2.1.4 (d)\nIn the previous part, we assumed that \\(AB\\) was invertible. Why is \\(AB\\) (as calculated below) invertible? State two conditions for an arbitrary matrix to be invertible.\n\\[\\begin{align*}\nAB = \\begin{bmatrix}\n9 & 12 & 4\\\\\n7 & 12 & 15\\\\\n0 & 0 & 100\n\\end{bmatrix}\n\\end{align*}\\]\n\n\nAnswer\n\nBesides needing to be a square matrix (same number of rows and columns), answers include, but are not limited to:\n\nThe column vectors are linearly independent (meaning the matrix is full column rank)\nThe determinant of the matrix is nonzero\n\nFor (2), we can compute the determinant of a \\(3 \\times 3\\) matrix by the following computation:\n\\[\\begin{align*}\ndet(AB) &= 9*det(\\begin{bmatrix}12 & 15 \\\\ 0 & 100\\end{bmatrix}) - 12 * det(\\begin{bmatrix}7 & 15 \\\\ 0 & 100\\end{bmatrix}) + 4 * det(\\begin{bmatrix}7 & 12 \\\\ 0 & 0\\end{bmatrix})\\\\\n&= 9 * (12*100 - 15*0) - 12 * (7 * 100 - 15 * 0) + 4 * (7 * 0 - 12 * 0)\\\\\n&= 2400 \\neq 0\n\\end{align*}\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Math Prerequisites</span>"
    ]
  },
  {
    "objectID": "disc01/disc01.html#more-linear-algebra",
    "href": "disc01/disc01.html#more-linear-algebra",
    "title": "2  Math Prerequisites",
    "section": "2.2 More Linear Algebra",
    "text": "2.2 More Linear Algebra\nIn this question, we will apply various essential concepts in linear algebra.\n\n2.2.1 (a)\nLinear dependence among a set of vectors \\(\\{\\vec{v_1}, \\vec{v_2}, ..., \\vec{v_n}\\}\\) is defined as follows:\n\n\n\n\n\n\nNote\n\n\n\nIf any (non-trivial) linear combination of the vectors can produce the zero vector, then the set of vectors is linearly dependent.\nIn other words, if we can scale the vectors \\(\\vec{v_j}\\) by scalars \\(\\alpha_j\\) and sum the quantity to obtain the zero vector (given at least one \\(\\alpha_j \\neq 0\\)), then the set is linearly dependent.\n\\[\\begin{align*}\n\\sum_{i=1}^n\\alpha_i\\vec{v_i}=\\vec{0} \\text{ such that some } \\alpha_j \\neq 0 \\Longrightarrow \\text{ linear dependence}\n\\end{align*}\\]\nAny set of vectors such that we cannot obtain the zero vector as described above is linearly independent.\n\n\nThe column rank of a matrix \\(M\\) is the maximal number of linearly independent column vectors in \\(M\\). A full column rank matrix has a column rank equal to the number of column vectors.\nNow consider the matrix \\(M = \\begin{bmatrix}2 & 0 \\\\ 0 & 3\\end{bmatrix}\\) containing two column vectors \\(\\vec{v_1} = \\begin{bmatrix}2 \\\\ 0\\end{bmatrix}\\) and \\(\\vec{v_2} = \\begin{bmatrix} 0 \\\\ 3 \\end{bmatrix}\\). Is it possible to construct the zero vector using a linear combination of the column vectors? What can be concluded about the column rank of the matrix \\(M\\)?\n\n\nAnswer\n\nNo, it is impossible to construct the zero vector if we use at least one \\(\\alpha_i \\neq 0\\) since the first vector can’t affect the second dimension and vice versa. Hence, neither of the vectors can ”undo” each other. As a more formal proof:\n\\[\\begin{align*}\n\\alpha_1\\vec{v_1} + \\alpha_2\\vec{v_2} = \\begin{bmatrix} 2\\alpha_1 \\\\ 3\\alpha_2 \\end{bmatrix}\n\\end{align*}\\]\nIf at least one of the \\(\\alpha\\) values are not \\(0\\), then the vector cannot be \\(\\vec{0}\\). Hence, this matrix is full column rank (i.e., the set of vectors is linearly independent).\n\n\n\n2.2.2 (b)\nThe inverse of a square invertible matrix \\(M\\) is denoted \\(M^{-1}\\). It is defined as a matrix such that \\(MM^{-1} = I\\) and \\(M^{-1}M = I\\). The matrix \\(I\\) is a special matrix known as the identity matrix, where the diagonal elements are \\(1\\) and the non-diagonal elements are \\(0\\).\nConsider the inverse matrix \\(M^{-1} = \\begin{bmatrix}a & b \\\\ c & d\\end{bmatrix}\\) of \\(M\\). Carry out the matrix multiplication \\(MM^{-1}\\), and determine what \\(M^{-1}\\) must be. Recall that \\(M = \\begin{bmatrix}2 & 0 \\\\ 0 & 3 \\end{bmatrix}\\).\n\n\nAnswer\n\nWe carry out the matrix multiplication:\n\\[\\begin{align*}\n\\begin{bmatrix}2 & 0 \\\\ 0 & 3\\end{bmatrix}\\begin{bmatrix}a & b \\\\ c & d\\end{bmatrix} = \\begin{bmatrix}2a & 2b \\\\ 3c & 3d\\end{bmatrix}\n\\end{align*}\\]\nHence, since we know that \\(MM^{-1} = I, b = c = 0\\) and \\(2a = 1\\) and \\(3d = 1\\). Thus, the inverse matrix is:\n\\[\\begin{align*}\nM^{-1} = \\begin{bmatrix}\\frac{1}{2} & 0 \\\\ 0 & \\frac{1}{3}\\end{bmatrix}\n\\end{align*}\\]\nAn interesting fact about diagonal matrices (i.e. matrices that are nonzero on the diagonal entries but zero everywhere else) is that their inverse is simply the elementwise multiplicative inverse (reciprocal) of the diagonal entries!\n\n\n\n2.2.3 (c)\nConsider a difference matrix \\(Q = \\begin{bmatrix}1 & 0 & 5 \\\\ 0 & 1 & 5\\end{bmatrix} = \\begin{bmatrix}\\vec{v_1} & \\vec{v_2} & \\vec{v_3}\\end{bmatrix}\\). What is the column rank of the matrix? Is the matrix invertible?\n\n\nAnswer\n\nWe can construct the zero vector with all of \\(\\vec{v_1}\\), \\(\\vec{v_2}\\), and \\(\\vec{v_3}\\) with the combination \\(5\\vec{v_1} + 5\\vec{v_2} = \\vec{v_3} = 0\\), so we know that the matrix \\(Q\\) does not have full column rank. However, we are not able to construct the zero vector using only any of the two columns. This tells us that the column rank is 2.\nThe matrix is not invertible since it is not square (it has more columns than rows).\n\n\n\n2.2.4 (d)\nThe transpose of a matrix is an opration on matrices of any size in which the rows and columns are “flipped”. In more precise terms, if \\(A\\) is an \\(m \\times n\\) matrix, its transpose \\(A^T\\) is the \\(n \\times m\\) matrix whose element in the ith row and jth column is the element in the jth row and ith column of \\(A\\).\nConsider a matrix \\(R\\), which is equal to the transpose of \\(Q\\): \\(R = Q^T\\). What is the column rank of \\(R\\)?\n\n\nAnswer\n\nWe take the transpose:\n\\[\\begin{align*}\nR = Q^T = \\begin{bmatrix}1 & 0\\\\ 0 & 1 \\\\ 5 & 5\\end{bmatrix}\n\\end{align*}\\]\nThe column rank is 2 because neither column is a scalar multiple of the other. Thus, this matrix is full column rank.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Math Prerequisites</span>"
    ]
  },
  {
    "objectID": "disc01/disc01.html#calculus",
    "href": "disc01/disc01.html#calculus",
    "title": "2  Math Prerequisites",
    "section": "2.3 Calculus",
    "text": "2.3 Calculus\nIn this class, we will have to determine which inputs to functions minimize the output (for instance, when we choose a model and need to fit it to our data). This process involves taking derivatives. In cases where we have multiple inputs, the derivative of our function with respect to one of our inputs is called a partial derivative. For example, given a function \\(f(x, y)\\), the partial derivative with respect to \\(x\\) (denoted by \\(\\frac{\\partial f}{\\partial x}\\)) is the derivative of \\(f\\) with respect to \\(x\\), taken while treating all other variables as if they’re constants.\nSuppose we have the following scalar-values function on \\(x\\) and \\(y\\):\n\\[\\begin{align*}\nf(x, y) = x^2 + 4xy + 2y^3 + e^{-3y} + \\ln(2y)\n\\end{align*}\\]\n\n2.3.1 (a)\nCompute the partial derivative of \\(f(x, y)\\) with respect to \\(x\\).\n\n\nAnswer\n\n\\[\\begin{align*}\n\\frac{\\partial}{\\partial x}f(x, y) = 2x + 4y\n\\end{align*}\\]\n\n\n\n2.3.2 (b)\nCompute the partial derivative of \\(f(x, y)\\) with respect to \\(y\\).\n\n\nAnswer\n\n\\[\\begin{align*}\n\\frac{\\partial}{\\partial y}f(x, y) = 4x + 6y^2 - 3e^{-3y} + \\frac{1}{y}\n\\end{align*}\\]\n\n\n\n2.3.3 (c)\nThe gradient of a function \\(f(x, y)\\) is a vector of its partial derivatives. That is,\n\\[\\begin{align*}\n\\nabla f(x, y) = \\begin{bmatrix}\\frac{\\partial f}{\\partial x} & \\frac{\\partial f}{\\partial y}\\end{bmatrix}^T\n\\end{align*}\\]\nAs a vector, \\(\\nabla f(x, y)\\) has both a magnitude and direction. The direction of \\(\\nabla f(x, y)\\) corresponds to the direction in which the graph of \\(f(x, y)\\) is increasing most steeply from the point \\((x, y)\\). The magnitude gives a sense of how steep the ascent up the graph is in this particular direction. This is a generalization of the single variable case, where \\(f'(x)\\) is the rate of changes of \\(f\\), at the point \\(x\\). In the two-variable case, we specify a direction to evaluate the rate of change, since the function is technically changing in all directions from \\((x, y)\\).\nUsing your answers to the above two parts, compute \\(\\nabla f(x, y)\\) and evaluate the gradient at the point \\((x = 2, y = -1)\\).\n\n\nAnswer\n\n\\[\\begin{align*}\n\\nabla f(x, y) &=\n\\begin{bmatrix}\n2x + 4y\\\\\n4x + 6y^2 - 3e^{-3y} + \\frac{1}{y}\n\\end{bmatrix}\n\\\\\\\\\n\\nabla f(2, -1) &=\n\\begin{bmatrix}\n2(2) + 4(-1)\\\\\n4(2) + 6(-1)^2 - 3e^{-3(-1)} + \\frac{1}{(-1)}\n\\end{bmatrix}\\\\\n&= \\begin{bmatrix} 0 \\\\ 13 - 3e^3 \\end{bmatrix}\n\\end{align*}\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Math Prerequisites</span>"
    ]
  },
  {
    "objectID": "disc01/disc01.html#probability-sampling",
    "href": "disc01/disc01.html#probability-sampling",
    "title": "2  Math Prerequisites",
    "section": "2.4 Probability & Sampling",
    "text": "2.4 Probability & Sampling\n\n\n\nHouses\n\n\nIshani wants to measure interest for a party on her street. She assigns numbers and letters to each house on her street as illustrated above. She picks a letter “a”, “b”, or “c” at random and then surveys every household on the street ending in that letter.\n\n2.4.1 (a)\nWhat is the chance that two houses next door to each other are both in the sample?\n\n\nAnswer\n\nNone of the adjacent houses end in the same letter, so the chance is zero. This is an example of a cluster sample with each cluster representing each group of houses ending in the same letter.\n\nNote: We will no longer be following Ishani’s sampling method of picking a letter and surveying households ending in that letter for subsequent questions.\n\n\n2.4.2 (b)\nIshani decides to collect a simple random sample (SRS) of four houses. What is the probability that house 1a is not in Ishani’s simple random sample of four houses?\n\n\nAnswer\n\nRecall that a simple random sample is a sample drawn uniformly at random without replacement.\nThis time, we are taking a sample of 4 houses. But, we can apply a similar approach from part b to determine the probability of missing house 1a in each of the four selected houses. Then we multiply the four probabilities together to get our answer. The probability that house 1a is not in Ishani’s sample is \\(\\frac{11}{12}*\\frac{10}{11}*\\frac{9}{10}*\\frac{8}{9} = \\frac{8}{12} = \\frac{2}{3}\\)\n\n\n\n2.4.3 (c)\nInstead of surveying every member of each house from the SRS of four houses, Ishani decides to survey two members chosen without replacement from each selected house. Four people live in house 1a, one of whom is Bob. What is the probability that Bob is not chosen in Ishani’s new sample?\n\n\nAnswer\n\nThe probability that house 1a is included in Ishani’s initial SRS of four houses is \\(\\frac{1}{3}\\). Given that house 1a is selected, the probability that Bob is one of the two people surveyed is \\(\\frac{1}{2}\\). Therefore, the probability that Bob is surveyed is \\(\\frac{1}{3}*\\frac{1}{2} = \\frac{1}{6}\\). Thus, the probability that Bob is not selected is \\(1 - \\frac{1}{6} = \\frac{5}{6}\\)\n\n\n\n2.4.4 (d)\nTo increase interest in her party, Ishani randomly selects 4 houses to gift a small present. Assuming she samples with replacement (meaning that a house could be chosen multiple times), what’s the probability that all “c” houses are given a gift?\n\n\nAnswer\n\nFor all “c” houses to be selected, the first house selected can be any of the 4 “c” houses out of the 12 houses total. The second house selected can be any of the 3 “c” houses left (out of 12 houses total). the third house can be any of the 2 “c” houses left, and the final selected house must be the last “c” house left. Therefore, we get \\(\\frac{4}{12}*\\frac{3}{12}*\\frac{2}{12}*\\frac{1}{12} = \\frac{4!}{12^4}\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Math Prerequisites</span>"
    ]
  },
  {
    "objectID": "disc02/disc02.html",
    "href": "disc02/disc02.html",
    "title": "3  Pandas I",
    "section": "",
    "text": "3.0.1 Link to Slides\nThis discussion is all about practicing using pandas, and testing your knowledge about its various functionalities to accomplish small tasks.\nWe will be using the elections dataset from lecture.\n\n\nCode\n# import packages\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\n\n\nCode\nelections = pd.read_csv('elections.csv')\nelections.head(10)\n\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\n\n\n1\n1824\nJohn Quincy Adams\nDemocratic-Republican\n113142\nwin\n42.789878\n\n\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\n\n\n3\n1828\nJohn Quincy Adams\nNational Republican\n500897\nloss\n43.796073\n\n\n4\n1832\nAndrew Jackson\nDemocratic\n702735\nwin\n54.574789\n\n\n5\n1832\nHenry Clay\nNational Republican\n484205\nloss\n37.603628\n\n\n6\n1832\nWilliam Wirt\nAnti-Masonic\n100715\nloss\n7.821583\n\n\n7\n1836\nHugh Lawson White\nWhig\n146109\nloss\n10.005985\n\n\n8\n1836\nMartin Van Buren\nDemocratic\n763291\nwin\n52.272472\n\n\n9\n1836\nWilliam Henry Harrison\nWhig\n550816\nloss\n37.721543\n\n\n\n\n\n\n\nWrite a line of code that returns the elections table sorted in descending order by \"Popular vote\". Store your result in a variable named sorted. Would calling sorted.iloc[[0], :] give the same result as sorted.loc[[0], :]?\n\n\nCode\nsorted = elections.sort_values(\"Popular vote\", ascending = False)\nsorted\n\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n178\n2020\nJoseph Biden\nDemocratic\n81268924\nwin\n51.311515\n\n\n182\n2024\nDonald Trump\nRepublican\n77303568\nwin\n49.808629\n\n\n183\n2024\nKamala Harris\nDemocratic\n75019230\nloss\n48.336772\n\n\n179\n2020\nDonald Trump\nRepublican\n74216154\nloss\n46.858542\n\n\n162\n2008\nBarack Obama\nDemocratic\n69498516\nwin\n53.023510\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n110\n1956\nT. Coleman Andrews\nStates' Rights\n107929\nloss\n0.174883\n\n\n141\n1992\nBo Gritz\nPopulist\n106152\nloss\n0.101918\n\n\n99\n1948\nClaude A. Watson\nProhibition\n103708\nloss\n0.212747\n\n\n89\n1932\nWilliam Z. Foster\nCommunist\n103307\nloss\n0.261069\n\n\n6\n1832\nWilliam Wirt\nAnti-Masonic\n100715\nloss\n7.821583\n\n\n\n\n187 rows × 6 columns\n\n\n\n\n\nExplanation\n\n\n\nWe can sort a DataFrame by a column using the .sort_values() function! Remember to specify ascending = False, or else it will sort in increasing order.\n\n\n\n\nCode\nsorted.iloc[[0], :]\n\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n178\n2020\nJoseph Biden\nDemocratic\n81268924\nwin\n51.311515\n\n\n\n\n\n\n\n\n\nCode\nsorted.loc[[0], :]\n\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\n\n\n\n\n\n\n\n\n\nExplanation\n\n \n\nThe difference is that .loc[] uses label-based indexing, while .iloc[] uses integer position-based indexing. Using .loc[] will simply grab the row with the label 0 regardless of where it is, while .iloc[] will grab the first row of the sorted DataFrame.\n\n\n\nUsing Boolean slicing, write one line of pandas code that returns a DataFrame that only contains election results from the 1900s.\n\n\nCode\nelections[(elections[\"Year\"] &gt;= 1900) & (elections[\"Year\"] &lt; 2000)]\n\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n54\n1900\nJohn G. Woolley\nProhibition\n210864\nloss\n1.526821\n\n\n55\n1900\nWilliam Jennings Bryan\nDemocratic\n6370932\nloss\n46.130540\n\n\n56\n1900\nWilliam McKinley\nRepublican\n7228864\nwin\n52.342640\n\n\n57\n1904\nAlton B. Parker\nDemocratic\n5083880\nloss\n37.685116\n\n\n58\n1904\nEugene V. Debs\nSocialist\n402810\nloss\n2.985897\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n146\n1996\nHarry Browne\nLibertarian\n485759\nloss\n0.505198\n\n\n147\n1996\nHoward Phillips\nTaxpayers\n184656\nloss\n0.192045\n\n\n148\n1996\nJohn Hagelin\nNatural Law\n113670\nloss\n0.118219\n\n\n149\n1996\nRalph Nader\nGreen\n685297\nloss\n0.712721\n\n\n150\n1996\nRoss Perot\nReform\n8085294\nloss\n8.408844\n\n\n\n\n97 rows × 6 columns\n\n\n\n\n\nExplanation\n\n \n\nWe can “filter” DataFrames by using boolean slicing! 1. Construct a boolean Series that is True if a row contains election results from the 1900s, and False otherwise. * We can use the & (and) logical operator! 1900 or after and before 2000. 2. Use the boolean Series to slice the DataFrame * df[boolean_array]\n\n\n\nWrite one line of pandas code that returns a Series, where the index is the \"Party\", and the values are how many times that party won an election. Only include parties that have won an election.\n\n\nCode\nelections[elections[\"Result\"] == \"win\"][\"Party\"].value_counts()\n\n\nParty\nRepublican               24\nDemocratic               23\nWhig                      2\nDemocratic-Republican     1\nNational Union            1\nName: count, dtype: int64\n\n\n\n\nCode\nelections[elections[\"Result\"] == \"win\"].groupby(\"Party\").size()\n\n\nParty\nDemocratic               23\nDemocratic-Republican     1\nNational Union            1\nRepublican               24\nWhig                      2\ndtype: int64\n\n\n\n\nExplanation\n\n \n\nTwo parts to this! 1. Filter DataFrame to only include winners. * Use boolean slicing again! Construct a boolean Series that has True if the row contains a winner, and False otherwise * elections[elections[\"Result\"] == \"win\"] 2. Within filtered DataFrame (let’s call this winners), count the number of times each party won an election. Two ways to do this. * Extract the Party column from winners, and call value_counts(). * winners[\"Party\"].value_counts() * Group by the Party column, and aggregate by the number of rows in each sub-DataFrame. * winners.groupby(\"Party\").size() * The two methods above return the same thing, except .value_counts() sorts by the values in decreasing order, while .groupby() sort by the index in increasing order!\n\n\n\nWrite a line of pandas code that returns a Series whose index is the years and whose values are the number of candidates that participated in those years’ elections.\n\n\nCode\nelections[\"Year\"].value_counts().head() #.head() to limit output\n\n\nYear\n1996    7\n1948    6\n1976    6\n2004    6\n2008    6\nName: count, dtype: int64\n\n\n\n\nCode\nelections.groupby(\"Year\").size().head() #.head() to limit output\n\n\nYear\n1824    2\n1828    2\n1832    3\n1836    3\n1840    2\ndtype: int64\n\n\n\n\nExplanation\n\n \n\nVery similar to Problem 3! Might even be easier, actually. Each row corresponds to one candidate per election cycle, so we simply need to count the number of times each Year appears in the elections DataFrame. Just like in Problem 3, two ways to do this.\n\nExtract the Year column as a Series, call .value_counts() on it.\n\nelections[\"Year\"].value_counts()\n\n\nGroup by the Year column, creating a sub-DataFrame for each unique Year. Aggregate by .size(), counting the number of rows in each sub-DataFrame.\n\nelections.groupby(\"Year\").size()\n\n\n\n\n\nWrite a line of pandas code that creates a filtered DataFrame named filtered_parties from the elections dataset and keeps only the parties that have at least one election % more than 50%.\n\n\nCode\nfiltered_parties = elections.groupby(\"Party\").filter(lambda df: df[\"%\"].max() &gt; 50)\nfiltered_parties\n\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\n\n\n1\n1824\nJohn Quincy Adams\nDemocratic-Republican\n113142\nwin\n42.789878\n\n\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\n\n\n4\n1832\nAndrew Jackson\nDemocratic\n702735\nwin\n54.574789\n\n\n7\n1836\nHugh Lawson White\nWhig\n146109\nloss\n10.005985\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n176\n2016\nHillary Clinton\nDemocratic\n65853514\nloss\n48.521539\n\n\n178\n2020\nJoseph Biden\nDemocratic\n81268924\nwin\n51.311515\n\n\n179\n2020\nDonald Trump\nRepublican\n74216154\nloss\n46.858542\n\n\n182\n2024\nDonald Trump\nRepublican\n77303568\nwin\n49.808629\n\n\n183\n2024\nKamala Harris\nDemocratic\n75019230\nloss\n48.336772\n\n\n\n\n99 rows × 6 columns\n\n\n\n\n\nExplanation\n\n \n\nThis filtering is different from boolean slicing! Boolean slicing considers rows individually, while .filter() considers groups of rows. Rows of a sub-DataFrame either all make it, or none make it.\n\nGroup by the Party column, creating one sub-DataFrame for each party.\n\nelections.groupby(\"Party\")\n\nFilter using .filter()\n\nPass in a function into .filter() that takes in a DataFrame and returns True or False. Can be a lambda function!\n.filter(lambda df: df[\"%\"].max() &gt; 50)\n\nIf the lambda function returns True, it means you keep the entire sub-DataFrame. False means you exclude it entirely!\n\n\n\n\n\n\nWrite a line of pandas code that uses the filtered_parties DataFrame to return a new DataFrame with row indices that correspond to the year and columns that correspond to each party. Each entry should be the total percentage of votes for all the candidates that ran during that particular year for the specified party. Missing values from the dataset (the cases where a party did not have a candidate in a particular year) should be entered as 0. Below is an example.\n\n\n\nCode\nelections_pivot = filtered_parties.pivot_table(\n    index = \"Year\",\n    columns = \"Party\",\n    values = \"%\",\n    aggfunc = np.sum,\n    fill_value = 0)\nelections_pivot.head(10)\n\n\n\n\n\n\n\n\nParty\nDemocratic\nDemocratic-Republican\nNational Union\nRepublican\nWhig\n\n\nYear\n\n\n\n\n\n\n\n\n\n1824\n0.000000\n100.0\n0.0\n0.000000\n0.000000\n\n\n1828\n56.203927\n0.0\n0.0\n0.000000\n0.000000\n\n\n1832\n54.574789\n0.0\n0.0\n0.000000\n0.000000\n\n\n1836\n52.272472\n0.0\n0.0\n0.000000\n47.727528\n\n\n1840\n46.948787\n0.0\n0.0\n0.000000\n53.051213\n\n\n1844\n50.749477\n0.0\n0.0\n0.000000\n49.250523\n\n\n1848\n42.552229\n0.0\n0.0\n0.000000\n47.309296\n\n\n1852\n51.013168\n0.0\n0.0\n0.000000\n44.056548\n\n\n1856\n45.306080\n0.0\n0.0\n33.139919\n0.000000\n\n\n1860\n0.000000\n0.0\n0.0\n39.699408\n0.000000\n\n\n\n\n\n\n\n\n\nExplanation\n\n \n\nFirst thing to notice is that the columns are values of the Party column! This tells us that what we see is a pivot table.\n\nUse the .pivot_table() function on filtered_parties\n\nindex = \"Year\" and columns = \"Party\", saying that the unique values of Year should make up the row indices, and the unique values of Party should make up the columns.\nvalues = \"%\" indicates that we populate the cells with the % values for each combination of Year, Party\naggfunc = np.sum describes how to aggregate the values in a cell\nfill_value = 0 says to impute 0 in case there is no % value for a specific Year, Party combination",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Pandas I</span>"
    ]
  },
  {
    "objectID": "disc03/disc03.html",
    "href": "disc03/disc03.html",
    "title": "4  Pandas II, EDA",
    "section": "",
    "text": "4.0.1 Link to Slides\nCode\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pandas II, EDA</span>"
    ]
  },
  {
    "objectID": "disc03/disc03.html#dealing-with-missing-data",
    "href": "disc03/disc03.html#dealing-with-missing-data",
    "title": "4  Pandas II, EDA",
    "section": "4.1 Dealing with Missing Data",
    "text": "4.1 Dealing with Missing Data\nWhile exploring a Berkeley dataset (separate from babynames) with a million records, you realize that a portion of measurements in different fields are NaN values! You decide to impute these missing values before continuing your EDA. Given the empirical distribution of each of the below variables, determine how to solve the missing data problem. (Note that the data in these graphs are fictional).\nSuppose that you plot “cups of coffee sold at V&A Cafe per day” versus “inches of rain per day” across a period of 2 months, shown below. V&A Cafe is not missing any data, but 30% of the data in “inches of rain” are NaN values that have been represented with “-2”, an impossible amount of rain. Which of the following techniques would be most effective in solving the issue of missing data? (Select all that apply)\n\n\n\nQuestion 1a\n\n\nA. Using the mean to impute the missing values\nB. Using the mode(s) to impute the missing values\nC. Using the median to impute the missing values\nD. Dropping any rows with missing values\nE. Imputing missing values through interpolation\n\n\nAnswer\n\nCorrect Options: A, C, E\n\nSuppose we examine the amount of money lost/gained in a game of poker and see that this variable is missing 1% of its values. Its distribution, shown below, is constructed from all valid (non-NaN) values. Which of the following techniques would be reasonably effective in solving this issue of missing data? (Select all that apply)\n\n\n\nQuestion 1b graph\n\n\nA. Using the mean to impute the missing values\nB. Using the mode(s) to impute the missing values\nC. Using the median to impute the missing values\nD. Dropping any rows with missing values\nE. Imputing missing values through interpolation\n\n\nAnswer\n\nCorrect Options: B, D\n\nSuppose that the relationship between students’ time asleep (in hours) and the amount of extra credit they received in Data 100 is shown below. There is no missing data for “hours asleep”, but 0.5% of “extra credit score” is missing. Like in part a, the missing NaN values were replaced with an impossible score of -0.002, making the graph look funky. Which of the following techniques would be most effective in solving this issue of missing data? (Select all that apply)\n\n\n\nQuestion 1c graph\n\n\nA. Using the mean to impute the missing values\nB. Using the mode(s) to impute the missing values\nC. Using the median to impute the missing values\nD. Dropping any rows with missing values\nE. Imputing missing values through interpolation\n\n\nAnswer\n\nCorrect Options: D, E",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pandas II, EDA</span>"
    ]
  },
  {
    "objectID": "disc03/disc03.html#pandas-eda-exam-prep-modeled-after-fa22-midterm-q1",
    "href": "disc03/disc03.html#pandas-eda-exam-prep-modeled-after-fa22-midterm-q1",
    "title": "4  Pandas II, EDA",
    "section": "4.2 Pandas + EDA exam prep (modeled after Fa22 Midterm Q1)",
    "text": "4.2 Pandas + EDA exam prep (modeled after Fa22 Midterm Q1)\nIt’s the annual Monopoly World Championship! The finalists: Shawn, Amanda, Neil, and Annie are playing Monopoly, a board game where players pay a price to buy properties, which can then generate income for them. Each property can be owned by only one player at a time. At the end of the game, the player with the most money wins.\nShawn wants to figure out which properties are most worth buying. He creates a DataFrame income with data on the current game state, shown on the left. He also finds a DataFrame properties with data on Monopoly properties, shown on the right.\nBoth tables have 28 rows. For brevity, only the first few rows of each DataFrame are shown.\n\n\nCode\n# First DataFrame: income\ndata_income = {\n    'Player': ['Shawn', 'Amanda', 'Neil', np.nan, 'Shawn', 'Annie', 'Amanda'],\n    'Property': ['Boardwalk', 'Park Place', 'Marvin Gardens', 'Kentucky Ave', 'Pennsylvania Ave', 'Oriental Ave', 'Baltic Ave'],\n    'Income Generated': ['$425', '$375', '$200', np.nan, '$150', '$50', '$60']\n}\n\nincome = pd.DataFrame(data_income)\nincome\n\n\n\n\n\n\n\n\n\nPlayer\nProperty\nIncome Generated\n\n\n\n\n0\nShawn\nBoardwalk\n$425\n\n\n1\nAmanda\nPark Place\n$375\n\n\n2\nNeil\nMarvin Gardens\n$200\n\n\n3\nNaN\nKentucky Ave\nNaN\n\n\n4\nShawn\nPennsylvania Ave\n$150\n\n\n5\nAnnie\nOriental Ave\n$50\n\n\n6\nAmanda\nBaltic Ave\n$60\n\n\n\n\n\n\n\nincome\n\nPlayer is the name of the player, as a str.\nProperty is a property currently owned by the player, as a str.\nIncome Generated is the amount of income a player has earned from that property so far, as a str.\n\n\n\nCode\n# Second DataFrame: properties\ndata_properties = {\n    'Property': ['Park Place', 'Oriental Ave', 'Vermont Ave', 'Pacific Ave', 'Boardwalk', 'Illinois Ave', 'Atlantic Ave'],\n    'Property Color': ['Dark Blue', 'Light Blue', 'Light Blue', 'Green', 'Dark Blue', 'Red', 'Yellow'],\n    'Purchase Price': [350.0, 100.0, 100.0, 300.0, 400.0, 240.0, 260.0]\n}\n\nproperties = pd.DataFrame(data_properties)\nproperties\n\n\n\n\n\n\n\n\n\nProperty\nProperty Color\nPurchase Price\n\n\n\n\n0\nPark Place\nDark Blue\n350.0\n\n\n1\nOriental Ave\nLight Blue\n100.0\n\n\n2\nVermont Ave\nLight Blue\n100.0\n\n\n3\nPacific Ave\nGreen\n300.0\n\n\n4\nBoardwalk\nDark Blue\n400.0\n\n\n5\nIllinois Ave\nRed\n240.0\n\n\n6\nAtlantic Ave\nYellow\n260.0\n\n\n\n\n\n\n\nproperties\n\nProperty is the name of the property, as a str. There are 28 unique properties.\nProperty Color is a color group that the property belongs to, as a str. There are 10 unique color groups, and each property belongs to a single group.\nPurchase Price is the price to buy the property, as a float.\n\nNote: For the properties that are not currently owned by any player, the Player and Income Generated columns in the income table have a NaN value.\n(a) What is the granularity of the income table?\n\n\nAnswer\n\n\nProperty\n\nEach unique property has its own row\nNotice how one player can have own multiple properties and can appear in multiple rows! This tells us that the granularity of this table is not Player.\n\n\n(b) Consider the Player and Purchase Price variables. What type of variable is each one? (quantitative, qualitative nominal, qualitative ordinal)\n\n\nAnswer\n\n\nPlayer: Qualitative nominal\nPurchase Price: Quantitative\n\n\n(c) Which of the following line(s) of code successfully returns a Series with the number of properties each player owns? Select all that apply.\n\n\nAnswer\n\n\nincome[\"Player\"].value_counts()\nincome.groupby(\"Player\").size()\n\n\n\n\nCode\nincome.groupby(\"Player\").agg(pd.value_counts)\n\n\n\n\n\n\n\n\n\nProperty\nIncome Generated\n\n\nPlayer\n\n\n\n\n\n\nAmanda\n[1, 1]\n[1, 1]\n\n\nAnnie\n1\n1\n\n\nNeil\n1\n1\n\n\nShawn\n[1, 1]\n[1, 1]\n\n\n\n\n\n\n\n\n\nCode\nincome[\"Player\"].value_counts()\n\n\nPlayer\nShawn     2\nAmanda    2\nNeil      1\nAnnie     1\nName: count, dtype: int64\n\n\n\n\nCode\n# income[\"Player\", \"Property\"].groupby(\"Player\").size()\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe above code will error! Make sure to use double brackets when selecting columns.\n\n\n\n\nCode\nincome.groupby(\"Player\")[[\"Player\"]].count()\n\n\n\n\n\n\n\n\n\nPlayer\n\n\nPlayer\n\n\n\n\n\nAmanda\n2\n\n\nAnnie\n1\n\n\nNeil\n1\n\n\nShawn\n2\n\n\n\n\n\n\n\n(d) He now decides to calculate the amount of profit from each property. He wants to store this in a column called Profit in the income DataFrame. To do this, he first has to transform the Income Generated column to be of a float datatype.\nWrite one line of code to replace the old column with a new column, also called Income Generated, with the datatype modification described above. You may assume that each entry in Income Generated consists of a dollar sign ($) followed by a number, except for the NaN values.\n\n\nCode\nincome[\"Income Generated\"] = income[\"Income Generated\"].str[1:].astype(float)\n\n\n(e) Assuming that the answer to (d) is correct, let’s add a Profit column to the income DataFrame. Fill in the following blanks to do this, and please add arguments to function class as you see appropriate.\nNote: Profit is calculated by subtracting the purchase price from generated income.\ncombined_df = income._____A_____(_______B_______)\nincome[\"Profit\"] = _______C_______\n\n\nCode\ncombined_df = income.merge(properties, on = \"Property\")\nincome[\"Profit\"] = combined_df[\"Income Generated\"] - combined_df[\"Purchase Price\"]\n\n\nShawn realizes he’s lost more money than he’s made. To solve this problem, he begins by writing some Pandas code to merge the Property Color column into the income DataFrame and drops all rows with NaN values. He calls this DataFrame merged_df. Shown below are the first few rows.\n\n\nCode\nmerged_df = pd.DataFrame({\"Player\": [\"Shawn\", \"Amanda\", \"Neil\", \"Shawn\", \"Annie\", \"Amanda\"],\n                          \"Property\": [\"Boardwalk\", \"Park Place\", \"Marvin Gardens\", \"Pennsylvania Ave\", \"Oriental Ave\", \"Baltic Ave\"],\n                          \"Income Generated\": [425., 375., 200., 150., 50., 60.],\n                          \"Profit\": [-25., 25., 50., -100., 0., 0.],\n                          \"Property Color\": [\"Dark Blue\", \"Dark Blue\", \"Yellow\", \"Green\", \"Light Blue\", \"Purple\"]})\nmerged_df\n\n\n\n\n\n\n\n\n\nPlayer\nProperty\nIncome Generated\nProfit\nProperty Color\n\n\n\n\n0\nShawn\nBoardwalk\n425.0\n-25.0\nDark Blue\n\n\n1\nAmanda\nPark Place\n375.0\n25.0\nDark Blue\n\n\n2\nNeil\nMarvin Gardens\n200.0\n50.0\nYellow\n\n\n3\nShawn\nPennsylvania Ave\n150.0\n-100.0\nGreen\n\n\n4\nAnnie\nOriental Ave\n50.0\n0.0\nLight Blue\n\n\n5\nAmanda\nBaltic Ave\n60.0\n0.0\nPurple\n\n\n\n\n\n\n\nShawn decides he will now only buy properties from a color group that he deems “profitable.” He deems a color group “profitable” if at least 50% of the properties in the group that are currently owned by players have made a positive (non-zero) profit for those players.\nFill in the following lines of code to help him display a DataFrame with a subset of the rows in merged_df: the rows with properties that belong to profitable color groups. Your solution may use fewer lines of code than we provide.\n\n\nCode\ndef func(group):\n    if np.mean(group[\"Profit\"] &gt; 0) &gt;= 0.5:\n        return True\n    return False\n\nmerged_df.groupby(\"Property Color\").filter(func)\n\n\n\n\n\n\n\n\n\nPlayer\nProperty\nIncome Generated\nProfit\nProperty Color\n\n\n\n\n0\nShawn\nBoardwalk\n425.0\n-25.0\nDark Blue\n\n\n1\nAmanda\nPark Place\n375.0\n25.0\nDark Blue\n\n\n2\nNeil\nMarvin Gardens\n200.0\n50.0\nYellow",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pandas II, EDA</span>"
    ]
  },
  {
    "objectID": "disc04/disc04.html",
    "href": "disc04/disc04.html",
    "title": "5  Regex, Visualization, and Transformation",
    "section": "",
    "text": "5.0.1 Link to Slides",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regex, Visualization, and Transformation</span>"
    ]
  },
  {
    "objectID": "disc04/disc04.html#regular-expressions",
    "href": "disc04/disc04.html#regular-expressions",
    "title": "5  Regex, Visualization, and Transformation",
    "section": "5.1 Regular Expressions",
    "text": "5.1 Regular Expressions\nRegular Expressions (RegEx for short) are an immensely powerful tool for parsing strings. However, it’s many rules make RegEx very confusing, even for veteran users, so please don’t hesitate to ask questions! Here’s a snippet of the RegEx portion of the Fall 2023 Midterm\nReference Sheet:\n\n\n\nRegex Reference\n\n\n\n5.1.1 (a)\nWhich string contains a match for the following regular expression, “\\(\\texttt{1+1\\$}\\)”?\n\n\\(\\texttt{What is 1+1}\\)\n\\(\\texttt{Make a wish at 11:11}\\)\n\\(\\texttt{111 Ways to Succeed}\\)\n\n\n\nAnswer\n\n\n\\(\\texttt{Make a wish at 11:11}\\)\n\nNote that there are two operators here, the + and the $. The $ operator states that our match must occur at the end of the string, so that already rules out the third option. Furthermore, the + operator indicates that we need “one or more 1s” which is followed by another 1 (so two or more 1s in total). The only string ending with two or more consecutive 1s is the second option.\n\n\n\n5.1.2 (b)\nWrite a regular expression that matches a string which contains only one word containing only lowercase letters and numbers (including the empty string).\n\n\nAnswer\n\n\\(\\texttt{\\^{}[a-z0-9]*\\$}\\)\n\n\\(\\texttt{[a-z0-9]}\\) to indicate that the word only include lowercase letters and/or numbers\n\\(\\texttt{*}\\) operator to state that the word can contain “0 or more” of \\(\\texttt{[a-z0-9]}\\). Note that we also want to match empty strings, which is why \\(\\texttt{*}\\) is preferred over \\(\\texttt{+}\\) here.\n\\(\\texttt{\\^{}}\\) and \\(\\texttt{\\$}\\) to indicate that the pattern must match the string at the beginning of the string, and at the end of the string respectively. This ensures that the pattern only matches strings with one word (not two or more words).\n\n\n\n\n5.1.3 (c)\nGiven \\(\\texttt{sometext = \"I've got 10 eggs, 20 gooses, and 30 giants.\"}\\), use \\(\\texttt{re.findall}\\) to extract all items and quantities from the string. The result should look like \\(\\texttt{[\"10 eggs\", \"20 gooses\", \"30 giants\"]}\\). You may assume that a space separates quantity and type, and that each item ends in s.\n\n\nAnswer\n\n\\(\\texttt{re.findall(r\"\\\\d+\\\\s\\\\w+\", sometext)}\\)\nThe strings we want to match begin with a number, followed by a space, followed by a word. We can use this to construct our pattern.\n\n\\(\\texttt{\\\\d+}\\) to match one or more digits\n\\(\\texttt{\\\\s}\\) to match a single space\n\\(\\texttt{\\\\w+}\\) to match one or more word characters \\(\\texttt{[A-Za-z0-9\\_]}\\)\n\n\n\n\n5.1.4 (d)\nFor each pattern specify the starting and ending position of the first match in the string. The index starts at zero and we are using closed intervals (both endpoints are included).\n\n\n\n\n\n\n\n\n\n\n\n\\(\\texttt{abcdefg}\\)\n\\(\\texttt{abcs!}\\)\n\\(\\texttt{ab abc}\\)\n\\(\\texttt{abc, 123}\\)\n\n\n\n\n\\(\\texttt{abc*}\\)\n[0,2]\n\n\n\n\n\n\\(\\texttt{[ \\^{} \\\\s]+}\\)\n\n\n\n\n\n\n\\(\\texttt{ab.*c}\\)\n\n\n\n\n\n\n\\(\\texttt{[a-z1,9]+}\\)\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\texttt{abcdefg}\\)\n\\(\\texttt{abcs!}\\)\n\\(\\texttt{ab abc}\\)\n\\(\\texttt{abc, 123}\\)\n\n\n\n\n\\(\\texttt{abc*}\\)\n[0,2]\n[0,2]\n[0,1]\n[0,2]\n\n\n\\(\\texttt{[ \\^{} \\\\s]+}\\)\n[0,6]\n[0,4]\n[0,1]\n[0,3]\n\n\n\\(\\texttt{ab.*c}\\)\n[0,2]\n[0,2]\n[0,5]\n[0,2]\n\n\n\\(\\texttt{[a-z1,9]+}\\)\n[0,6]\n[0,3]\n[0,1]\n[0,3]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regex, Visualization, and Transformation</span>"
    ]
  },
  {
    "objectID": "disc04/disc04.html#visualizing-bigfoot",
    "href": "disc04/disc04.html#visualizing-bigfoot",
    "title": "5  Regex, Visualization, and Transformation",
    "section": "5.2 Visualizing Bigfoot",
    "text": "5.2 Visualizing Bigfoot\nMany of you have probably heard of Bigfoot before. It’s a mysterious ape-like creature that is said to live in North American forests. Most doubt its existence, but a passionate few swear that Bigfoot is real. In this discussion, you will be working with a dataset on Bigfoot sightings, visualizing variable distributions and combinations thereof to better understand how/when/where Bigfoot is reportedly spotted, and possibly either confirm or cast doubt on its existence. The Bigfoot data contains a ton of variables about each reported Bigfoot spotting, including location information, weather, and moon phase.\n\n\nCode\n# Importing packages\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n\nCode\n# Loading bigfoot data\n\nurl = 'https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-09-13/bigfoot.csv'\ndf = pd.read_csv(url)\n\n\nThis dataset is extremely messy, with observations missing many values across multiple columns. This is normally the case with data based on citizen reports (many do not fill out all required fields). For the purposes of this discussion, we will drop all observations with any missing values and some unneeded columns. However, note this is not a good practice and you should almost never do this in real life!\n\n\nCode\n# Drop unneeded rows and observations with missing values\n\nbigfoot = df.dropna().rename({'temperature_high':'temp_high' ,'temperature_low':'temp_low'},axis = 1)\nbigfoot = bigfoot.drop(['observed', 'location_details', 'county', 'state', 'title',\n       'latitude', 'longitude', 'number', 'classification', 'geohash',\n       'temperature_mid', 'dew_point','precip_probability', 'precip_type','summary', \n       'wind_bearing'], axis = 1)\n\n\nHere are the first few entries of the bigfoot table:\n\n\nCode\nbigfoot.head(5)\n\n\n\n\n\n\n\n\n\nseason\ndate\ntemp_high\ntemp_low\nhumidity\ncloud_cover\nmoon_phase\nprecip_intensity\npressure\nuv_index\nvisibility\nwind_speed\n\n\n\n\n10\nSummer\n2016-06-07\n74.69\n53.80\n0.79\n0.61\n0.10\n0.0010\n998.87\n6.0\n9.70\n0.49\n\n\n21\nSummer\n2015-10-02\n49.06\n44.24\n0.87\n0.93\n0.67\n0.0092\n1022.92\n3.0\n9.16\n2.87\n\n\n32\nFall\n2009-10-31\n69.01\n34.42\n0.77\n0.81\n0.42\n0.0158\n1011.48\n3.0\n1.97\n3.94\n\n\n34\nSummer\n1978-07-15\n68.56\n63.05\n0.88\n0.80\n0.33\n0.0285\n1014.70\n5.0\n5.71\n5.47\n\n\n55\nSummer\n2015-11-26\n20.49\n5.35\n0.65\n0.08\n0.54\n0.0002\n1037.98\n1.0\n10.00\n0.40\n\n\n\n\n\n\n\nLet’s first look at distributions of individual quantitative variables. Let’s say we’re interested in wind_speed.\n\n5.2.1 (a)\nWhich of the following are appropriate visualizations for plotting the distribution of a quantitative variable? (Select all that apply.)\nA. Pie charts\nB. Kernel Density Plot\nC. Scatter plot\nD. Box plot\nE. Histogram\nF. Hex plot\n\n\n\nAnswer\n\nKernel Density Plot, Box plot, Histogram\nA Pie chart would not be appropriate because they are used to visualize the distribution of categories, or of a single qualitative variables. Scatter plots and Hex plots are also not appropriate as they visualize the relationship between two quantitative variables.\n\n\n5.2.2 (b)\nWrite a line of code that produces a visualization that depicts the variable’s distribution (example shown below).\n\n\n\nwindspeed\n\n\n\n\nCode\nsns.histplot(data = bigfoot, x = \"wind_speed\", kde = True);\n\n\n\n\n\n\n\n\n\nNote that kde = True is required to overlay the KDE curve over the actual histogram!\n\n\n5.2.3 (c)\nNow, let’s look at some qualitative variables. Write a line of code that produces a visualization that shows the distribution of bigfoot sightings across the variable season (example shown below).\nHint: Use seaborn’s sns.countplot or matplotlib’s plt.bar.\n\n\n\nCode\nsns.countplot(data = bigfoot, x = \"season\");\n\n\n\n\n\n\n\n\n\nIn order to also replicate the colors of the bars, you would need to manually specify them.\n\n\nCode\nsns.countplot(data = bigfoot, x = \"season\", palette = [\"blue\", \"orange\", \"green\", \"red\", \"purple\"]);\n\n\n\n\n\n\n\n\n\nYou could have alternatively used Matplotlib!\n\n\nCode\nseason_counts = bigfoot[\"season\"].value_counts()\nplt.bar(season_counts.index, season_counts.values, color = [\"blue\", \"orange\", \"green\", \"red\", \"purple\"]);\n\n\n\n\n\n\n\n\n\n\n\n5.2.4 (d)\nFinally, produce a single visualization that showcases how the prevalence of bigfoot sightings at particular combinations of moon_phase and wind_speed vary across each season.\nHint: Think about color as the third information channel in the plot.\n\n\n\nCode\nsns.scatterplot(data = bigfoot,\n                x = \"moon_phase\",\n                y = \"wind_speed\",\n                hue = \"season\",\n                alpha = 0.2);\n\n\n\n\n\n\n\n\n\nNote the two rather unfamiliar arguments:\n\nhue specifies which column of bigfoot we want to color the points according to\nalpha specifies how transparent the points should be. Higher values of alpha lead to more opaque points.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regex, Visualization, and Transformation</span>"
    ]
  },
  {
    "objectID": "disc04/disc04.html#kernel-density-estimation-kde",
    "href": "disc04/disc04.html#kernel-density-estimation-kde",
    "title": "5  Regex, Visualization, and Transformation",
    "section": "5.3 Kernel Density Estimation (KDE)",
    "text": "5.3 Kernel Density Estimation (KDE)\nKernel Density Estimation is used to estimate a probability density function (or density curve) from a set of data. A kernel with a bandwidth parameter α is placed on data observations \\(x_i\\) with \\(i ∈ \\{1, ..., n\\}\\), and the density estimation is calculated by averaging all kernels. Below, Gaussian and Boxcar kernel equations are listed:\n\nGaussian Kernel: \\(K_{\\alpha}(x, x_i) = \\frac{1}{\\sqrt{2\\pi\\alpha^2}}\\exp({-\\frac{(x-x_i)^2}{2\\alpha^2}})\\)\nBoxcar Kernel: \\(B_{\\alpha}(x, x_i) = \\begin{cases}\n    \\frac{1}{\\alpha} & \\text{ if }-\\frac{\\alpha}{2} \\leq x - x_i \\leq \\frac{\\alpha}{2} \\\\\n    0 & \\text{ else}\n\\end{cases}\\)\n\nThe KDE is calculated as follows: \\(f_\\alpha(x) = \\frac{1}{n}\\sum_{i = 1}^{n} K_\\alpha(x, x_i)\\).\n\n5.3.1 (a)\nDraw a KDE plot (by hand is fine) for data points [1, 4, 8, 9] using Gaussian Kernel and \\(\\alpha = 1\\). On the plot show \\(x\\), \\(x_i\\), \\(\\alpha\\), and the KDE.\n\n\nAnswer\n\nWith \\(\\alpha = 1\\), we get a Gaussian Kernel of \\(K_{1}(x, x_i) = \\frac{1}{\\sqrt{2 \\pi}} \\exp\\left(-\\frac{(x - x_i)^2}{2} \\right)\\).\nThis kernel is greatest when \\(x = x_i\\), giving us maximum point at \\[K_{1}(x, x) = \\frac{1}{\\sqrt{2 \\pi}} = 0.3989 \\approx 0.4\\]\nEach individual kernel is a Gaussian centered, respectively, at . Since we have 4 kernels, each with an area of 1, we normalize by dividing each kernel by 4. This gives us a maximum height of \\(0.1\\). We then sum those kernels together to obtain the final KDE plot:\n\n\n\nKDE\n\n\n\n\n\n5.3.2 (b)\nWe wish to compare the results of KDE using a Gaussian kernel and a boxcar kernel. For \\(\\alpha &gt; 0\\), which of the following statements is true? Choose all that apply.\nA. Decreasing \\(\\alpha\\) for a Gaussian kernel decreases the smoothness of the KDE.\nB. The Gaussian kernel is always better than the boxcar kernel for KDEs.\nC. Because the Gaussian kernel is smooth, we can safely use large \\(\\alpha\\) values for kernel density estimation without worrying about the actual distribution of data.\nD. The area under the boxcar kernel is 1, regardless of the value of \\(\\alpha\\).\nE. None of the above.\n\n\nAnswer\n\nCorrect options: A, D\nB is false because a boxcar kernel can perform better is \\(\\alpha\\) is not chosen properly for the Gaussian kernel\nC is false because an \\(\\alpha\\) that is too high risks including too many points in the estimate, resulting in a flatter curve.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regex, Visualization, and Transformation</span>"
    ]
  },
  {
    "objectID": "disc05/disc05.html",
    "href": "disc05/disc05.html",
    "title": "6  Transformations, Sampling, and SLR",
    "section": "",
    "text": "6.0.1 Link to Slides",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Transformations, Sampling, and SLR</span>"
    ]
  },
  {
    "objectID": "disc05/disc05.html#logarithmic-transformations",
    "href": "disc05/disc05.html#logarithmic-transformations",
    "title": "6  Transformations, Sampling, and SLR",
    "section": "6.1 Logarithmic Transformations",
    "text": "6.1 Logarithmic Transformations\nIshani is a development economist interested in studying the relationship between literacy rates and gross national income in countries across the world. Originally, she plotted the data on a linear (absolute) scale, shown on the left. She noticed that the non-linear relationship between the variables with a lot of points clustered towards the larger values of literacy rate, so she consults the Tukey-Mosteller Bulge diagram and decides to do a \\(\\log_{10}\\) transformation of the \\(y\\)-axis, shown on the right. The solid blue line is a “line of best fit” (we’ll formalize this later in the course).\n\n\n\nlog transform plot\n\n\n\n6.1.1 (a)\nInstead of using the \\(\\log_{10}\\) transformation of the \\(y\\)-axis, what other transformations could Ishani have used to attempt to linearize the relationship between literacy rate (\\(x\\)) and gross national income per capita (\\(y\\)). Select all that apply.\nA. \\(\\log_e(y)\\)\nB. \\(10^y\\)\nC. \\(\\sqrt{x}\\)\nD. \\(x^2\\)\nE. \\(y^2\\)\n\n\nAnswer\n\n\n\\(\\log_e(y)\\), \\(x^2\\)\n\nThe original plot displays a very strong non-linear relationship that looks exponential. For large values of \\(x\\), and increase in \\(x\\) is matched with a very significant increase in \\(y\\). Therefore, we would want to apply a transformation that makes \\(x\\) values larger or \\(y\\) values smaller.\n\\(x^3\\), \\(\\sqrt{y}\\), and \\(\\log(y)\\) are also valid as suggested by the Tukey-Mosteller Bulge Diagram.\n\n\n\n6.1.2 (b)\nLet \\(C\\) and \\(k\\) be some constant values and \\(x\\) and \\(y\\) represent literacy rate and gross national income per capita, respectively. Based on the plots, which of the following best describes the pattern seen in the data?\nA. \\(y = C + kx\\)\nB. \\(y = C*10^{kx}\\)\nC. \\(y = C + k\\log_{10}(x)\\)\nD. \\(y = Cx^k\\)\n\n\nAnswer\n\n\n\\(y = C*10^{kx}\\)\n\nThe basic format of a regression line is \\(y = kx + b\\). Noticed that the plot on the right applied a log transformation to the \\(y\\) axis, so we can apply that same transformation to the equation to derive the true relationship.\n\\[\\begin{align}\n\\log_{10}(y) &= kx + b\\\\\ny &= 10^{kx + b}\\\\\ny &= 10^b*10^{kx}\n\\end{align}\\]\nwhere \\(C = 10^b\\)\n\n\n\n6.1.3 (c)\nWhat parts of the plot could you use to make initial guesses on \\(C\\) and \\(k\\)?\n\n\nAnswer\n\n\n\\(C\\): \\(b\\) is the y-intercept of the transformed plot, and \\(C = 10^b\\)\n\\(k\\): Slope of the regression line in the transformed plot\n\n\n\n\n6.1.4 (d)\nIshani’s friend, Yash, points to the solid line on the transformed plot and says “since this line is going up and to the right, we can say that, in general, the higher the literacy rate, the greater the gross national income per capita”. Is this a reasonable interpretation of the plot?\n\n\nAnswer\n\nYes, the observation is equivalent to saying that the slope is positive, which means increases in \\(x\\) correspond to increases in \\(y\\). This does not mean higher literacy rates cause higher gross national incomes, just that they are positively correlated.\n\n\n\n6.1.5 (e)\nSuppose that instead of plotting positive quantities, our data contained some zero and negative values. How can we reasonably apply a logarithmic transform to this data?\n\n\nAnswer\n\nRecall that logarithms are not defined for 0 or negative values. Thus, we must first make all of our values positive. Suppose our data consists of three points, \\([-3, -2, 4]\\).\n\nAdd the magnitude of the smallest number to each value to make all values non-negative: \\([0, 1, 7]\\)\nAdd a small positive number to each value (e.g., \\(1\\)) to make all values positive: \\([1, 2, 8]\\)\n\nNow, it is possible to take the logarithm of each value! Note that the steps above are merely shifting the data, they are not changing any underlying linear or non-linear relationships.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Transformations, Sampling, and SLR</span>"
    ]
  },
  {
    "objectID": "disc05/disc05.html#data-collection-through-sampling",
    "href": "disc05/disc05.html#data-collection-through-sampling",
    "title": "6  Transformations, Sampling, and SLR",
    "section": "6.2 Data Collection through Sampling",
    "text": "6.2 Data Collection through Sampling\nIt’s time for the Data 100 midterm, and the professors want to estimate the difficulty of the exam. They decided to survey students on the exam’s difficulty with a 10-point scale and then use the mean of the student’s responses as the estimate.\n\n6.2.1 (a)\nWhat is the population the professors are interested in trying to understand?\nA. Students in Data 100\nB. Students enrolled in the Data 100 Ed\nC. Students who attend the Data 100 lectures\nD. Students who took the Data 100 midterm\n\n\nAnswer\n\nD. Students who took the Data 100 midterm\nThe professors are only interested in the students who actually took the midterm. Some students in the first three options might not have taken the midterm exam!\n\n\n\n6.2.2 (b)\nThe professors consider a few different methods for collecting the survey data about the midterm. Which of the following methods is best? (think through which considerations go into “best”)\nA. The professors add one Slido poll in the first lecture following the exam and only consider synchronous responses.\nB. The professors add a question to the homework assignments of a simple random sample of students within every discussion section.\nC. The professors make a post on Ed asking students to submit a Google Form containing the survey question.\nD. The professors choose a simple random sample of discussion sections, go to each selected section and ask each student in the group as part of the final discussion question.\n\n\nAnswer\n\nB. The professors add a question to the homework assignments of a simple random sample of students within every discussion section.\nAlthough not perfect, B describes the best method out of the four! This method samples randomly from uniformly from students across discussion times, as well as providing strong incentive for students to answer. The only issue would be the fact that the sampling frame does not include students who don’t have a discussion section.\nHere are reasons why the other options are not as good:\n\nA: Sample only from students who attend synchronously, introducing selection bias.\nC: Not all students check Ed periodically and the survey is optional, introducing selection bias and non-response bias.\nD: Room for social pressure, which can bias the results of the survey. Also, there may be systematic or inherent differences between discussion sections.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Transformations, Sampling, and SLR</span>"
    ]
  },
  {
    "objectID": "disc05/disc05.html#simple-linear-regression",
    "href": "disc05/disc05.html#simple-linear-regression",
    "title": "6  Transformations, Sampling, and SLR",
    "section": "6.3 Simple Linear Regression",
    "text": "6.3 Simple Linear Regression\nLillian and Prabhleen were watching their favorite chemistry Youtuber NileRed experimenting with turning gloves into grape soda and wanted to try it themselves. The experiment was done at various temperatures and yielded various amounts of grape soda. Since this reaction is very costly, they were only able to do it 10 times. This data set of size \\(n = 10\\) (Yield data) contains measurements of yield from an experiment done at five different temperature levels. The variables are \\(y\\) = yield in liters and \\(x\\) = temperature in degrees Fahrenheit. Below is a scatter plot of our data. \n\n\n\n\\(\\sigma_x\\)\n\\(\\sigma_y\\)\n\\(r\\)\n\\(\\bar{x}\\)\n\\(\\bar{y}\\)\n\n\n\n\n15\n0.3\n0.50\n75.00\n3\n\n\n\n\n6.3.1 (a)\nGiven the above statistics, calculate the slope (\\(\\hat{\\theta_1}\\)) and y-intercept (\\(\\hat{\\theta_0}\\)) of the line of best fit using Mean Squared Error (MSE) as our loss function and plot the line on the graph above:\n\\[\\begin{align*}\ny = \\hat{\\theta_0} + \\hat{\\theta_1}x\n\\end{align*}\\]\n\n\nAnswer\n\n\\[\\begin{align*}\n\\hat{\\theta_1} &= r*\\frac{\\sigma_y}{\\sigma_x}\\\\\n\\hat{\\theta_1} &= 0.5*\\frac{0.3}{15}\\\\\n&= 0.01\\\\\\\\\n\\hat{\\theta_0} &= \\bar{y} - \\hat{\\theta_1}\\bar{x}\\\\\n\\hat{\\theta_0} &= 3 - 75.00 * 0.01\\\\\n&= 3 - 0.75\\\\\n&= 2.25\n\\end{align*}\\]\n\n\n\nYield Temp Regression Line\n\n\nNote that the \\(\\sigma_x\\) and \\(\\sigma_y\\) values in the table are slightly different from what can be seen in the plot in order to make the calculations easier! As a result, the slope and intercept values you obtained may also be slightly different from what you see in the plot.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Transformations, Sampling, and SLR</span>"
    ]
  },
  {
    "objectID": "disc06/disc06.html",
    "href": "disc06/disc06.html",
    "title": "7  Models, OLS",
    "section": "",
    "text": "7.0.1 Link to Slides",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Models, OLS</span>"
    ]
  },
  {
    "objectID": "disc06/disc06.html#driving-with-a-constant-model",
    "href": "disc06/disc06.html#driving-with-a-constant-model",
    "title": "7  Models, OLS",
    "section": "7.1 Driving with a Constant Model",
    "text": "7.1 Driving with a Constant Model\nLillian is trying to use modeling to drive her car autonomously. To do this, she collects a lot of data from driving around her neighborhood and stores it in drive. She wants your help to design a model that can drive on her behalf in the future using the outputs of the models you design. First, she wants to tackle two aspects of this autonomous car modeling framework: going forward and turning. Some statistics from the collected dataset are shown below using drive.describe(), which returns the mean, standard deviation, quartiles, minimum, and maximum for the two columns in the dataset: target_speed and degree_turn.\n\n\n\n\ntarget_speed\ndegree_turn\n\n\n\n\ncount\n500.000000\n500.000000\n\n\nmean\n32.923408\n143.721153\n\n\nstd\n46.678744\n153.641504\n\n\nmin\n0.231601\n0.000000\n\n\n25%\n12.350025\n6.916210\n\n\n50%\n25.820689\n45.490086\n\n\n75%\n39.788716\n323.197168\n\n\nmax\n379.919965\n359.430309\n\n\n\n\n7.1.1 (a)\nSuppose the first part of the model predicts the target speed of the car. Using constant models trained on the speeds of the collected data shown above with \\(L_1\\) and \\(L_2\\) loss functions, which of the following is true?\nA. The model trained with the \\(L_1\\) loss will always drive slower than the model trained with \\(L_2\\) loss.\nB. The model trained with the \\(L_2\\) loss will always drive slower than the model trained with \\(L_1\\) loss.\nC. The mode trained with the \\(L_1\\) loss will sometimes drive slower than the model trained with \\(L_2\\) loss.\nD. The model trained with the \\(L_2\\) loss will somtimes drive slower than the model trained with \\(L_1\\) loss.\n\n\nAnswer\n\nA. The model trained with the \\(L_1\\) loss will always drive slower than the model trained with \\(L_2\\) loss.\nRemember that when we have a constant model, using \\(L_1\\) will always predict the median while \\(L_2\\) loss will always predict the mean. Since the median (\\(25.82\\)) is lower than the mean (\\(32.92\\)), we know that the model trained on \\(L_1\\) loss will always drive slower.\n\n\n\n7.1.2 (b)\nFinding that the model trained with the \\(L_2\\) loss drives too slowly, Lillian changes the loss function for the constant model where the loss is penalized more if the true speed is higher. That way, in order to minimize loss, the model would have to output predictions closer to the true value, particularly as speeds get faster, the end result being a higher constant speed. Lillian writes this as \\(L(y, \\hat{y}) = y(y-\\hat{y})^2\\).\nFind the optimal \\(\\hat{\\theta}_0\\) for the constant model using the new empirical risk function \\(R(\\theta_0)\\) below:\n\\[\\begin{align*}\nR(\\theta_0) = \\frac{1}{n}\\sum_i y_i(y_i-\\theta_0)^2\n\\end{align*}\\]\n\n\nAnswer\n\nTake the derivative: \\[\\begin{align}\n\\frac{dR}{d\\theta_0} &= \\frac{1}{n} \\sum_i \\frac{d}{d\\theta_0}y_i (y_i - \\theta_0)^2 \\\\\n&= \\frac{1}{n} \\sum_i -2y_i (y_i - \\theta_0) = -\\frac{2}{n} \\sum_i y_i^2 - y_i\\theta_0\n\\end{align}\\] Set the derivative to 0: \\[\\begin{align}\n-\\frac{2}{n} \\sum_i y_i^2 - y_i\\theta_0 = 0 \\\\\n\\theta_0 \\sum_i y_i = \\sum_i y_i^2\\\\\n\\theta_0 = \\frac{\\sum_i y_i^2}{ \\sum_i y_i }\n\\end{align}\\]\nNote that the empirical risk function is convex, which you can show by computing the second derivative of \\(R(\\theta_0)\\) and noting that it is positive for all values of \\(\\theta_0\\).\n\\[\\frac{d^2R}{d\\theta_0^2} = \\frac{2}{n} \\sum_i y_i &gt; 0\\] since \\(y_i\\) represents speed, also validated by the fact that the minimum value is positive.\nTherefore, any critical point must be the global minimum and so the optimal value we found minimizes empirical risk.\n\n\n\n7.1.3 (c)\nLillian’s friend, Yash, also begins working on a model that predicts the degree of turning at a particular time between 0 and 359 degrees using the data in the degree_turn column. Explain why a constant model is likely inappropriate in this use case.\nExtra: If you’ve studied some physics, you may recognize the behavior of our constant model!\n\n\nAnswer\n\nAny constant model will essentially be always turning at an angle and will be unable to turn either direction or go straight (i.e., it’ll essentially go in a circle forever).\n\n\n\n7.1.4 (d)\nSuppose we finally expand our modeling framework to use simple linear regression (i.e., \\(f_\\theta(x) = \\theta_{w,0}+\\theta_{w,1}x\\)) For our first simple linear regression model, we predict the turn angle (\\(y\\)) using target speed (\\(x\\)). Our optimal parameters are: \\(\\hat{\\theta}_{w, 1}=0.019\\) and \\(\\hat{\\theta}_{w,0}=143.1\\)\nHowever, we realize that we actually want a model that predicts target speed (our new \\(y\\)) using turn angle, our new \\(x\\) (instead of the other way around)! What are our new optimal parameters for this new model?\n\n\nAnswer\n\nTo predict target speed (new \\(y\\)) from turn angle (new \\(x\\)), we need to compute \\(\\hat{\\theta}_1=r*\\frac{\\sigma_{\\text{speed}}}{\\sigma_{\\text{turn}}}\\)\nWhen we predicted the turn angle from target speed, we computed \\(\\hat{\\theta}_{w,1}=r*\\frac{\\sigma_{\\text{turn}}}{\\sigma_{\\text{speed}}} = 0.019\\)\nTherefore, \\(\\hat{\\theta}_1 = r * \\frac{\\sigma_{\\text{speed}}}{\\sigma_{\\text{turn}}} = (r*\\frac{\\sigma_{\\text{turn}}}{\\sigma_{\\text{speed}}})\\frac{\\sigma_{\\text{speed}}^2}{\\sigma_{\\text{turn}}^2} = 0.019 * \\frac{46.678744^2}{153.641504^2} \\approx 0.00175\\)\nThen, \\(\\hat{\\theta}_0 = \\bar{y} - \\hat{\\theta}_1\\bar{x} = 32.92 - 0.00175*143.72 = 32.67\\)\nNote that \\(\\bar{y}\\) and \\(\\bar{x}\\) are the means of target speed and turn angle respectively.\n\n\n\n\n\n\nNote\n\n\n\nWe can’t use the inverse function \\(f_\\theta^{-1}(x)\\) since minimizing the sum of squared vertical residuals is not the inverse problem of minimizing the sum of squared horizontal residuals.\n\nThe slope of the line that minimizes \\(MSE(y, \\hat{y})\\) is \\(r*\\frac{\\sigma_y}{\\sigma_x}\\)\nThe slope of the line that minimizes \\(MSE(x, \\hat{x})\\) is \\(r*\\frac{\\sigma_x}{\\sigma_y}\\)\n\nThey are not inverses! The visualization below shows the difference between minimizing horizontal and vertical residuals.\n\n\n\nHorizontal and Vertical Residuals",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Models, OLS</span>"
    ]
  },
  {
    "objectID": "disc06/disc06.html#geometry-of-least-squares",
    "href": "disc06/disc06.html#geometry-of-least-squares",
    "title": "7  Models, OLS",
    "section": "7.2 Geometry of Least Squares",
    "text": "7.2 Geometry of Least Squares\nSuppose we have a dataset represented with the design matrix span(\\(\\mathbb{X}\\)) and response vector \\(\\mathbb{Y}\\). We use linear regression to solve for this and obtain optimal weights as \\(\\hat{\\theta}\\). Label the following terms on the geometric interpretation of ordinary least squares:\n\n\\(\\mathbb{X}\\) (i.e., span(\\(\\mathbb{X}\\)))\nThe response vector \\(\\mathbb{Y}\\)\nThe residual vector $ - \\(\\mathbb{X}\\hat{\\theta}\\)\nThe prediction vector \\(\\mathbb{X}\\hat{\\theta}\\) (using optimal parameters)\nA prediction vector \\(\\mathbb{X}\\alpha\\) (using an arbitrary vector \\(\\alpha\\))\n\n\n\n\nGeometric Empty\n\n\n\n\nAnswer\n\n\n\n\nGeometric Answer\n\n\nA couple things to note:\n\nThe rectangle represents span(\\(\\mathbb{X}\\)), or everywhere that you’re able to get to with a linear combination of the columns of \\(\\mathbb{X}\\). Therefore, \\(\\mathbb{X}\\hat{\\alpha}\\) and \\(\\mathbb{X}\\hat{\\theta}\\) must be the arrows in the rectangle.\nWe know that the residual vector \\(\\mathbb{Y} - \\mathbb{X}\\hat{\\theta}\\) is orthogonal to span(\\(\\mathbb{X}\\))\n\\(\\mathbb{X}\\hat{\\theta}\\) is the best/closest to \\(\\mathbb{Y}\\) in span(\\(\\mathbb{X}\\)), so we know that it is the one that looks like it is the shadow of the bolded arrow representing \\(\\mathbb{Y}\\)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Models, OLS</span>"
    ]
  },
  {
    "objectID": "disc06/disc06.html#multiple-choice-questions",
    "href": "disc06/disc06.html#multiple-choice-questions",
    "title": "7  Models, OLS",
    "section": "7.3 Multiple Choice Questions",
    "text": "7.3 Multiple Choice Questions\nUsing the geometry of least squares, let’s answer a few questions about Ordinary Least Squares (OLS)!\n\n7.3.1 (a)\nWhich of the following are true about the optimal solution \\(\\hat{\\theta}\\) to OLS? Recall that the least squares estimate \\(\\hat{\\theta}\\) solves the normal equation \\((\\mathbb{X}^T\\mathbb{X})\\theta = \\mathbb{X}^T\\mathbb{Y}\\)\n\\[\\begin{align*}\n\\hat{\\theta} = (\\mathbb{X}^T\\mathbb{X})^{-1}\\mathbb{X}^T\\mathbb{Y}\n\\end{align*}\\]\n\\(\\Box\\) A. Using the normal equation, we can derive an optimal solution for simple linear regression with an \\(L_2\\) loss.\n\\(\\Box\\) B. Using the normal equation, we can derive an optimal solution for simple linear regression with an \\(L_1\\) loss.\n\\(\\Box\\) C. Using the normal equation, we can derive an optimal solution for a constant model with an \\(L_2\\) loss.\n\\(\\Box\\) D. Using the normal equation, we can derive an optimal solution for a constant model with an \\(L_1\\) loss.\n\\(\\Box\\) E. Using the normal equation, we can derive an optimal solution for the model \\(\\hat{y} = \\theta_1x + \\theta_2sin(x^2)\\)\n\\(\\Box\\) F. Using the normal equation, we can derive an optimal solution for the model \\(\\hat{y} = \\theta_1\\theta_2 + \\theta_2x^2\\)\n\n\nAnswer\n\nWe can derive solutions to both simple linear regression and constant model with an \\(L_2\\) loss since they can be represented in the form \\(y = x^T\\theta\\) in some way. Specifically, one of the two entries of \\(x\\) would be \\(1\\) for SLR (and the other would be the explanatory variable). The only entry for the constant model would be \\(1\\).\nWe cannot derive solutions for anything with the \\(L_1\\) loss since the normal equation optimizes for MSE.\nSince option E is linear with respect to \\(\\theta\\), we can use the normal equation. A good rule of thumb to determine if an equation is linear in \\(\\theta\\) is checking if an expression can be separated into a matrix product of two terms:\n\\[\\begin{align*}\n\\hat{y} = \\begin{bmatrix}x & sin(x^2)\\end{bmatrix}\\begin{bmatrix}\\theta_1 \\\\ \\theta_2\\end{bmatrix}\n\\end{align*}\\]\nConversely, option F is not linear in \\(\\theta\\) so we cannot use the normal equations.\n\n\n\n7.3.2 (b)\nWhich of the following conditions are required for the least squares estimate in the previous subpart?\n\\(\\Box\\) A. \\(\\mathbb{X}\\) must be full column rank\n\\(\\Box\\) B. \\(\\mathbb{Y}\\) must be full column rank\n\\(\\Box\\) C. \\(\\mathbb{X}\\) must be invertible\n\\(\\Box\\) D. \\(\\mathbb{X}^T\\) must be invertible\n\n\nAnswer\n\nA. \\(\\mathbb{X}\\) must be full column rank\n\\(\\mathbb{X}\\) must be full column rank in order for the normal equation to have a unique solution. Otherwise, \\(\\mathbb{X}^T\\mathbb{X}\\) will not be invertible, and there will be infinite least squares estimates. Noth that invertibility is required of \\(\\mathbb{X}^T\\mathbb{X}\\): neither \\(\\mathbb{X}\\) nor \\(\\mathbb{X}^T\\) need to be invertible. \\(\\mathbb{X}\\) and \\(\\mathbb{X}^T\\) don’t even need to be square matrices! \\(\\mathbb{Y}\\) is a vector so the matrix notion of “full column rank” does not really apply here (a vector will always span one dimension unless it is a vector of zeros).\n\n\n\n7.3.3 (c)\nWhat is always true about the residuals in the least squares regression? Select all that apply.\n\\(\\Box\\) A. They are orthogonal to the column space of the design matrix.\n\\(\\Box\\) B. They represent the errors of the predictions.\n\\(\\Box\\) C. Their sum is equal to the mean squared error.\n\\(\\Box\\) D. Their sum is equal to zero.\n\\(\\Box\\) E. None of the above.\n\n\nAnswer\n\nA, B\n(C) is wrong because the mean squared error is the mean of the sum of the squares of the residuals.\n(D) A counter-example is:\n\\[\\begin{align*}\n\\mathbb{X} = \\begin{bmatrix}2 & 3 \\\\ 1 & 5 \\\\ 2 & 4 \\end{bmatrix}, \\mathbb{Y} = \\begin{bmatrix} 1 \\\\ 3 \\\\ 2 \\end{bmatrix}\n\\end{align*}\\]\nAfter solving the least squares problem, the sum of the residuals is \\(-0.0247\\), which is not equal to zero. However, note that this statement is, in general, true if the design matrix \\(\\mathbb{X}\\) contains a column for the intercept!\n\n\n\n7.3.4 (d)\nWhich of the following are true about the predictions made by OLS? Select all that apply.\n\\(\\Box\\) A. They are projections of the observations onto the column space of the design matrix.\n\\(\\Box\\) B. They are linear combinations of the features.\n\\(\\Box\\) C. They are orthogonal to the residuals.\n\\(\\Box\\) D. They are orthogonal to the column space of the features.\n\\(\\Box\\) E. None of the above.\n\n\nAnswer\n\n(A), (B), (C)\n(A) is correct because the predictions made by OLS is the vector in the column space of \\(\\mathbb{X}\\) closest to the response vector \\(\\mathbb{Y}\\), which is the orthogonal projection of \\(\\mathbb{Y}\\) onto \\(\\mathbb{X}\\).\n(B) is correct because the predictions \\(\\mathbb{X}\\hat{\\theta}\\) is a linear combination of the columns of \\(\\mathbb{X}\\) (the features).\n(C) is correct because we know that the predictions \\(\\mathbb{X}\\hat{\\theta}\\) is the vector in span(\\(\\mathbb{X}\\)) closest to \\(\\mathbb{Y}\\). Therefore, \\(\\mathbb{Y} - \\mathbb{X}\\hat{\\theta}\\) (the residuals) must be orthogonal to the predictions.\n(D) is not correct because the predictions are in the column space of the features.\n\n\n\n7.3.5 (e)\nWhich of the following is true of the mystery quantity \\(\\vec{v} = (I - \\mathbb{X}(\\mathbb{X}^T\\mathbb{X})^{-1}\\mathbb{X}^T)\\mathbb{Y}\\)?\n\\(\\Box\\) A. The vector \\(\\vec{v}\\) represents the residuals for any linear model.\n\\(\\Box\\) B. If the matrix \\(\\mathbb{X}\\) contains the \\(\\vec{1}\\) vector, then the sum of the elements in vector \\(\\vec{v}\\) is 0 (i.e., \\(\\sum_iv_i = 0\\))\n\\(\\Box\\) C. All the column vectors \\(x_i\\) of \\(\\mathbb{X}\\) are orthogonal to \\(\\vec{v}\\).\n\\(\\Box\\) D. If \\(\\mathbb{X}\\) is of shape \\(n\\) by \\(p\\), there are \\(p\\) elements in vector \\(\\vec{v}\\).\n\\(\\Box\\) E. For any \\(\\vec{\\alpha}\\), \\(\\mathbb{X}\\vec{\\alpha}\\) is orthogonal to \\(\\vec{v}\\).\n\n\nAnswer\n\n(B), (C), (E)\nThe trick is to recognize the following:\n\\[\\begin{align*}\n\\vec{v} &= (I - \\mathbb{X}(\\mathbb{X}^T\\mathbb{X})^{-1}\\mathbb{X}^T)\\mathbb{Y}\\\\\n&= \\mathbb{Y} - \\mathbb{X}(\\mathbb{X}^T\\mathbb{X})^{-1}\\mathbb{X}^T\\mathbb{Y}\\\\\n&= \\mathbb{Y} - \\mathbb{X}\\hat{\\theta}\n\\end{align*}\\]\nwhich is the residual vector for an OLS model.\n(A) is incorrect because any linear model does not create the residual vector \\(v\\); only the optimal linear model with weights \\(\\hat{\\theta}\\) does.\n(D) is incorrect because the vector \\(v\\) is of size \\(n\\) since there are \\(n\\) data points.\nThe rest are correct by properties of orthogonality as given by the geometry of least squares.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Models, OLS</span>"
    ]
  },
  {
    "objectID": "disc07/disc07.html",
    "href": "disc07/disc07.html",
    "title": "8  Gradient Descent, Feature Engineering, Housing",
    "section": "",
    "text": "8.0.1 Link to Slides",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Gradient Descent, Feature Engineering, Housing</span>"
    ]
  },
  {
    "objectID": "disc07/disc07.html#dive-into-gradient-descent",
    "href": "disc07/disc07.html#dive-into-gradient-descent",
    "title": "8  Gradient Descent, Feature Engineering, Housing",
    "section": "8.1 Dive into Gradient Descent",
    "text": "8.1 Dive into Gradient Descent\nWe want to minimize the loss function \\(L(\\theta) = (\\theta_1-1)^2 + |\\theta_2-3|\\). While you may notice that this function is not differentiable everywhere, we can still use gradient descent wherever the function is differentiable!\nRecall that for a function \\(f(x) = k|x|\\), \\(\\frac{df}{dx} = k\\) for all \\(x &gt; 0\\) and \\(\\frac{df}{dx} = -k\\) for all \\(x &lt; 0\\).\n\n8.1.1 (a)\nWhat are the optimal values \\(\\hat{\\theta}_1\\) and \\(\\hat{\\theta}_2\\) to minimize \\(L(\\theta)\\)? What is the gradient at those values \\(\\nabla L = \\begin{bmatrix} \\frac{\\partial L}{\\partial \\theta_1} & \\frac{\\partial L}{\\partial \\theta_2} \\end{bmatrix}^T \\Bigr|_{\\substack{\\theta_1 = \\hat{\\theta}_1, \\theta_2 = \\hat{\\theta}_2}}\\)?\n\n\nAnswer\n\nWe can start off by finding the partial derivatives:\n\n\\(\\frac{\\partial L}{\\partial \\theta_1} = 2(\\theta_1 - 1)\\)\n\nThe above derivative is \\(0\\) when \\(\\theta_1 = 1\\)\n\n\\(\\frac{\\partial L}{\\partial \\theta_2} = \\begin{cases}\n              1 & \\text{if } \\theta_2 &gt; 3 \\\\\n              -1 & \\text{if } \\theta_2 &lt; 3 \\\\\n              \\text{undefined} & \\text{if } \\theta_2 = 3\n          \\end{cases}\\)\n\nAlthough the partial derivative is undefined at \\(\\theta_2 = 3\\), we can see that \\(\\theta_2 = 3\\) is a minimizing value of \\(L(\\theta)\\) (\\(|3 - 3| = 0\\), which is the smallest possible result).\n\n\nTherefore, the gradient at the optimal values of \\(\\theta_1, \\theta_2\\) is:\n\\[\\begin{align*}\n\\nabla L &= \\begin{bmatrix} \\frac{\\partial L}{\\partial \\theta_1} & \\frac{\\partial L}{\\partial \\theta_2} \\end{bmatrix}^T \\Bigr|_{\\substack{\\theta_1\n= \\hat{\\theta}_1, \\theta_2 = \\hat{\\theta}_2}} \\\\\n&= [0, \\text{undefined}]^T\n\\end{align*}\\]\n\n\n\n\n\n\nNote\n\n\n\nSetting the derivative to 0 only finds the minimum if the function is convex. Both \\((\\theta_1-1)^2\\) and \\(|\\theta_2-3|\\) are convex. We proved in homework 5 that the sum of convex functions is also convex. Therefore, \\(L(\\theta)\\) is convex and we can set its derivative to \\(0\\) to find the minimum.\nAnother way to test convexity is by performing the second derivative test, i.e., show \\(L''(\\theta) \\geq 0\\)\n\n\n\n\n\n8.1.2 (b)\nSuppose we initialize our gradient descent algorithm randomly at \\(\\theta_1 = 2\\) and \\(\\theta_2 = 5\\). Calculate the gradient \\(\\nabla L = \\begin{bmatrix} \\frac{\\partial L}{\\partial \\theta_1} & \\frac{\\partial L}{\\partial \\theta_2} \\end{bmatrix}^T \\Bigr|_{\\substack{\\theta_1 = 2, \\theta_2 = 5}}\\) at the specified \\(\\theta_1\\) and \\(\\theta_2\\) values.\n\n\nAnswer\n\nWe can simply substitute the values into the partial derivates we obtained in the previous question:\n\\[\\begin{align*}\n\\begin{bmatrix} \\frac{\\partial L}{\\partial \\theta_1} \\\\\\\\ \\frac{\\partial L}{\\partial \\theta_2} \\end{bmatrix} &= \\begin{bmatrix} 2(\\theta_1-1) \\\\ 1 \\end{bmatrix} \\\\\n&= \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}\n\\end{align*}\\]\n\n\n\n8.1.3 (c)\nApply the first gradient update with a learning rate \\(\\alpha = 0.5\\). In other words, calculate \\(\\theta_1^{(1)}\\) and \\(\\theta_2^{(1)}\\) using the initializations \\(\\theta_1^{(0)} = 2\\) and \\(\\theta_2^{(0)} = 5\\).\n\n\nAnswer\n\n\\[\\begin{align*}\n\\theta^{(1)} &= \\theta^{(0)} - \\alpha\\nabla L\\\\\n&= \\begin{bmatrix} 2 \\\\ 5 \\end{bmatrix} - 0.5\\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}\\\\\n&= \\begin{bmatrix} 2 - 0.5(2) \\\\ 5 - 0.5(1) \\end{bmatrix}\\\\\n&= \\begin{bmatrix} 1 \\\\ 4.5 \\end{bmatrix}\n\\end{align*}\\]\n\n\n\n8.1.4 (d)\nHow many gradient descent steps does it take for \\(\\theta_1\\) and \\(\\theta_2\\) to converge to their optimal values obtained in part (a) assuming we keep a constant learning rate of \\(\\alpha = 0.5\\)? In other words, what is the value of t when \\(\\theta^{(t)}=\\begin{bmatrix} \\hat{\\theta}_1& \\hat{\\theta}_2 \\end{bmatrix}^T\\).\nHint: After part (c), what is the derivative \\(\\frac{\\partial L}{\\partial \\theta_1}\\) evaluated at \\(\\theta_1^{(1)}\\)?\n\n\nAnswer\n\nNote that the derivative with respect to \\(\\theta_1\\) is 0 at \\(\\theta_1^{(1)} = 1\\) since it is the optimal solution! Then, we essentially only update \\(\\theta_2\\) where the partial derivative is always 1 (until we reach the optimal solution - then our derivative is undefined)! Every time, the partial derivative of \\(\\theta_2\\) is 1 - so the update is simply:\n\\[\\begin{align*}\n\\theta_2^{(i+1)} = \\theta_2^{(i)} - 0.5\n\\end{align*}\\]\nHence, to update this from 5 to 3, we must take 4 gradient steps (i.e. from 5 to 4.5, 4.5 to 4, 4 to 3.5, 3.5 to 3).\nWriting this all out:\n\\[ \\theta^{(2)} = \\theta^{(1)} - \\alpha \\nabla L = \\begin{bmatrix} 1 - 0.5(0) \\\\ 4.5 - 0.5(1) \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix} \\]\n\\[ \\theta^{(3)} = \\theta^{(2)} - \\alpha \\nabla L = \\begin{bmatrix} 1 - 0.5(0) \\\\ 4 - 0.5(1) \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 3.5 \\end{bmatrix} \\]\n\\[ \\theta^{(4)} = \\theta^{(3)} - \\alpha \\nabla L = \\begin{bmatrix} 1 - 0.5(0) \\\\ 3.5 - 0.5(1) \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix} \\]\nNotice that every time, we reduce \\(\\theta_2\\) by 0.5 as expected, so the number of gradient descent steps is 4.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Gradient Descent, Feature Engineering, Housing</span>"
    ]
  },
  {
    "objectID": "disc07/disc07.html#one-hot-encoding",
    "href": "disc07/disc07.html#one-hot-encoding",
    "title": "8  Gradient Descent, Feature Engineering, Housing",
    "section": "8.2 One-hot Encoding",
    "text": "8.2 One-hot Encoding\nIn order to include a qualitative variable in a model, we convert it into a collection of Boolean vectors that only contain the values 0 and 1. For example, suppose we have a qualitative variable with 3 possible values, call them \\(A\\), \\(B\\), and \\(C\\), respectively. For concreteness, we use a specific example with 10 observations: \\[\n[A, A, A, A, B, B, B, C, C, C]\n\\] We can represent this qualitative variable with 3 Boolean vectors that take on values \\(1\\) or \\(0\\) depending on the value of this qualitative variable. Specifically, the values of these 3 Boolean vectors for this dataset are \\(x_A\\), \\(x_B\\), and \\(x_C\\), arranged from left to right in the following design matrix, where we use the following indicator variable:\n\\[\\begin{align*}\n                x_{i,k} =\\begin{cases}\n                        1 &\\text{if $i$-th observation has value $k$}\\\\\n                        0 &\\text{otherwise}.\n                \\end{cases}\n\\end{align*}\\]\nFurthermore, let \\(\\vec{y}\\) represent any vector of outcome variables, and \\(y_i\\) is the value of said outcome for the \\(i\\)-th subject. This representation is also called one-hot encoding. It should be noted here that \\(\\vec{x_A}\\), \\(\\vec{x_B}\\), \\(\\vec{x_C}\\), and \\(\\vec{y}\\) are all vectors.\n\\[\n\\Bbb{X}\n=\n\\begin{bmatrix}\n\\vert & \\vert & \\vert \\\\\n\\vec{x_A} & \\vec{x_B} & \\vec{x_C} \\\\\n\\vert & \\vert & \\vert \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 0 & 0\\\\\n1 & 0 & 0\\\\\n1 & 0 & 0\\\\\n1 & 0 & 0\\\\\n0 & 1 & 0\\\\\n0 & 1 & 0\\\\\n0 & 1 & 0\\\\\n0 & 0 & 1\\\\\n0 & 0 & 1\\\\\n0 & 0 & 1\\\\\n\\end{bmatrix}\n\\]\nWe will show that the fitted coefficients for \\(\\vec{x_A}\\), \\(\\vec{x_B}\\), and \\(\\vec{x_C}\\) are \\(\\bar{y}_A\\), \\(\\bar{y}_B\\), and \\(\\bar{y}_C\\), the average of the \\(y_i\\) values for each of the groups, respectively.\n\n8.2.1 (a)\nShow that if you augment your \\(\\mathbb{X}\\) matrix with an additional \\(\\vec{1}\\) bias vector as shown below, \\(\\mathbb{X}^T\\mathbb{X}\\) is not full rank. Conclude that the new \\(\\mathbb{X}^T\\mathbb{X}\\) is not invertible, and we cannot use the least squares estimate in this situation.\n\\[\\begin{align*}\n\\mathbb{X}\n=\n\\begin{bmatrix}\n\\vert & \\vert & \\vert  & \\vert \\\\\n\\vec{1} & \\vec{x_A} & \\vec{x_B} & \\vec{x_C} \\\\\n\\vert & \\vert & \\vert  & \\vert \\\\\n\\end{bmatrix}\n\\end{align*}\\]\n\n\nAnswer\n\nSolution 1:\nBy the definition of one-hot encoding, the one-hot-encoded columns of \\(\\mathbb{X}\\) sum up to $ $:\n\\[ \\vec{x_A} + \\vec{x_B} + \\vec{x_C} = \\vec{1}\\]\nSince the leftmost vector of \\(\\mathbb{X}\\) is a linear combination of the other vectors, \\(\\mathbb{X}\\) is not full column rank. Since \\(\\mathbb{X}\\) has the same rank as \\(\\mathbb{X}^T\\mathbb{X}\\) (proof is out of scope), \\(\\mathbb{X}^T\\mathbb{X}\\) is not invertible.\nSolution 2: We can show that \\(\\mathbb{X}^T\\mathbb{X}\\) is equal to the following.\n\\[\n\\mathbb{X}^T\\mathbb{X} =\n\\begin{bmatrix}\nn & n_A & n_B & n_C \\\\\nn_A & n_A & 0 & 0 \\\\\nn_B & 0 & n_B & 0 \\\\\nn_C & 0 & 0 & n_C\n\\end{bmatrix}\n\\]\nIt can be observed that since \\(n_A + n_B + n_C = n\\), the sum of the final 3 columns subtracted from the first column yields the zero vector \\(\\vec{0}\\). By the definition of linear dependence, we can conclude that this matrix is not full rank, and hence, we cannot invert it. As a result, we cannot compute our least squares estimate since it requires \\(\\mathbb{X}^T\\mathbb{X}\\).\n\n\n\n8.2.2 (b)\nShow that the columns of \\(\\mathbb{X}\\) (without the additional \\(\\vec{1}\\) bias vector) are orthogonal, (i.e., the dot product between any pair of column vectors is 0).\n\n\nAnswer\n\nThe argument is the same for any pair of \\(\\Bbb{X}\\)’s columns so we show the orthogonality for one pair, \\(\\vec{x_A} \\cdot \\vec{x_B}\\).\n\\[\\begin{align*}\n\\vec{x_A} \\cdot \\vec{x_B} &= \\sum_{i=1}^{10} x_{A,i} x_{B,i} \\\\\n&= \\sum_{i=1}^4 (1 \\times 0)  + \\sum_{i=5}^7 (0 \\times 1) +\n\\sum_{i=8}^{10} (0 \\times 0) \\\\\n&= 0\n\\end{align*}\\]\n\n\n\n8.2.3 (c)\nShow that \\[\n\\mathbb{X}^T\\Bbb{X} =\n\\begin{bmatrix}\nn_A & 0 & 0 \\\\\n0 & n_B & 0 \\\\\n0 & 0& n_C \\\\\n\\end{bmatrix}\n\\] Here, \\(n_A\\), \\(n_B\\), and \\(n_C\\) are the number of observations in each of the three groups defined by the levels of the qualitative variable.\n\n\nAnswer\n\nHere, we note that \\[\n\\mathbb{X}^T =\n\\begin{bmatrix}\n1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 &0 \\\\\n0 & 0 & 0 &0 & 1 & 1 & 1 & 0 & 0 & 0\\\\\n0 & 0 & 0 &0 & 0 & 0 & 0 & 1 & 1 & 1\\\\\n\\end{bmatrix}\n\\] We also note that \\[\n\\mathbb{X}^T\\mathbb{X} =\n\\begin{bmatrix}\n\\vec{x_A}^T\\vec{x_A} & \\vec{x_A}^T\\vec{x_B} & \\vec{x_A}^T\\vec{x_C} \\\\\n\\vec{x_B}^T\\vec{x_A} & \\vec{x_B}^T\\vec{x_B} & \\vec{x_B}^T\\vec{x_C} \\\\\n\\vec{x_C}^T\\vec{x_A} & \\vec{x_C}^T\\vec{x_B} & \\vec{x_C}^T\\vec{x_C} \\\\\n\\end{bmatrix}\n\\] Since we earlier established the orthogonality of the vectors in \\(\\mathbb{X}\\), we find \\(\\mathbb{X}^T\\mathbb{X}\\) to be the diagonal matrix: \\[\n\\mathbb{X}^T\\mathbb{X} =\n\\begin{bmatrix}\n4 & 0 & 0 \\\\\n0 & 3 & 0 \\\\\n0 & 0& 3 \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\nn_A & 0 & 0 \\\\\n0 & n_B & 0 \\\\\n0 & 0& n_C \\\\\n\\end{bmatrix}\n\\]\n\n\n\n8.2.4 (d)\nShow that \\[\n\\mathbb{X}^T\\mathbb{Y} =\n\\begin{bmatrix}\n\\sum_{i \\in A} y_i\\\\\n\\sum_{i \\in B} y_i\\\\\n\\sum_{i \\in C} y_i\\\\\n\\end{bmatrix}\n\\] where \\(i\\) is an element in group \\(A, B,\\) or \\(C\\).\n\n\nAnswer\n\nNote in the previous solution we found \\(\\Bbb{X}^T\\). The solution follows from recognizing that for a row in \\(\\Bbb{X}^T\\), e.g., the first row, we have \\[\n\\sum_{i=1}^{10} x_{A,i} \\times y_i = \\sum_{i=1}^4 y_i = \\sum_{i \\in \\textrm{ group A}} y_i\n\\]\n\n\n\n8.2.5 (e)\nUse the results from the previous questions to solve the normal equations for \\(\\hat{\\theta}\\), i.e., \\[\\begin{align*}\n\\hat{\\theta} &= [\\Bbb{X}^T\\Bbb{X}]^{-1} \\Bbb{X}^T\\mathbb{Y} \\\\\n&=\n\\begin{bmatrix}\n\\bar{y}_A\\\\\n\\bar{y}_B\\\\\n\\bar{y}_C\\\\\n\\end{bmatrix}\n\\end{align*}\\]\n\n\nAnswer\n\nUsing our results from part (c), we see that: \\[\n[\\Bbb{X}^T\\Bbb{X}]^{-1} =\n\\begin{bmatrix}\n\\frac{1}{n_A} & 0 & 0 \\\\\n0 & \\frac{1}{n_B} & 0 \\\\\n0 & 0 & \\frac{1}{n_C}\\\\\n\\end{bmatrix}\n\\]\nWhen we pre-multiply \\(\\Bbb{X}^T \\mathbb{Y}\\) by this matrix, we get\n\\[\n\\begin{bmatrix}\n\\frac{1}{n_A} & 0 & 0 \\\\\n0 & \\frac{1}{n_B} & 0 \\\\\n0 & 0 & \\frac{1}{n_C}\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\sum_{i \\in A} y_i\\\\\n\\sum_{i \\in B} y_i\\\\\n\\sum_{i \\in C} y_i\\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\bar{y}_A\\\\\n\\bar{y}_B\\\\\n\\bar{y}_C\\\\\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Gradient Descent, Feature Engineering, Housing</span>"
    ]
  },
  {
    "objectID": "disc07/disc07.html#human-contexts-and-ethics-case-study",
    "href": "disc07/disc07.html#human-contexts-and-ethics-case-study",
    "title": "8  Gradient Descent, Feature Engineering, Housing",
    "section": "8.3 Human Contexts and Ethics: Case Study",
    "text": "8.3 Human Contexts and Ethics: Case Study\nWhich of the following scenarios strike you as unfair and why? You can choose more than one. There is no single right answer, but you must explain your reasoning.\nA. A homeowner those home is assessed at a higher price than it would sell for.\nB. A homeowner whose home is assessed at a lower price than it would sell for.\nC. An assessment process that systematically overvalues inexpensive properties and under-values expensive properties.\nD. An assessment process that systematically undervalues inexpensive properties and over- values expensive properties.\n\n\nAnswer\n\nFindings can be used for Project A2, so explicit solutions will not be released for this question. However, here are some discussion points:\n\nWhat is the definition of “fairness” in this context?\nWhich individual or group of people are considered when we evaluate fairness?\nAre we talking about whether each situation above is fair to an individual home- owner, a subgroup of society, or the whole society?\nAt the level of the individual homeowner, any error can feel unfair: why am I be over-/under-taxed?\nAt the level of the society, is it fair to have a general over or underestimation that varies with the value of the home (a regressive or progressive tax scheme)?\n\nOther important things to note:\n\nSome may feel that both regressive and progressive are unfair; that any systematic errors should not depend at all on the value of the property\nIt is possible to have a tax scheme that is systematically fair tax (i.e., not regressive or progressive) but is still quite inaccurate and still generates errors for individual homeowners. However, a tax scheme that generates no errors for homeowners will also have no systematic unfairness.\nUnfairness of the second type (scenarios C and D) is a form of statistical bias that varies as a function of y (the response variable).\n\n\n\n\n\n\n\nSome comments/opinions from Discussion (not verbose)\n\n\n\n\nAll are unfair! Mismatch between predicted vs actual creates unfairness.\nA, B are not unfair. It’s inevitable that predicted \\(\\neq\\) actual. It becomes unfair when there is systematic overvaluing or undervaluing, which can negatively affect many people.\nD is unfair because owners of undervalued inexpensive properties suffer from a smaller sale price, while owners of overvalued expensive properties can incur much more property tax.\n\n\n\n\nSuppose you created a model to predict property values (as you are doing presently!) and wanted to understand whether your model’s predictions align more with Scenario C or D from the last question. You decide to break down your data into different intervals depending on the true Log Sale Price and compute the Root Mean Squared Error (RMSE) and percentage of property values overestimated for each interval. Using this information, you create the plots below:\n\n\n\nLog Sale Price plots\n\n\nWhich plot would be more useful in determining whether the model’s predicted property val- ues align more with Scenario C or D? Provide a brief justification for your answer.\n\n\nAnswer\n\nAgain, findings here can be used for Project A2, so explicit solutions will not be released. However, some questions to consider:\n\nHow does the sign of the residual relate to whether a property is overvalued or undervalued?\nDoes a low RMSE necessarily mean a model’s predictions are more accurate?\nDoes a low RMSE necessarily mean a model’s predictions are more “fair”?\nWhat is the difference between your answers to both questions?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Gradient Descent, Feature Engineering, Housing</span>"
    ]
  },
  {
    "objectID": "disc08/disc08.html",
    "href": "disc08/disc08.html",
    "title": "9  Cross-Validation and Regularization",
    "section": "",
    "text": "9.0.1 Link to Slides",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cross-Validation and Regularization</span>"
    ]
  },
  {
    "objectID": "disc08/disc08.html#cross-validation",
    "href": "disc08/disc08.html#cross-validation",
    "title": "9  Cross-Validation and Regularization",
    "section": "9.1 Cross Validation",
    "text": "9.1 Cross Validation\n\n9.1.1 (a)\nAfter running \\(5\\)-fold cross-validation, we get the following mean squared errors for each fold and value of \\(\\lambda\\) when using Ridge regularization:\n\n\n\n\n\n\n\n\n\n\n\nFold Num.\n\\(\\lambda = 0.1\\)\n\\(\\lambda = 0.2\\)\n\\(\\lambda = 0.3\\)\n\\(\\lambda = 0.4\\)\nRow Avg\n\n\n\n\n1\n80.2\n70.2\n91.2\n91.8\n83.4\n\n\n2\n76.8\n66.8\n88.8\n98.8\n82.8\n\n\n3\n81.5\n71.5\n86.5\n88.5\n82.0\n\n\n4\n79.4\n68.4\n92.3\n92.4\n83.1\n\n\n5\n77.3\n67.3\n93.4\n94.3\n83.0\n\n\nCol Avg\n79.0\n68.8\n90.4\n93.2\n\n\n\n\nSuppose we wish to use the results of this 5-fold cross-validation to choose our hyperparameter \\(\\lambda\\), among the following four choices in the table. Using the information in the table, which \\(\\lambda\\) would you choose? Why?\n\n\nAnswer\n\nWe should use \\(\\lambda = 0.2\\) because this value has the least average MSE across all folds.\n\n\n\n9.1.2 (b)\nYou build a model with two hyperparameters, the coefficient for the regularization term (\\(\\lambda\\)) and our learning rate (\\(\\alpha\\)). You have 4 good candidate values for \\(\\lambda\\) and 3 possible values for \\(\\alpha\\), and you are wondering which \\(\\lambda,\n\\alpha\\) pair will be the best choice. If you were to perform 5-fold cross-validation, how many validation errors would you need to calculate?\n\n\nAnswer\n\nThere are \\(4 \\times 3 = 12\\) pairs of \\(\\lambda, \\alpha\\), and each pair will have \\(5\\) validation errors, one for each fold. So, there would be 60 validation errors in total.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cross-Validation and Regularization</span>"
    ]
  },
  {
    "objectID": "disc08/disc08.html#ridge-and-lasso-regression",
    "href": "disc08/disc08.html#ridge-and-lasso-regression",
    "title": "9  Cross-Validation and Regularization",
    "section": "9.2 Ridge and LASSO Regression",
    "text": "9.2 Ridge and LASSO Regression\nThe goal of linear regression is to find the \\(\\theta\\) value that minimizes the average squared loss. In other words, we want to find \\(\\hat{\\theta}\\) that satisfies the equation below:\n\\[\\hat{\\theta}\n= \\underset{\\theta}{\\operatorname{argmin}} L(\\theta)\n= \\underset{\\theta}{\\operatorname{argmin}} \\dfrac{1}{n}||\\mathbb{Y} - \\mathbb{X}{\\theta}||_2^2\\]\nHere, \\(\\Bbb{X}\\) is a \\(n \\times (p + 1)\\) matrix, \\(\\theta\\) is a \\((p + 1) \\times 1\\) vector and \\(\\mathbb{Y}\\) is a \\(n \\times 1\\) vector. Recall that the extra \\(1\\) in \\((p+1)\\) comes from the intercept term. As we saw in lecture, the optimal \\(\\hat{\\theta}\\) is given by the closed-form expression \\(\\hat{\\theta} = (\\Bbb{X}^T\\Bbb{X})^{-1}\\Bbb{X}^T \\mathbb{Y}\\).\nTo prevent overfitting, we saw that we can instead minimize the sum of the average squared loss plus a regularization term \\(\\lambda g(\\theta)\\). The optimization problem for such a loss function then becomes:\n\\[\\begin{align*}\n    \\hat{\\theta} = \\underset{\\theta}{\\operatorname{argmin}} L(\\theta) = \\underset{\\theta}{\\operatorname{argmin}} \\left[\\frac{1}{n} \\|\\mathbb{Y} - \\mathbb{X}\\theta\\|_{2}^{2} + \\lambda g(\\theta) \\right]\n\\end{align*}\\]\n\nIf we use the function \\(g(\\theta) = \\sum_{j=1}^p\\theta_j^2 = ||{\\theta}||_2^2\\), we have “Ridge regression”. Recall that \\(g\\) is the \\(\\ell_2\\) norm of \\(\\theta\\), so this is also referred to as “\\(\\ell_2 / L_2\\) regularization”.\nIf we use the function \\(g(\\theta) = \\sum_{j=1}^p |\\theta_j| = ||{\\theta}||_1\\), we have “LASSO regression”. Recall that \\(g\\) is the \\(\\ell_1\\) norm of \\(\\theta\\), so this is also referred to as “\\(\\ell_1 / L_1\\) regularization”.\n\n\n\n\nridge_lasso\n\n\nIn this question, we intentionally choose to regularize also on the intercept term to simplify the mathematical formulation of the Ridge and LASSO regression. In practice, we would not actually want to regularize the intercept term (and you should always assume that there should not be a regularization on the intercept term).\nFor example, if we choose \\(g(\\theta) = ||{\\theta}||_2^2\\), our goal is to find \\(\\hat{\\theta}\\) that satisfies the equation below:\n\\[\\begin{align*}\n\\hat\\theta\n= \\underset{\\theta}{\\operatorname{argmin}} L_2(\\theta)\n&= \\underset{\\theta}{\\operatorname{argmin}} \\left[ \\dfrac{1}{n}||\\mathbb{Y} - \\Bbb{X}{\\theta}||_2^2 + \\lambda ||{\\theta}||_2^2\n\\right] \\\\\n&= \\underset{\\theta}{\\operatorname{argmin}} \\left[ \\dfrac{1}{n}\\sum_{i=1}^n (y_i - \\Bbb{X}_{i,\\cdot}^T \\theta) ^2 + \\lambda \\sum_{j=0}^d\\theta_j^2 \\right]\n\\end{align*}\\]\nRecall that \\(\\lambda\\) is a hyperparameter that determines the impact of the regularization term. Like ordinary least squares, we can also find a closed-form solution to Ridge regression: \\(\\hat{\\theta}=(\\Bbb{X}^T\\Bbb{X} + n \\lambda \\mathbf{I})^{-1} \\Bbb{X}^T \\mathbb{Y}\\). For LASSO regression, there is no such closed-form expression.\n\n9.2.1 (a)\nSuppose we are dealing with the OLS case (i.e., don’t worry about regularization yet). We increase the complexity of the model until test error stops decreasing. If we continue to increase model complexity, what do we expect to happen to the training error of the model trained using OLS? What about the test error?\n\\(\\Box\\) Training error decreases\n\\(\\Box\\) Training error increases\n\\(\\Box\\) Test error decreases\n\\(\\Box\\) Test error increases\n\n\nAnswer\n\nTraining error decreases, Test error increases\nThe training error decreases since the model fits/recognizes more relationships between features and responses found in the training dataset. However, these relationships increasingly become specific to the training set and will not necessarily generalize to the test set, so we expect the test error to increase.\nNote: The above is what we expect to happen, but there may be rare cases where this might not be true.\n\n\n\n9.2.2 (b)\nNow suppose we choose one of the above regularization methods, either \\(L1\\) or \\(L2\\), for some regularization parameter \\(\\lambda &gt; 0\\) then we solve for our optimum. In terms of variance, how does a regularized model compare to ordinary least squares regression (assuming the same features between both models)?\n\n\nAnswer\n\nRegularized regression has a lower variance relative to ordinary least squares regression. This is because regularization tends to make the model “simpler” (pushing the vector of regression coefficients to be in some ball around the origin). So, upon slight changes in input variables, our predictions will vary less under regularization than under no regularization.\n\n\n\n9.2.3 (c)\nSuppose we have a large number of features (10,000+), and we suspect that only a handful of features are useful. Would LASSO or Ridge regression be more helpful in interpreting useful features? Why?\n\n\nAnswer\n\nLASSO would be better as it sets many values to 0, so it would be effectively selecting useful features and “ignoring” less useful ones.\nYou can see this behavior in the GIF above with two parameters!\n\n\n\n9.2.4 (d)\nWhat are the two benefits of using Ridge regression over OLS?\n\n\nAnswer\n\n\nIf \\(\\mathbb{X}^T\\mathbb{X}\\) is not full rank (not invertible), then we end up with infinitely many solutions for least squares. On the other hand, using Ridge regression guarantees invertibility of \\((\\mathbb{X}^T\\mathbb{X} + n \\lambda \\mathbb{I})\\) and ensures that \\(\\hat\\theta = (\\mathbb{X}^T\\mathbb{X} + n \\lambda \\mathbb{I})^{-1}\\mathbb{X}^T\\mathbb{Y}\\) always has a unique solution when \\(\\lambda &gt; 0\\); the proof for these facts is out of scope for Data 100.\nRidge regression also allows for feature selection/reducing overfitting because it down weights features that are less important in predicting the response. However, it still stands that LASSO is normally better for feature selection since LASSO will actually set these unimportant coefficients to \\(0\\) as opposed to just down-weighting them.\n\n\n\n\n9.2.5 (e)\nIn Ridge regression, what happens to \\(\\hat{\\theta}\\) if we set \\(\\lambda = 0\\)? What happens as \\(\\lambda\\) approaches \\(\\infty\\)?\n\n\nAnswer\n\n\n\\(\\lambda = 0\\): \\[\\hat{\\theta}=(\\Bbb{X}^T\\Bbb{X} + n \\lambda \\mathbf{I})^{-1} \\Bbb{X}^T \\mathbb{Y} =(\\Bbb{X}^T\\Bbb{X})^{-1} \\Bbb{X}^T \\mathbb{Y}\\] Which is the normal OLS solution.\n\\(\\lambda \\rightarrow \\infty\\): \\[ (\\Bbb{X}^T\\Bbb{X} + n \\lambda \\mathbf{I})^{-1} \\rightarrow \\vec{0} \\text{  as  } \\lambda \\rightarrow \\infty \\] Therefore, \\(\\hat{\\theta} \\rightarrow \\vec{0}\\) as well\n\nIntuitively: As \\(\\lambda \\rightarrow \\infty\\), the penalty term will dominat ethe least-squares term. After a certain point, it will be more optimal to set \\(\\hat{\\theta} = 0\\) and simply incur the loss of a constant model.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cross-Validation and Regularization</span>"
    ]
  },
  {
    "objectID": "disc08/disc08.html#guessing-at-random",
    "href": "disc08/disc08.html#guessing-at-random",
    "title": "9  Cross-Validation and Regularization",
    "section": "9.3 Guessing at Random",
    "text": "9.3 Guessing at Random\nA multiple choice test has 100 questions, each with five answer choices. Assume for each question that there is only one correct choice. The grading scheme is as follows:\n\n4 points are awarded for each right answer.\nFor each other answer (wrong, missing, etc), one point is taken off; that is, -1 point is awarded.\n\nA student hasn’t studied at all and therefore selects each question’s answer uniformly at random, independently of all the other questions.\nDefine the following random variables:\n\n\\(R\\): The number of answers the student gets right.\n\\(W\\): The number of answers the student does not get right.\n\\(S\\): The student’s score on the test.\n\n\n9.3.1 (a)\nWhat is the distribution of \\(R\\)? Provide the name and parameters of the appropriate distribution. Explain your answer.\n\n\nAnswer\n\n\\(R\\) is counting the number of “successes” (or 1s) out of \\(100\\) total independent Bernoulli trials, where a “success” is defined as answering the question correctly, and each question is a trial. The trials are independent because the student selects a random answer with the same probability distribution, no matter whether the other answers are chosen. The probability of “success” on any single trial is \\(1/5 = 0.2\\), so, \\(R\\) must follow a binomial distribution with \\(n = 100\\) and \\(p = 0.2\\).\n\n\n\n9.3.2 (b)\nFind \\(\\mathbb{E}[R]\\)\n\n\nAnswer\n\nFrom class, the expectation of a \\(\\text{Binomial}(n,p)\\) random variable is always \\(np\\). So, we obtain: \\[\\mathbb{E}[R] = n \\cdot p = 100 \\cdot 0.2 = 20\\]\n\n\n\n9.3.3 (c)\nTrue or False: \\(\\text{SD}(R) = \\text{SD}(W)\\)? Remember that \\(\\text{Var}(X) = \\text{SD}(X)^2\\).\n\n\nAnswer\n\nTrue. Note that \\(R + W = 100\\). Hence, \\[\\begin{align*}\n    \\text{Var}(R) &= \\text{Var}(100 - W) \\\\\n     &= (-1)^2\\text{Var}(W)\\\\\n     &= \\text{Var}(W)\n\\end{align*}\\]\nWe use the non-linearity of variance, \\(\\text{Var}(aX+b) = a^2\\text{Var}(X)\\), to simplify our expression.\n\n\n\n9.3.4 (d)\nFind \\(\\mathbb{E}[S]\\), the student’s expected score on the test.\n\n\nAnswer\n\nThe student’s score on the test is a function of how many they get correct and how many they get incorrect. Using the point scheme given in the question, we can write this score as \\(S = 4R - W\\) since each correct answer is awarded \\(4\\) points, and each wrong answer is penalized by \\(1\\) point. Note that \\(S\\) is also a random variable since it is a function of random variables \\(R\\) and \\(W\\). Note that \\(R + W = 100\\), since there are \\(100\\) questions. Substituting \\(W = 100 - R\\) and using linearity of expectations, we see:\n\\[\\begin{align*}\n    \\mathbb{E}[S] &= \\mathbb{E}[4R - W] \\\\\n    &= \\mathbb{E}[4R - 100 + R] \\\\\n    &= \\mathbb{E}[5R - 100] \\\\\n    &= 5\\mathbb{E}[R] - 100 \\\\\n\\end{align*}\\]\nSubstituting \\(\\mathbb{E}[R] = 20\\) from part (b), we see the students expected score on the exam using this guessing strategy is \\(0\\).\n\n\n\n9.3.5 (e)\nFind \\(\\text{SD}(S)\\)\n\n\nAnswer\n\nWe know from the question above that we can write \\(4R - W\\) as \\(5R - 100\\). Since the variance of a random variable plus a constant is just the variance of the original random variable:\n\\[\\begin{align*}\n    \\text{Var}(S) &= \\text{Var}(5R - 100) \\\\\n    &= 5^{2}\\text{Var}(R) \\\\\n    &= 25\\text{Var}(R)\n\\end{align*}\\]\nWe know that the variance of a \\(\\text{Binomial}(n,p)\\) variable is \\(np(1-p)\\). Plugging in the values of \\(n, p\\) from part (a), we see \\(\\text{Var}(R) = 16\\), giving us \\(\\text{Var}(S) = 400\\). Hence, \\(SD(S) = \\sqrt{400} = 20\\).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cross-Validation and Regularization</span>"
    ]
  },
  {
    "objectID": "disc09/disc09.html",
    "href": "disc09/disc09.html",
    "title": "10  Bias and Variance",
    "section": "",
    "text": "10.0.1 Link to Slides",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bias and Variance</span>"
    ]
  },
  {
    "objectID": "disc09/disc09.html#bias-variance-tradeoff",
    "href": "disc09/disc09.html#bias-variance-tradeoff",
    "title": "10  Bias and Variance",
    "section": "10.1 Bias-Variance Tradeoff",
    "text": "10.1 Bias-Variance Tradeoff\nYour team would like to train a machine learning model to predict the next YouTube video a user will click on based on the videos the user has watched. We extract up to \\(d\\) attributes (such as length of video, view count, etc.) from each video, and our model will be based on the previous \\(m\\) videos watched by that user. Hence, the number of features for each data point for the model is \\(m \\times d\\). Currently, you’re not sure how many videos to consider.\n\n10.1.1 (a)\nYour colleague, Lillian, generates the following plot, where the value \\(d\\) on the \\(x\\)-axis denotes the number of features used for a particular model. However, she forgot to label the \\(y\\)-axis. Assume that the features are added to the model in decreasing levels of importance: More important features are first, and less important ones are after.\n\n\n\nWhich of the following could the \\(y\\)-axis represent? Select all that apply.\n\\(\\Box\\) A. Training Error\n\\(\\Box\\) B. Validation error\n\\(\\Box\\) C. Bias\n\\(\\Box\\) D. Variance\n\n\nAnswer\n\nTraining Error, Validaiton Error, Bias\n\nTraining Error: Can decrease as we add more features.\nValidation Error: This can be true depending on the underlying complexity of the data.\nBias: Typically decreases with increasing model complexity.\nVariance: Typically increases with increasing model complexity.\n\n\n\n\n10.1.2 (b)\nLillian generates the following plot, where the value \\(d\\) is on the \\(x\\)-axis. However, she forgot to label the \\(y\\)-axis again.\n\n\n\nWhich of the following could the \\(y\\)-axis represent? Select all that apply.\n\\(\\Box\\) A. Training Error\n\\(\\Box\\) B. Validation error\n\\(\\Box\\) C. Bias\n\\(\\Box\\) D. Variance\n\n\nAnswer\n\nValidation Error\n\nTraining Error: Cannot increase when increasing model complexity. If a feature increases training error, its weight will be set to 0.\nValidation Error: Typically decreases up to a certain point, and then starts to increase as model becomes very complex.\nBias: Doesn’t increase with increasing model complexity.\nVariance: Doesn’t decrease with increasing model complexity.\n\n\n\n\n10.1.3 (c)\nExplain what happens to the error on the holdout set as we increase \\(d\\). Why?\n\n\nAnswer\n\nNote: The holdout set refers to a test set, not a validation set\nFor smaller \\(d\\), we are underfitting the training data since the model is less complex. The number of features we have does not allow us to fully understand the pattern and so our holdout error is high. As we increase \\(d\\), we have more features that allow us to pick up on some of the trends in the data and so the holdout error decreases a bit. However, there comes a point after which the features result in overfitting. Our model becomes more complex and does not seem to generalize as well to unseen data resulting in a high holdout error once again.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bias and Variance</span>"
    ]
  },
  {
    "objectID": "disc09/disc09.html#bias-variance-tradeoff-1",
    "href": "disc09/disc09.html#bias-variance-tradeoff-1",
    "title": "10  Bias and Variance",
    "section": "10.2 Bias-Variance Tradeoff",
    "text": "10.2 Bias-Variance Tradeoff\nWe randomly sample \\(n\\) data points, \\((x_i, y_i)\\), and use them to fit a model \\(f_{\\hat\\theta}(x)\\) according to some procedure (e.g. OLS, Ridge, LASSO). Then, we sample a new data point (independent of our existing points) from the same underlying data distribution. Furthermore, assume that we have a function \\(g(x)\\), which represents the ground truth and \\(\\epsilon\\), which represents random noise. \\(\\epsilon\\) is produced by some noise generation process such that \\(\\mathbb{E}\\left\\lbrack\\epsilon\\right\\rbrack = 0\\) and \\(\\text{Var}(\\epsilon)=\\sigma^2\\). Whenever we query \\(Y\\) at a given \\(x\\), we are given \\(Y = g(x) + \\epsilon\\), a corrupted version of the real ground truth output. A new \\(\\epsilon\\) is generated each time, independent of the last. We showed in the lecture that:\n\\[\\underbrace{\\mathbb{E}[(Y - f_{\\hat\\theta}(x))^2}]_{} = \\underbrace{\\sigma^2}_{} + \\underbrace{(g(x) - \\mathbb{E}[f_{\\hat\\theta}(x)]})^2_{} + \\underbrace{\\mathbb{E}[(f_{\\hat\\theta}(x) - \\mathbb{E}[f_{\\hat\\theta}(x)]_{})^2}]\\]\n\n10.2.1 (a)\nLabel each of the terms above using the following word bank. Not all words will be used.\n\nObservation variance\nModel variance\n(Observation Bias) \\(^2\\)\n(Model Bias) \\(^2\\)\nModel Risk\nEmpirical Mean Squared Error\n\n\n\nAnswer\n\n\\[\\underbrace{\\mathbb{E}{[(Y - f_{\\hat\\theta}(x))^2}]}_{\\text{model risk}}\n   = \\underbrace{\\sigma^2}_{\\text{observation variance}}\n   + \\underbrace{(g(x) - \\mathbb{E}[f_{\\hat\\theta}(x)])^2}_{(\\text{model bias})^2}\n   + \\underbrace{\\mathbb{E}[{ (f_{\\hat\\theta}(x) - \\mathbb{E}[{f_{\\hat\\theta}(x)}]_{})^2}]}_{\\text{model variance}}\\]\n\n\n\n10.2.2 (b)\nWhat quantities are random variables in the above equation? In our assumed data-generation process, where is the randomness in each variable coming from (i.e., which part of the assumed underlying model makes each random variable “random”)?\n\n\nAnswer\n\n\n\\(Y\\) - this is the new observation at \\(x\\). Its randomness comes from the random noise \\(\\epsilon\\).\n\\(f_{\\hat\\theta}\\) - this is the model fitted from the data. Its randomness comes from sampling and the random noise \\(\\epsilon\\).\n\\(\\hat{\\theta}\\) - these are the optimal theta parameters. Like the model itself, the randomness stems from the sampling and the random noise \\(\\epsilon\\).\n\n\n\n\n10.2.3 (c)\nCalculate the value of \\(\\mathbb{E}[\\epsilon f_{\\hat\\theta}(x')]\\), where \\(f_{\\hat{\\theta}}(x')\\) is some predicted value of the response variable at some new fixed \\(x'\\) using a model trained on a random sample, and \\(\\epsilon\\) is the observation error for a new data point at this fixed value of \\(x'\\).\n\n\nAnswer\n\nNote that \\(f_{\\hat{\\theta}}(x)\\) is a random variable whose randomness comes from \\(\\hat{\\theta}\\) being random, which is itself random because of randomness in the training data. The data-generating mechanism for our new data point at \\(x\\) is independent of all the other data used to train our model by assumption, so we have that the sources of randomness for \\(\\epsilon\\) and \\(f_{\\hat{\\theta}}(x)\\) are independent. Since \\(\\epsilon\\) and \\(\\hat\\theta\\) are independent, \\[\\mathbb{E}[\\epsilon f_{\\hat\\theta}(x)] = \\mathbb{E}[\\epsilon]\\mathbb{E}[f_{\\hat\\theta}(x)]\\]\nAnd since \\(\\mathbb{E}[\\epsilon] = 0\\) by the definition we have given. Therefore, \\[\\mathbb{E}[\\epsilon f_{\\hat\\theta}(x)] = \\mathbb{E}[\\epsilon]\\mathbb{E}[f_{\\hat\\theta}(x)] = 0\\]\nNote that \\(\\epsilon\\) here refers to the noise associated with our new data point \\(x'\\), which is independently drawn (as stated at the start of the question). This can seem a bit confusing since we’ve previously used \\(\\epsilon\\) to refer to the noise associated with each of the data points, but it is used to simplify the notation. It might help to think of the noise associated with each of our points \\(x_i\\) as being \\(\\epsilon_i\\) (which are i.i.d.), with the noise associated with \\(x'\\) as being \\(\\epsilon'\\). It then becomes clearer why \\(\\epsilon'\\) and \\(f_{\\hat\\theta}(x)\\) are independent.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bias and Variance</span>"
    ]
  },
  {
    "objectID": "disc09/disc09.html#regularization-and-bias-variance-tradeoff",
    "href": "disc09/disc09.html#regularization-and-bias-variance-tradeoff",
    "title": "10  Bias and Variance",
    "section": "10.3 Regularization and Bias-Variance Tradeoff",
    "text": "10.3 Regularization and Bias-Variance Tradeoff\nWe will use a simple constant model \\(f_\\theta(x) = \\theta\\) to show the effects of regularization on bias and variance. For the sake of simplicity, we will assume that there is no noise or observational variance, so the ground truth output is equal to the observed outputs: \\(Y = g(x)\\).\n\n10.3.1 (a)\nRecall that the optimal solution for the constant model with an MSE loss and a dataset \\(\\mathcal{D}\\) with \\(y_1, y_2, ..., y_n\\) is the mean \\(\\bar{y}\\).\nWe use L2 regularization with a regularization penalty of \\(\\lambda &gt; 0\\) to train another constant model. Derive the optimal solution to this new constant model to minimize the objective function below.\n\\[\nR(\\theta) = \\arg \\min_\\theta \\left[ \\left( \\frac{1}{n} \\sum_{i=1}^n (y_i - \\theta)^2 \\right)+ \\lambda \\theta^2 \\right]\n\\]\nNote: As mentioned in the lecture, we do not impose a regularization penalty on the bias term and this problem only serves as a practice.\n\n\nAnswer\n\nWe use calculus to derive this result below:\n\\[\\frac{dR}{d\\theta} = \\left( \\frac{1}{n} \\sum_i \\frac{d}{d\\theta} (y_i - \\theta)^2 \\right) + \\lambda \\frac{d}{d\\theta}\\theta^2 = \\left( -\\frac{2}{n} \\sum_i (y_i - \\theta) \\right) + 2\\lambda \\theta\\]\nSet the derivative to 0, and solve for the optimal \\(\\hat{\\theta}\\).\n\\[ \\left( -\\frac{2}{n}\\sum_i (y_i - \\hat{\\theta}) \\right) + 2\\lambda \\hat{\\theta} = 0 \\implies \\left( -\\frac{1}{n}\\sum_i y_i \\right) + \\hat{\\theta} + \\lambda \\hat{\\theta} = 0 \\]\nThe optimal \\(\\hat{\\theta}\\) is:\n\\[\\frac{1}{n}\\sum_i y_i = \\hat{\\theta} (1+\\lambda)\\]\n\\[\n\\hat{\\theta} = \\frac{1}{n(1 + \\lambda)} \\sum_i y_i = \\frac{1}{1 + \\lambda} \\bar{y}\n\\]\n\n\n\n10.3.2 (b)\nUse the bias-variance decomposition to show that for a constant model with L2 regularization, its optimal expected loss on a sample test point \\((x, Y)\\) in terms of the training data \\(y\\) is equal to the following.\n\\[\\mathbb{E}_\\mathcal{D}[(Y - f_{\\hat\\theta}(x))^2] = (Y - \\frac{1}{1 + \\lambda} \\mathbb{E}_\\mathcal{D}[\\bar{y}])^2 + \\frac{1}{(1 + \\lambda)^2} \\text{Var}_\\mathcal{D}(\\bar{y})\\]\nWhat expected loss do we obtain when \\(\\lambda=0\\), and what does that mean in terms of our model?\nNote: The subscript next to the expectation and variance lets you know what is random inside the expectation (i.e., what is the expectation taken over?). In this case, we calculate the expectation and variance of \\(\\bar{y}\\) across the dataset \\(\\mathcal{D}\\).\n\n\nAnswer\n\nStarting from the bias-variance decomposition presented in the lecture: \\[\\mathbb{E}[(Y - f_{\\hat\\theta}(x))^2] = \\sigma^2 + (g(x) - \\mathbb{E}[f_{\\hat\\theta}(x)])^2 + \\text{Var}(f_{\\hat\\theta}(x))\\]\nRecall that we are not dealing with any noise (\\(\\epsilon \\sim \\mathbb{N}(0, 0)\\)), so we can eliminate the \\(\\sigma^2\\) term. Then, \\(g(x) = Y\\), so we can replace \\(g(x)\\) with \\(Y\\).\n\\[\\mathbb{E}[(Y - f_{\\hat\\theta}(x))^2] = (Y - \\mathbb{E}[f_{\\hat\\theta}(x)])^2 + \\text{Var}(f_{\\hat\\theta}(x))\\]\nWe can then derive \\(f_{\\hat\\theta}(x)\\) using the assumptions of our constant model. We know that \\(f_{\\hat\\theta}(x) = \\frac{1}{1 + \\lambda}\\bar{y}\\). We simplify the following expressions:\n\\[\\mathbb{E}[(Y - f_{\\hat\\theta}(x))^2] = (Y - \\mathbb{E}[\\frac{1}{1 + \\lambda}\\bar{y}])^2 + \\text{Var}(\\frac{1}{1 + \\lambda}\\bar{y})\\]\nWe can simplify:\n\\[\\mathbb{E}[(Y - f_{\\hat\\theta}(x))^2] = (Y - \\frac{1}{1 + \\lambda} \\mathbb{E}[\\bar{y}])^2 + \\frac{1}{(1 + \\lambda)^2} \\text{Var}(\\bar{y})\\]\nSetting \\(\\lambda=0\\) is equivalent to using a constant model without L2 regularization, and we get the following equation for the expected loss:\n\\[\\mathbb{E}[(Y - f_{\\hat\\theta}(x))^2] = (Y - \\mathbb{E}[\\bar{y}])^2 + \\text{Var}(\\bar{y})\\]\n\n\n\n10.3.3 (c)\nRemark on how regularization has affected the model bias and variance as \\(\\lambda\\) increases. Consider what would happen to these quantities as \\(\\lambda \\to \\infty\\).\n\n\nAnswer\n\nThe model bias has increased since the gap between a test point \\(Y\\) and a scaled “best estimate” value is likely larger than between the test point and the actual best estimate. As \\(\\lambda \\to \\infty\\), the model bias squared will tend towards \\(Y^2\\).\nThe model variance has decreased since \\(\\frac{1}{(1 + \\lambda)^2}\\) if \\(\\lambda &gt; 0\\) will always be less than 1. Hence, our variance compared to the vanilla bias-variance decomposition from part (b) is reduced by a factor greater than 1. As \\(\\lambda \\to \\infty\\), the model variance will become 0.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bias and Variance</span>"
    ]
  },
  {
    "objectID": "disc10/disc10.html",
    "href": "disc10/disc10.html",
    "title": "11  SQL (will update once code works)",
    "section": "",
    "text": "11.1 Link to Slides",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>SQL (will update once code works)</span>"
    ]
  },
  {
    "objectID": "disc10/disc10.html#sql-syntax",
    "href": "disc10/disc10.html#sql-syntax",
    "title": "11  SQL (will update once code works)",
    "section": "11.3 SQL Syntax",
    "text": "11.3 SQL Syntax\nAll SQL queries should follow this basic framework. Note that the order of the clauses matter.\nSELECT [DISTINCT] ___&lt;columns&gt;___\nFROM ___&lt;tables&gt;___\n[WHERE ___&lt;predicate&gt;___]\n[GROUP BY ___&lt;columns&gt;___]\n[HAVING ___&lt;predicate&gt;___]\n[ORDER BY ___&lt;columns&gt;___]\n[LIMIT ___&lt;number of rows&gt;___]\n\n11.3.1 Q1\nFor this question, we will be working with the UC Berkeley Undergraduate Career Survey dataset, named survey. Each year, the UC Berkeley Career Center surveys graduating seniors for their plans after graduating. Below is a sample of the full dataset that contains many thousands of rows.\n\n\n\n\n\nCode\n# Run this cell to create the survey table for Q1\ndata = {'j_name': ['Llama Technician','Software Engineer','Open Source Maintainer','Big Data Engineer', 'Data Analyst', 'Analyst Intern'],\n        'c_name': [\"Google\",\"Salesforce\", \"Github\",\"Microsoft\",\"Startup\",\"Google\"],\n        'c_location' : ['Mountain View', 'SF', 'SF', 'Redmond', 'Berkeley', 'SF'],\n        'm_name': [\"Applied Math\",\"ORMS\",\"Computer Science\", \"Data Science\", \"Data Science\",\"Philosophy\"]\n        }\n\nsurvey = pd.DataFrame(data, columns = list(data.keys()))\n\n\nEach record of the survey table is an entry corresponding to a student. We have the job title, company information, and the student’s major.\n\n11.3.1.1 1a\nWrite an SQL query that selects all data science major graduates that got jobs in Berkeley. The result generated by your query should include all 4 columns.\n# write your query here\n\n\nAnswer\n\nSELECT * FROM survey\nWHERE m_name = 'Data Science'\nAND c_location = 'Berkeley';\n\n\n\n\n11.3.1.2 1b\nWrite an SQL query to find the top 2 most popular companies that data science graduates will work at, from most popular to 2nd most popular.\n\n-- write your query here --\nSELECT c_name, ____________ AS count\nFROM survey\nWHERE _____________ = 'Data Science'\nGROUP BY ______________\nORDER BY ______________\nLIMIT 2;\n\n\n\nAnswer\n\n#| connection: con",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>SQL (will update once code works)</span>"
    ]
  },
  {
    "objectID": "disc10/disc10.html#joins",
    "href": "disc10/disc10.html#joins",
    "title": "11  SQL (will update once code works)",
    "section": "11.4 Joins",
    "text": "11.4 Joins\n\n\n\nNote: You do not need the JOIN keyword to join SQL tables. The following are equivalent:\n`SELECT column1, column2`\n\n`FROM table1, table2`\n\n`WHERE table1.id = table2.id;`\n\n\n\n`SELECT column1, column2`\n\n`FROM table1 JOIN table2` \n\n`ON table1.id = table2.id;`\n\n11.4.1 Q2\nIn the figure above, assume table1 has \\(m\\) records, while table2 has \\(n\\) records. Describe which records are returned from each type of join. What is the maximum possible number of records returned in each join? Consider the cases where on the joined field, (1) both tables have unique values, and (2) both tables have duplicated values. Finally, what is the minimum possible number of records returned in each join?\nWrite your answer in this cell\n\n\n11.4.2 Q3\nConsider the following real estate schema (underlined column names have unique values and no duplicates):\n\n homes(home_id int, city text, bedrooms int, bathrooms int, area text) \n transactions(home_id int, buyer_id int, seller_id int, transaction_date date, sale_price int) \n buyers(buyer_id int, name text) \n sellers(seller_id int, name text) \n\nFill in the blanks in the SQL query to find the home_id, selling price, and area for each home in Berkeley with an area greater than 600. If the home has not been sold yet and has an area greater than 600, it should still be included in the table with the price as None.\n-- fill in the blanks --\nSELECT \nFROM _________\n_________ JOIN _________\nON _______________\nWHERE _______________;",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>SQL (will update once code works)</span>"
    ]
  },
  {
    "objectID": "disc10/disc10.html#more-sql-queries",
    "href": "disc10/disc10.html#more-sql-queries",
    "title": "11  SQL (will update once code works)",
    "section": "11.5 More SQL Queries",
    "text": "11.5 More SQL Queries\n\n11.5.1 Q4\nExamine this schema for these two tables:\nCREATE TABLE cat_owners (\n    id integer, \n    name text, \n    age integer,\n    PRIMARY KEY (id)\n); \n\nCREATE TABLE cats (\n    id integer\n    owner_id integer, \n    name text, \n    breed text, \n    age integer, \n    PRIMARY KEY (id),\n    FOREIGN KEY (owner_id) REFERENCES cat_owners\n);\n\n11.5.1.1 4a\nWrite an SQL query to create an almost identical table as cats, except with an additional column Nickname that has the value “Kitten” for cats less than or equal to the age of 1, “Catto” for cats between 1 and 15, and “Wise One” for cats older than or equal to 15\n-- write your query here --\n\n\n11.5.1.2 4b\nConsidering only cats with ages strictly greater than 1, write an SQL query that returns the owner_ids of owners that own more than one cat.\n-- write your query here --\n\n\n11.5.1.3 4c\nWrite an SQL query that returns the total number of cats each owner_id owns sorted by the number of cats in descending order. There should be two columns (owner_id and num_cats).\n-- write your query here --\n\n\n11.5.1.4 4d\nWrite an SQL query to figure out the names of all of the cat owners who have a cat named Pishi.\n-- write your query here --\n\n\n11.5.1.5 4e\nIt is possible to have a cat with an owner not in the cat_owners table? Explain your answer.\nWrite your answer in this cell\n\n\n11.5.1.6 4f\nWrite an SQL query to select all rows from the cats table that have cats of the top 2 most popular cat breeds.\n\n-- write your query here --",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>SQL (will update once code works)</span>"
    ]
  },
  {
    "objectID": "disc10/disc10.html#setup",
    "href": "disc10/disc10.html#setup",
    "title": "11  SQL (will update once code works)",
    "section": "11.2 Setup",
    "text": "11.2 Setup\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport duckdb\n\n%load_ext sql\n\nconn = duckdb.connect()\nconn.query(\"INSTALL sqlite\")\n%sql conn --alias duckdb\n\n\n\n\nCode\n# Run this cell to create the survey table for Q1\ndata = {'j_name': ['Llama Technician','Software Engineer','Open Source Maintainer','Big Data Engineer', 'Data Analyst', 'Analyst Intern'],\n        'c_name': [\"Google\",\"Salesforce\", \"Github\",\"Microsoft\",\"Startup\",\"Google\"],\n        'c_location' : ['Mountain View', 'SF', 'SF', 'Redmond', 'Berkeley', 'SF'],\n        'm_name': [\"Applied Math\",\"ORMS\",\"Computer Science\", \"Data Science\", \"Data Science\",\"Philosophy\"]\n        }\n\nsurvey = pd.DataFrame(data, columns = list(data.keys()))\n\n\n\n\nCode\n# Run this cell to create the tables for Q3\nhomes_data = {'home_id': [1,2,3,4,5,6],\n        'city': [\"Berkeley\",\"San Jose\",\"Berkeley\",\"Berkeley\",\"Berkeley\", \"Sunnyvale\"],\n        'bedrooms': [2,1,5,3,4,1],\n        'bathrooms': [2,2,1,1,3,2],\n        'area': [str(i) for i in [500,750,1000,1500,500,1000]] \n        }\n\nhomes = pd.DataFrame(homes_data, columns = list(homes_data.keys()))\n\ntransactions_data = {'home_id': [1,2,3,5],\n        'buyer_id': [5,6,7,8],\n        'seller_id': [8,7,6,5],\n        'transaction_data': ['1/12/2001','4/14/2001','8/11/2001','12/21/2001'],\n        'sale_price': [1000,500,750,1200]\n        }\n\ntransactions = pd.DataFrame(transactions_data, columns = list(transactions_data.keys()))\n\n\nbuyers_data = {'buyer_id': [5,6,7,8],\n        'name': [\"Xiaorui\",\"Conan\",\"Rose\",\"Brandon\"],\n        }\n\nbuyers = pd.DataFrame(buyers_data, columns = list(buyers_data.keys()))\n\nseller_data = {'seller_id': [8,7,6,5],\n        'name': [\"Shreya\",\"Emrie\",\"Jake\",\"Sam\"],\n        }\n\nseller = pd.DataFrame(seller_data, columns = list(seller_data.keys()))\n\n\n\n\nCode\n# Run this cell to create the tables for Q4\ncat_owners_data = {'id': [10,11,12],\n        'name': [\"Alice\",\"Bob\",\"Candice\"],\n        }\n\ncat_owners = pd.DataFrame(cat_owners_data, columns = list(cat_owners_data.keys()))\n\ncats_data = {'id': [51,52,53,54,55],\n        'owner_id': [10, 10, 11, 11, 12],\n        'name': [\"Mittens\",\"Whisker\",\"Pishi\",\"Lucky\",\"Fluffy\"],\n        'breed' : [\"Tabby\",\"Black\",\"Orange\",\"Tabby\",\"Black\"],\n        'age': [2,3,1,2,16]\n        }\n\ncats = pd.DataFrame(cats_data, columns = list(cats_data.keys()))",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>SQL (will update once code works)</span>"
    ]
  },
  {
    "objectID": "disc11/disc11.html",
    "href": "disc11/disc11.html",
    "title": "12  Logistic Regression",
    "section": "",
    "text": "12.1 Link to Slides",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "disc11/disc11.html#logistic-regression",
    "href": "disc11/disc11.html#logistic-regression",
    "title": "12  Logistic Regression",
    "section": "12.2 Logistic Regression",
    "text": "12.2 Logistic Regression\nSuppose we are given the following dataset, with two features (\\(\\Bbb{X}_{:, 0}\\) and \\(\\Bbb{X}_{:, 1}\\)) and one binary response variable (\\(y\\))\n\n\n\n\n\\(\\mathbb{X}_{:, 0}\\)\n\\(\\mathbb{X}_{:, 1}\\)\n\\(y\\)\n\n\n\n\n2\n2\n0\n\n\n1\n-1\n1\n\n\n\n\nHere, \\(\\vec{x}^T\\) corresponds to a single row of our data matrix, not including the \\(y\\) column. Thus, we can write \\(\\vec{x}_1^T\\) as \\(\\vec{x}_1^T = \\left[2 \\quad 2\\right]\\). Note that there is no intercept term!\nSuppose you run a Logistic Regression model to determine the probability that \\(Y=1\\) given \\(\\vec{x}\\). We denote probability as \\(P_{\\hat{\\theta}}\\) as opposed to just \\(P\\) to show that \\(\\hat{\\theta}\\) is a like it is in OLS, where we denote our function as \\(f_{\\theta}(x)\\).\n\\[P_{\\hat{\\theta}}(Y=1|\\vec{x}) = \\sigma(\\vec{x}^T \\theta) = \\frac{1}{1 + \\exp(- \\vec{x}^T \\theta)}\\]\nYour algorithm learns that the optimal \\(\\hat\\theta\\) value is \\(\\hat\\theta = \\left[-\\frac{1}{2} \\quad-\\frac{1}{2}\\right]^T\\).\n\n12.2.1 (a)\n\\(\\sigma(z)\\) is called a “sigmoid” function with the equation \\[\\sigma(z) = \\frac{1}{1 + \\exp(-z)}\\] for some arbitrary real number \\(z\\). What is the range of possible values for \\(\\sigma(\\cdot)\\)?\n\n\nAnswer\n\n\n\\(x \\to -\\infty: \\frac{1}{1 + e^{-z}} \\to 0\\)\n\\(x \\to \\infty: \\frac{1}{1 + e^{-z}} \\to \\frac{1}{1 + 0} = 1\\)\n\n\\(0 &lt; \\sigma(z) &lt; 1\\). Note that the sigmoid function can never equal 0 or 1.\n\n\n\n12.2.2 (b)\nCalculate \\(P_{\\hat{\\theta}}(Y=1|\\vec{x}^T=\\left[1 \\quad   0\\right])\\).\n\n\nAnswer\n\nCalculate \\(\\vec{x}^T\\hat{\\theta}\\) first:\n\\[\\begin{align*}\n    \\vec{x}^T\\hat{\\theta} &= \\begin{bmatrix}1 & 0\\end{bmatrix}\\begin{bmatrix} -\\frac{1}{2} \\\\ -\\frac{1}{2} \\end{bmatrix}\\\\\n    &= 1*-\\frac{1}{2} + 0*-\\frac{1}{2}\\\\\n    &= -\\frac{1}{2}\n\\end{align*}\\]\nWe can then plug this into the sigmoid function:\n\\[\\begin{align*}\n    \\sigma(-\\frac{1}{2}) &= \\frac{1}{1 + e^{\\frac{1}{2}}}\\\\\n    &\\approx 0.38\n\\end{align*}\\]\n\n\n\n12.2.3 (c)\nUsing a threshold of \\(T = 0.5\\), what would our algorithm classify \\(y\\) as given the results of part b?\n\n\nAnswer\n\nOur \\(p\\) of \\(0.38\\) is smaller than the threshold of \\(0.5\\), so our algorithm would classify \\(y\\) as class \\(0\\).\nA Decision Rule tells us how to classify a data point:\n\\[\\hat{y} =\n\\begin{cases}\n\\text{Class 1} &\\text{if } \\quad p \\geq T\\\\\n\\text{Class 0} &\\text{if } \\quad p \\lt T\n\\end{cases}\n\\]\n\n\n\n12.2.4 (d)\nThe empirical risk using cross-entropy loss is given by the following expression. Remember, whenever you see \\(\\log\\) in this course, you must assume the natural logarithm (base-\\(e\\)) unless explicitly told otherwise.\n\\[\\begin{align*}\n    R(\\theta) &= -\\dfrac{1}{n} \\sum_{i=1}^{n} \\big( y_i \\log P_{\\theta}(Y=1|\\vec{x_i}) + (1-y_i) \\log  P_{\\theta}(Y=0|\\vec{x_i}) \\big)\n\\end{align*}\\]\nSuppose we run a different algorithm and obtain \\(\\hat\\theta_{new} = \\left[0 \\quad 0\\right]^T\\). Calculate the empirical risk for \\(\\hat\\theta_{new}\\) on our dataset.\n\n\nAnswer\n\n\\[\\begin{align*}\n    R(\\hat\\theta_{new}) &= -\\dfrac{1}{2} \\sum_{i=1}^{2} \\big( y_i \\log P_{\\hat{\\theta_{new}}}(Y=1|\\vec{x_i}) + (1-y_i) \\log  P_{\\hat{\\theta_{new}}}(Y=0|\\vec{x_i}) )\\\\\n    &= -\\frac{1}{2} [(0 \\log P_{\\hat{\\theta_{new}}}(Y=1|\\vec{x_1}) + 1 \\log  P_{\\hat{\\theta_{new}}}(Y=0|\\vec{x_1})) + \\\\ & (1 \\log P_{\\hat{\\theta_{new}}}(Y=1|\\vec{x_2}) + 0 \\log  P_{\\hat{\\theta_{new}}}(Y=0|\\vec{x_2}))] \\\\\n    &= -\\frac{1}{2} (\\log P_{\\hat{\\theta_{new}}}(Y=0|\\vec{x_1}) + \\log P_{\\hat{\\theta_{new}}}(Y=1|\\vec{x_2})) \\\\\n    &= -\\frac{1}{2} (\\log (1 - \\sigma(0)) + \\log \\sigma(0)) \\\\\n    &= -\\log(0.5) = \\log 2 \\approx 0.693\n\\end{align*}\\]\nNotice that the inputs to the sigmoid function are \\(0\\) because \\(\\hat{\\theta}_{new}\\) is the zero vector, making all \\(\\vec{x}^T\\hat{\\theta}_{new} = 0\\)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "disc11/disc11.html#linearly-separable-data",
    "href": "disc11/disc11.html#linearly-separable-data",
    "title": "12  Logistic Regression",
    "section": "12.3 Linearly Separable Data",
    "text": "12.3 Linearly Separable Data\nSuppose we have two different Logistic Regression models, A and B, and we run gradient descent for 1000 steps to obtain the model parameters \\(\\hat\\theta_A = \\left[-\\frac{1}{2} \\quad-\\frac{1}{2}\\right]^T\\) and \\(\\hat\\theta_B = \\left[0 \\quad0\\right]^T\\). How do they compare?\nThe dataset is reproduced below for your convenience.\n\n\n\n\n\\(\\mathbb{X}_{:, 0}\\)\n\\(\\mathbb{X}_{:, 1}\\)\n\\(y\\)\n\n\n\n\n2\n2\n0\n\n\n1\n-1\n1\n\n\n\n\n\n12.3.1 (a)\nIs our dataset linearly separable? If so, write the equation of a hyperplane that separates the two classes. Otherwise, briefly explain why not (Hint: draw the two data points).\n\n\nAnswer\n\nYes, the line \\(\\mathbb{X}_{:, 1} = 0\\) (line on horizontal axis) separates the data in feature space!\n\n\n\n\n\n\n12.3.2 (b)\nIf we let gradient descent keep running indefinitely for our two models, will either of them converge given the design matrix above? Why? If not, how can we remedy this?\n\n\nAnswer\n\nNo.\nOur dataset is linearly separable, so the optimal cross-entropy loss is 0. However, a cross-entropy loss of 0 can never be achieved. Remember that \\(\\sigma(z)\\) never outputs exactly 0 or 1, but it can get arbitrarily close!\nHence, no single value of \\(\\theta\\) will ever “minimize” cross-entropy loss (ie. let \\(\\sigma(x^T \\theta) = 0\\)), but gradient descent can bring the cross-entropy loss closer and closer to 0 as \\(\\theta\\) goes to \\(\\pm \\infty\\).\nTo avoid our absolute values of the weights diverging to \\(\\infty\\), we can regularize our cross-entropy loss and penalize arbitrarily large \\(\\theta\\)!\n\n\n\n12.3.3 (c)\nAssume we add the data point \\([3, -2]\\) to the design matrix such that our resulting design matrix is as follows:\n\n\n\n\n\\(\\mathbb{X}_{:, 0}\\)\n\\(\\mathbb{X}_{:, 1}\\)\n\\(y\\)\n\n\n\n\n2\n2\n0\n\n\n1\n-1\n1\n\n\n3\n-2\n0\n\n\n\n\nIs it possible to achive a perfect accuracy using logistic regression?\n\n\nAnswer\n\nThe data is still linearly separable, so we can train a logistic regression model to achieve perfect accuracy! For example, \\(\\mathbb{X}_{:, 0} = 1.5\\) would result in perfect accuracy.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "disc11/disc11.html#roc-curves",
    "href": "disc11/disc11.html#roc-curves",
    "title": "12  Logistic Regression",
    "section": "12.4 ROC Curves",
    "text": "12.4 ROC Curves\nConsider the following ROC (receiver operating characteristic) curves that were each created from different models.\n\n\n\n\n12.4.1 (a)\nCompare the Area Under the Curve (AUC) of Line 1 and Line 4. What kind of model would predict Line 1? What kind of model would predict Line 4?\n\n\nAnswer\n\nLine 1 (solid black line) is known as a “perfect predictor”; it always predicts the correct class for \\(y\\), so its true positive rate (TPR) is 1, and its false positive rate (FPR) is 0. Because we want our classifier to be as close as possible to the perfect predictor, we aim to maximize the AUC.\nOn the other hand, Line 4 (solid grey line) is a random predictor that predicts \\(y=1\\) with a probability of 0.5 and \\(y=0\\) with a probability of 0.5. It’s AUC = 0.5, and it does no better than a random coin flip (proof in course notes).\n\n\n\n12.4.2 (b)\nSuppose we fix the decision threshold for all 4 models such that we get an FPR of 0.1 when we evaluate our model. In order of most to least preferred, rank models given their ROC curve.\n\n\nAnswer\n\nSince the FPR is now fixed at 0.1, the only thing that we can adjust is the TPR. To determine the most to least preferred model, we look at each model’s TPR at an FPR of 0.1. Higher TPRs are better because they indicate that a greater proportion of true positive cases are correctly identified by the model. Hence, we get the following ranking: \\[\\text{Line 1} &gt; \\text{Line 3} &gt;\\text{Line 2} &gt;\\text{Line 4} \\]",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "disc11/disc11.html#performance-metrics",
    "href": "disc11/disc11.html#performance-metrics",
    "title": "12  Logistic Regression",
    "section": "12.5 Performance Metrics",
    "text": "12.5 Performance Metrics\nHere are some classification performance metrics (from Fall 2023 Final Reference Sheet):\n\n\n\n\nMetric\nFormula\nOther Names\n\n\n\n\nAccuracy\n\\(\\frac{TP+TN}{n}\\)\n\n\n\nPrecision\n\\(\\frac{TP}{TP + FP}\\)\n\n\n\nRecall/TPR\n\\(\\frac{TP}{TP + FN}\\)\nTrue Positive Rate, Sensitivity\n\n\nFPR\n\\(\\frac{FP}{FP + TN}\\)\nFalse Positive Rate, Specificity\n\n\n\n\nSuppose we train a binary classifier on the following dataset where \\(y\\) is the set of true labels, and \\(\\hat{y}\\) is the set of predicted labels:\n\n\n\n\n\\(y\\)\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n\n\n\n\n\\(\\hat{y}\\)\n0\n1\n1\n1\n1\n1\n1\n0\n0\n0\n\n\n\n\n\n12.5.1 (a)\nFill out the confusion matrix for the given data. Hint: The first row contains the true negatives and false positives, and the second row contains false negatives and true positives (in that order).\n\n\n\n\nTN:  \nFP:  \n\n\n\n\nFN:  \nTP:  \n\n\n\n\n\n\nAnswer\n\n\n\n\n\nTN: 1\nFP: 4\n\n\n\n\nFN: 3\nTP: 2\n\n\n\n\n\n\n\n12.5.2 (b)\nThe precision of our classifier. Write your answer as a simplified fraction.\n\n\nAnswer\n\n\\(\\text{Precision} = \\frac{TP}{TP + FP} = \\frac{2}{2 + 4} = \\frac{1}{3}\\)\n\n\n\n12.5.3 (c)\nThe recall of our classifier. Write your answer as a simplified fraction.\n\n\nAnswer\n\n\\(\\text{Recall} = \\frac{TP}{TP + FN} = \\frac{2}{2+3} \\frac{2}{5}\\)\n\n\n\n12.5.4 (d)\n(Discussion) It is revealed that this dataset describes the results of an algorithm used to predict whether someone is at risk of developing a severe disease with expensive treatment. You are tasked with improving the classifier. Which metrics should you aim to optimize for (Accuracy/Precision/Recall)? Explain your reasoning. (Things to consider: cost of treatment, the severity of disease)\n\n\nAnswer\n\nThere is no singular correct answer, but here are some examples of reasons.\n\nAccuracy: Since this dataset is fairly balanced, accuracy is not too bad of a metric. (3/10). The main flaw of using accuracy as the metric is that it is agnostic towards the original class of the data point when evaluating performance.\nPrecision: Optimizing for precision means that we care more about making sure the positives we output are truly positive. In this setting, we want to ensure that the people we predict to have the disease truly have the disease. If the cost of treatment is very expensive, we don’t want to overburden people who may not have this disease financially.\nRecall: Optimizing for recall means we care more about detecting all the true positives from the dataset. In this setting, we want to make sure that almost everyone who has the disease knows they have it. If the disease is particularly deadly, then we would be aiming to save the most lives.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "disc12/disc12.html",
    "href": "disc12/disc12.html",
    "title": "13  PCA + Clustering",
    "section": "",
    "text": "13.1 Link to Slides",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>PCA + Clustering</span>"
    ]
  },
  {
    "objectID": "disc12/disc12.html#link-to-slides",
    "href": "disc12/disc12.html#link-to-slides",
    "title": "13  PCA + Clustering",
    "section": "",
    "text": "Note\n\n\n\nTerminology: The notation used for PCA starting from Spring 2024 differs from previous semesters a bit.\n\nPrincipal Component: The columns of \\(V\\). These vectors specify the principal coordinate system and represent the directions along which the most variance in the data is captured.\nLatent Vector Representation of X: The projection of our data matrix \\(X\\) onto the principal components, \\(Z = XV = US\\) (as denoted in lecture). The columns of \\(Z\\) are called latent factors or component scores. In previous semesters, the terminology was different and this was termed the principal components of \\(X\\).\n\\(S\\) (as in SVD): The diagonal matrix containing all the singular values of \\(X\\).\n\\(\\Sigma\\) : The covariance matrix of \\(X\\). Assuming \\(X\\) is centered, \\(\\Sigma = X^T X\\). In previous semesters, the singular value decomposition of \\(X\\) was written out as \\(X=U \\Sigma V^T\\), but we now use \\(X=USV^T\\). Note the difference between \\(\\Sigma\\) in that context compared to this semester.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>PCA + Clustering</span>"
    ]
  },
  {
    "objectID": "disc12/disc12.html#pca-basics",
    "href": "disc12/disc12.html#pca-basics",
    "title": "13  PCA + Clustering",
    "section": "13.2 PCA Basics",
    "text": "13.2 PCA Basics\nConsider the following dataset, where \\(X\\) is the corresponding \\(4 \\times 3\\) design matrix. The mean and variance for each of the features are also provided.\n\n\n\n\nObservations\nFeature 1\nFeature 2\nFeature 3\n\n\n\n\n1\n-3.59\n7.39\n-0.78\n\n\n2\n-8.37\n-5.32\n0.90\n\n\n3\n1.75\n-0.61\n-0.62\n\n\n4\n10.21\n-1.46\n0.50\n\n\nMean\n0\n0\n0\n\n\nVariance\n47.56\n21.35\n0.51\n\n\n\n\nSuppose we perform a singular value decomposition (SVD) on this data to obtain \\(X = U S V^T\\):\nNote: \\(U\\) and \\(V^T\\) are not perfectly orthonormal due to rounding to 2 decimal places.\n\\[\\begin{align*}\n    U =\n    \\begin{bmatrix}\n        -0.25 & 0.81 & 0.20 \\\\\n        -0.61 & -0.56 & 0.24 \\\\\n        0.13 & -0.06 & -0.85 \\\\\n        0.74 & -0.18 & 0.41 \\\\\n    \\end{bmatrix}, \\qquad\n    S =\n    \\begin{bmatrix}\n        13.79 & 0 & 0 \\\\\n        0 & 9.32 & 0 \\\\\n        0 & 0 & 0.81 \\\\\n    \\end{bmatrix}, \\qquad\n    V^T =\n    \\begin{bmatrix}\n        1.00 & 0.02 & 0.00 \\\\\n        -0.02 &  0.99 & -0.13 \\\\\n        0.00 & 0.13 & 0.99 \\\\\n    \\end{bmatrix}\n\\end{align*}\\]\n\n13.2.1 (a)\nRecall that we define the columns of \\(V\\) as the principal components. By projecting \\(X\\) onto the principal components (i.e. computing \\(XV\\)), we can construct the latent vector representation of \\(X\\). Alternatively, you can also calculate the latent vector representation using \\(US\\). Using \\(X=USV^T\\) prove that \\(XV = US\\).\n\n\nAnswer\n\nSince \\(V\\) is orthonormal, we know that \\(V^TV = I\\). Starting with \\(X = U S V^T\\), we right multiply by \\(V\\) on both sides:\n\\[\\begin{align*}\nXV &= U S V^TV\\\\\n&= U S I\\\\\n&= U S\n\\end{align*}\\]\n\n\n\n13.2.2 (b)\nCompute the projection of \\(X\\) onto the first principal component, also called the first latent factor (round to 2 decimal places).\n\n\nAnswer\n\nWe compute the first latent factor by multiplying \\(X\\) by the first row of \\(V^T\\) to get \\(\\approx \\begin{bmatrix} -3.44 & -8.47 & 1.74 & 10.18 \\end{bmatrix}^T\\) (your values may differ slightly due to rounding).\nYou can also compute the first latent factor by observing that \\(XV = U S\\). Therefore, the first latent factor is also the first column of \\(U S\\).\n\n\n\n13.2.3 (c)\nWhat is the component score of the first principal component? In other words, how much of the total variance of \\(X\\) is captured by the first principal component?\n\n\nAnswer\n\nThe variance captured by \\(i\\)-th principal component of the original data \\(X\\) is equal to \\[\\frac{(i\\text{-th singular value})^2}{\\text{number of observations } n}\\]\nIn this case, \\(n = 4\\), and \\(s_1 = 13.79\\). Therefore, the component score can be computed as follows:\n\\[\\frac{13.79^2}{4} = 47.54\\]\n\n\n\n13.2.4 (d) (Bonus)\nGiven the results of (a), how can we interpret the rows of \\(V^T\\)? What do the values in these rows represent?\n\n\nAnswer\n\nThe rows of \\(V^T\\) are the same as the columns of \\(V\\) which we know are the principal components of \\(X\\). Each latent factor of \\(X\\) is a linear combination of \\(X\\)’s features. The rows of \\(V^T\\) correspond to the weights of each feature in the linear combinations that make up their respective latent factors.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>PCA + Clustering</span>"
    ]
  },
  {
    "objectID": "disc12/disc12.html#applications-of-pca",
    "href": "disc12/disc12.html#applications-of-pca",
    "title": "13  PCA + Clustering",
    "section": "13.3 Applications of PCA",
    "text": "13.3 Applications of PCA\nMinoli wants to apply PCA to food_PCA, a dataset of food nutrition information to understand the different food groups.\n\n\n\nShe needs to preprocess her current dataset in order to use PCA.\n\n13.3.1 (a)\nWhich of the following are appropriate preprocessing steps when performing PCA on a dataset?\n\\(\\Box\\) Transform each row to have a magnitude of 1 (Normalization)\n\\(\\Box\\) Transform each column to have a mean of 0 (Centering)\n\\(\\Box\\) Transform each column to have a mean of 0 and a standard deviation of 1 (Standardization)\n\\(\\Box\\) None of the above\n\n\nAnswer\n\n\nTransform each column to have a mean of 0 (Centering)\nTransform each column to have a mean of 0 and a standard deviation of 1 (Standardization)\n\nWe can use standardization or centering of the columns for PCA, since each column contains values of a particular feature for many observations. Standardization ensures that the standard deviation of each collection of feature values is \\(1\\), so that the variability in each feature across the data points is on a uniform scale. Additionally, we cannot compute the covariance matrix correctly using SVD if the feature columns are not centered with mean 0. Choice (A) is incorrect because it doesn’t really make sense to preprocess by row in PCA, since PCA is all about finding combinations of features (columns) as opposed to rows.\n\n\n\n13.3.2 (b)\nAssume you have correctly preprocessed your data using the correct response in part (a). Write a line of code that returns the first 3 latent factors assuming you have the correctly preprocessed food_PCA and the following variables returned by SVD.\nu, s, vt = np.linalg.svd(food_PCA, full_matrices = False)\nfirst_3_latent_factors = ___________\n\n\nAnswer\n\nX @ vt.T[:, :3]\nIt is also possible to use U and S instead, although it may be a little more complicated that way.\nNote the following operators:\n\n**@** for matrix multiplication\n.T to transpose\n\n\n\n\n13.3.3 (c)\nThe scree plot below depicts the proportion of variance captured by each principal component\n\n\n\nWhich of the following lines of code could have created the plot above?\n\\(\\bigcirc\\) plt.plot(s**2/np.sum(s**2), u)\n\\(\\bigcirc\\) plt.plot(food_PCA[:, :7], s**2/np.sum(s))\n\\(\\bigcirc\\) plt.plot(np.arange(1, food_PCA.shape[1]+1), s**2/np.sum(s**2))\n\\(\\bigcirc\\) plt.plot(np.arange(1, food_PCA.shape[1]+1), s**2/np.sum(s))\n\\(\\bigcirc\\) plt.plot(u@s, s**2/np.sum(s**2))\n\n\nAnswer\n\nThe variance captured by the \\(i\\)-th PC is given by \\(\\frac{s[i]^2}{n}\\), where \\(n\\) is the number of rows. Therefore, the proportion of variance captured by the \\(i\\)-th PC must be given by \\[\\frac{\\frac{s[i]^2}{n}}{\\frac{\\text{np.sum}(s^2)}{n}}\\] Using NumPy operations, we can calculate the \\(y\\)-values as\ns**2/np.sum(s**2).\nThe corresponding \\(x\\)-values should run from 1 to the number of columns of\nfood_PCA, which gives us np.arange(1, food_PCA.shape[1]+1)\n\n\n\n13.3.4 (d)\nUsing the elbow method, how many principal components should we choose to represent the data?\n\n\nAnswer\n\nThe elbow of a scree plot is the point where adding more principal components results in diminishing returns. In other words, we look for the “elbow” in the curve just before the line flattens out. This point is at PC 3 in the graph above, so we choose 3 as the number of principal components.\nNote that not all scree plots will have an obvious elbow.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>PCA + Clustering</span>"
    ]
  },
  {
    "objectID": "disc12/disc12.html#interpreting-pca-plots",
    "href": "disc12/disc12.html#interpreting-pca-plots",
    "title": "13  PCA + Clustering",
    "section": "13.4 Interpreting PCA Plots",
    "text": "13.4 Interpreting PCA Plots\nXiaorui has three datasets \\(A\\), \\(B\\), and \\(C \\in \\mathbb{R}^{100 \\times 2}\\). That is, each dataset consists of 100 data points in two dimensions. He visualizes the datasets using scatterplots, labeled Plot A, Plot B, and Plot C, respectively:\n\n\n\n\n13.4.1 (a)\nIf he applies PCA to each of the above datasets and uses only the first principal component, which dataset(s) would have the lowest reconstruction error? Select all that apply.\n\\(\\Box\\) Dataset \\(A\\)\n\\(\\Box\\) Dataset \\(B\\)\n\\(\\Box\\) Dataset \\(C\\)\n\\(\\Box\\) Cannot determine with the given information\n\n\nAnswer\n\nDataset \\(B\\)\nMost of the variance in Plot B lies roughly about a single line. The variance captured by the second PC is far smaller in Plot B compared to plots A and C. In both Plot A and Plot C, there are significant amounts of variance in two orthogonal directions, so PC1 would not be able to capture as much of the total variance.\nSince we want to have the lowest reconstruction error while using only 1 PC, we pick Plot B.\n\n\n\n13.4.2 (b)\nIf he applies PCA to each of the above datasets and uses the first two principal compo- nents, which dataset(s) would have the lowest reconstruction error? Select all that apply.\n\\(\\Box\\) Dataset \\(A\\)\n\\(\\Box\\) Dataset \\(B\\)\n\\(\\Box\\) Dataset \\(C\\)\n\\(\\Box\\) Cannot determine with the given information\n\n\nAnswer\n\nDataset \\(A\\), Dataset \\(B\\), Dataset \\(C\\)\nNote that all 3 datasets have dimensions \\(100 \\times 2\\). Using two principal components corresponds to using the full singular value decomposition of \\(X\\), so we can perfectly reconstruct it. The sum of the variance captured by each PC will be exactly equal to the sum of the variance captured by both features. In all 3 cases, the reconstruction error would be 0.\n\n\n\n13.4.3 (c)\nSuppose he decides to take the Singular Value Decomposition (SVD) of one of the three datasets, which we will call Dataset \\(X\\). He runs the following piece of code:\n\nX_bar = X - np.mean(X, axis=0)\nU, S, V_T = np.linalg.svd(X_bar)\n\nHe gets the following output for S:\n\narray([15.59204498, 3.85871854])\n\nand the following output for V_T:\n\narray([[0.89238775, -0.45126944], [0.45126944, 0.89238775]])\n\nBased on the given plots and the SVD, which of the following datasets does Dataset \\(X\\) most closely resemble? Select one option.\n\\(\\bigcirc\\) Dataset \\(A\\)\n\\(\\bigcirc\\) Dataset \\(B\\)\n\\(\\bigcirc\\) Dataset \\(C\\)\n\n\nAnswer\n\nWe can identify the direction of the first principal component and figure out what plot it should correspond to. The first PC is the first column of \\(V\\), which is the same as the first row of \\(V^T = [0.89, -0.45]^T\\). This looks like the yellow arrow on the plot below, which aligns with Plot B because it lies in the direction of greatest variance. The direction that captures the most variance for both Plot A and C seems to be perpendicular to what’s provided and does not match up.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>PCA + Clustering</span>"
    ]
  },
  {
    "objectID": "disc12/disc12.html#clustering",
    "href": "disc12/disc12.html#clustering",
    "title": "13  PCA + Clustering",
    "section": "13.5 Clustering",
    "text": "13.5 Clustering\n\n13.5.1 (a)\nDescribe the difference between clustering and classification.\n\n\nAnswer\n\nBoth involve assigning arbitrary classes to each observation. In classifi- cation, we have a labeled training set, i.e. a ground truth we can use to train our model. In clustering, we have no ground truth, and are instead looking for patterns in unlabeled data. Classification falls under supervised machine learning because it is concerned with predicting the correct/specific class given the true labels to the data. Clustering falls under unsupervised machine learning because there are no provided “true” labels - instead it is concerned with the patterns across the data (rather than where the data belongs).\n\n\n\n13.5.2 (b)\nGiven a set of points and their labels (or cluster assignments) from a K-Means clustering, how can we compute the centroids of each of the clusters?\n\n\nAnswer\n\nFor each label, find the mean of all of the data points corresponding to that label. In other words, compute centroids \\(c_i\\) for each label/cluster’s set of data points \\(L_i\\) and corresponding data points \\(x_j \\in L_i\\):\n\\[\nc_i = \\frac{1}{n} \\sum_{x_j \\in L_i} x_j\n\\]\n\n\n\n13.5.3 (c)\nDescribe qualitatively what it means for a data point to have a negative silhouette score.\n\n\nAnswer\n\nThe silhouette score of a data point is negative if it is on average closer to the points in the neighboring cluster than its own cluster.\nRecall the formula for calculating the silouhette score:\n\\[\nS = \\frac{B - A}{\\max{(A, B)}}\n\\]\nwhere \\(A\\) is the average distance to the other points in the cluster, and B is the average distance to points in the closest cluster.\n\n\n\n13.5.4 (d)\nSuppose that no two points have the same distance from each other. Are the cluster labels computed by K-means always the same for a given dataset? What about for max agglomerative clustering?\n\n\nAnswer\n\nThe cluster labels computed by K-means could be different if we change the initial centers. This cannot happen for max agglomerative clustering because every point starts in its own cluster, and there is no choice involving initialization.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>PCA + Clustering</span>"
    ]
  }
]