[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data 100 Discussion Notes",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>disc-sp25</span>"
    ]
  },
  {
    "objectID": "disc01/disc01.html",
    "href": "disc01/disc01.html",
    "title": "2  Math Prerequisites",
    "section": "",
    "text": "2.1 Linear Algebra Fundamentals\nLinear algebra is what powers linear regression, logistic regression, and PCA (concepts that we will be studying in this course). This question aims to build an understanding of how matrix-vector operations work.\nConsider yourself starstruck: it’s 2016, and you just spotted the first family of music, Beyonce, husband Jay-Z, and their four year-old daughter Blue, shopping for fruit bowls at Berkeley Bowl. Each bowl contains some fruit and the price of a fruit bowl is simply the total price of all of its individual fruit.\nBerkeley Bowl has apples for $2, bananas for $1, and cantaloupes for $4 (expensive!). The price of each of these can be written in a vector:\n\\[\\begin{align*}\n    \\vec{v} =\n        \\begin{bmatrix}\n        2\\\\\n        1\\\\\n        4\n        \\end{bmatrix}\n\\end{align*}\\]\nBerkeley Bowl sells the following fruit bowls: 1. 2 of each fruit 2. 5 apples and 8 bananas 3. 2 bananas and 3 cantaloupes 4. 10 cantaloupes",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Math Prerequisites</span>"
    ]
  },
  {
    "objectID": "disc01/disc01.html#linear-algebra-fundamentals",
    "href": "disc01/disc01.html#linear-algebra-fundamentals",
    "title": "2  Math Prerequisites",
    "section": "",
    "text": "2.1.1 (a)\nDefine a matrix \\(B\\) such that \\(B\\vec{v}\\) evaluates to a length 4 column vector containing the price of each fruit bowl. The first entry of the result should be the cost of fruit bowl #1, the second entry the cost of fruit bowl #2, etc.\n\n\nAnswer\n\nRecognize that each of the costs can be expressed as a linear combination of quantities (\\(B\\)) and prices (\\(\\vec{v}\\)). The matrix \\(B\\) of quantities should have each row representing the unique fruit bowls, and each column representing the unique fruits.\n\\[\\begin{align*}\nB = \\begin{bmatrix}\n2 & 2 & 2\\\\\n5 & 8 & 0\\\\\n0 & 2 & 3\\\\\n0 & 0 & 10\n\\end{bmatrix}\n\\end{align*}\\]\n\n\n\n2.1.2 (b)\nBeyonce, Jay-Z, and Blue make the following purchases:\n\nBeyonce buys 2 fruit bowl #1s and 1 fruit bowl #2\nJay-Z buys 1 of each fruit bowl\nBlue buys 10 fruit bowl #4s\n\nDefine a matrix \\(A\\) such that the matrix expression \\(AB\\vec{v}\\) evaluates to a length 3 column vector containing how much each of them spent. The first entry of the result should be the total amount spent by Beyonce, the second entry the amount spend by Jay-Z, etc.\n\n\nAnswer\n\nWe know that \\(B\\vec{v}\\) represent the price of each fruit bowl. Therefore, in order to figure out how much each of them spent, we can multiply \\(A\\), a matrix containing the fruit bowls bought by each of the individuals, by the vector of prices \\(B\\vec{v}\\). Each row in \\(A\\) should represent one individual, and each column should represent one fruit (\\(B\\vec{v}\\) being \\(4\\times1\\) tells us that \\(A\\) should be \\(3\\times4\\) instead of the other way around)\n\\[\\begin{align*}\nA = \\begin{bmatrix}\n2 & 1 & 0 & 0\\\\\n1 & 1 & 1 & 1\\\\\n0 & 0 & 0 & 10\n\\end{bmatrix}\n\\end{align*}\\]\n\n\n\n2.1.3 (c)\nLet’s suppose Berkeley Bowl changes their fruit prices, but you don’t know what they changed their prices to. Beyonce, Jay-Z and Blue buy the same quantity of fruit bowls and the number of fruit in each bowl is the same, but now they each spent these amounts:\n\\[\\begin{align*}\n\\vec{x} = \\begin{bmatrix}80 \\\\ 80 \\\\ 100\\end{bmatrix}\n\\end{align*}\\]\nLet \\(\\vec{v_2}\\) be a vector containing the new prices of each fruit. Express \\(\\vec{v_2}\\) in terms of \\(A\\), \\(B\\), and \\(\\vec{x}\\).\n\n\nAnswer\n\nWe know from previous parts that \\(\\vec{x} = AB\\vec{v_2}\\). We can multiply both sides by \\((AB)^{-1}\\), resulting in \\(\\vec{v_2} = (AB)^{-1}\\vec{x}\\). Note that this assumes that \\(AB\\) is invertible!\n\n\n\n2.1.4 (d)\nIn the previous part, we assumed that \\(AB\\) was invertible. Why is \\(AB\\) (as calculated below) invertible? State two conditions for an arbitrary matrix to be invertible.\n\\[\\begin{align*}\nAB = \\begin{bmatrix}\n9 & 12 & 4\\\\\n7 & 12 & 15\\\\\n0 & 0 & 100\n\\end{bmatrix}\n\\end{align*}\\]\n\n\nAnswer\n\nBesides needing to be a square matrix (same number of rows and columns), answers include, but are not limited to:\n\nThe column vectors are linearly independent (meaning the matrix is full column rank)\nThe determinant of the matrix is nonzero\n\nFor (2), we can compute the determinant of a \\(3 \\times 3\\) matrix by the following computation:\n\\[\\begin{align*}\ndet(AB) &= 9*det(\\begin{bmatrix}12 & 15 \\\\ 0 & 100\\end{bmatrix}) - 12 * det(\\begin{bmatrix}7 & 15 \\\\ 0 & 100\\end{bmatrix}) + 4 * det(\\begin{bmatrix}7 & 12 \\\\ 0 & 0\\end{bmatrix})\\\\\n&= 9 * (12*100 - 15*0) - 12 * (7 * 100 - 15 * 0) + 4 * (7 * 0 - 12 * 0)\\\\\n&= 2400 \\neq 0\n\\end{align*}\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Math Prerequisites</span>"
    ]
  },
  {
    "objectID": "disc01/disc01.html#more-linear-algebra",
    "href": "disc01/disc01.html#more-linear-algebra",
    "title": "2  Math Prerequisites",
    "section": "2.2 More Linear Algebra",
    "text": "2.2 More Linear Algebra\nIn this question, we will apply various essential concepts in linear algebra.\n\n2.2.1 (a)\nLinear dependence among a set of vectors \\(\\{\\vec{v_1}, \\vec{v_2}, ..., \\vec{v_n}\\}\\) is defined as follows:\n\n\n\n\n\n\nNote\n\n\n\nIf any (non-trivial) linear combination of the vectors can produce the zero vector, then the set of vectors is linearly dependent.\nIn other words, if we can scale the vectors \\(\\vec{v_j}\\) by scalars \\(\\alpha_j\\) and sum the quantity to obtain the zero vector (given at least one \\(\\alpha_j \\neq 0\\)), then the set is linearly dependent.\n\\[\\begin{align*}\n\\sum_{i=1}^n\\alpha_i\\vec{v_i}=\\vec{0} \\text{ such that some } \\alpha_j \\neq 0 \\Longrightarrow \\text{ linear dependence}\n\\end{align*}\\]\nAny set of vectors such that we cannot obtain the zero vector as described above is linearly independent.\n\n\nThe column rank of a matrix \\(M\\) is the maximal number of linearly independent column vectors in \\(M\\). A full column rank matrix has a column rank equal to the number of column vectors.\nNow consider the matrix \\(M = \\begin{bmatrix}2 & 0 \\\\ 0 & 3\\end{bmatrix}\\) containing two column vectors \\(\\vec{v_1} = \\begin{bmatrix}2 \\\\ 0\\end{bmatrix}\\) and \\(\\vec{v_2} = \\begin{bmatrix} 0 \\\\ 3 \\end{bmatrix}\\). Is it possible to construct the zero vector using a linear combination of the column vectors? What can be concluded about the column rank of the matrix \\(M\\)?\n\n\nAnswer\n\nNo, it is impossible to construct the zero vector if we use at least one \\(\\alpha_i \\neq 0\\) since the first vector can’t affect the second dimension and vice versa. Hence, neither of the vectors can ”undo” each other. As a more formal proof:\n\\[\\begin{align*}\n\\alpha_1\\vec{v_1} + \\alpha_2\\vec{v_2} = \\begin{bmatrix} 2\\alpha_1 \\\\ 3\\alpha_2 \\end{bmatrix}\n\\end{align*}\\]\nIf at least one of the \\(\\alpha\\) values are not \\(0\\), then the vector cannot be \\(\\vec{0}\\). Hence, this matrix is full column rank (i.e., the set of vectors is linearly independent).\n\n\n\n2.2.2 (b)\nThe inverse of a square invertible matrix \\(M\\) is denoted \\(M^{-1}\\). It is defined as a matrix such that \\(MM^{-1} = I\\) and \\(M^{-1}M = I\\). The matrix \\(I\\) is a special matrix known as the identity matrix, where the diagonal elements are \\(1\\) and the non-diagonal elements are \\(0\\).\nConsider the inverse matrix \\(M^{-1} = \\begin{bmatrix}a & b \\\\ c & d\\end{bmatrix}\\) of \\(M\\). Carry out the matrix multiplication \\(MM^{-1}\\), and determine what \\(M^{-1}\\) must be. Recall that \\(M = \\begin{bmatrix}2 & 0 \\\\ 0 & 3 \\end{bmatrix}\\).\n\n\nAnswer\n\nWe carry out the matrix multiplication:\n\\[\\begin{align*}\n\\begin{bmatrix}2 & 0 \\\\ 0 & 3\\end{bmatrix}\\begin{bmatrix}a & b \\\\ c & d\\end{bmatrix} = \\begin{bmatrix}2a & 2b \\\\ 3c & 3d\\end{bmatrix}\n\\end{align*}\\]\nHence, since we know that \\(MM^{-1} = I, b = c = 0\\) and \\(2a = 1\\) and \\(3d = 1\\). Thus, the inverse matrix is:\n\\[\\begin{align*}\nM^{-1} = \\begin{bmatrix}\\frac{1}{2} & 0 \\\\ 0 & \\frac{1}{3}\\end{bmatrix}\n\\end{align*}\\]\nAn interesting fact about diagonal matrices (i.e. matrices that are nonzero on the diagonal entries but zero everywhere else) is that their inverse is simply the elementwise multiplicative inverse (reciprocal) of the diagonal entries!\n\n\n\n2.2.3 (c)\nConsider a difference matrix \\(Q = \\begin{bmatrix}1 & 0 & 5 \\\\ 0 & 1 & 5\\end{bmatrix} = \\begin{bmatrix}\\vec{v_1} & \\vec{v_2} & \\vec{v_3}\\end{bmatrix}\\). What is the column rank of the matrix? Is the matrix invertible?\n\n\nAnswer\n\nWe can construct the zero vector with all of \\(\\vec{v_1}\\), \\(\\vec{v_2}\\), and \\(\\vec{v_3}\\) with the combination \\(5\\vec{v_1} + 5\\vec{v_2} = \\vec{v_3} = 0\\), so we know that the matrix \\(Q\\) does not have full column rank. However, we are not able to construct the zero vector using only any of the two columns. This tells us that the column rank is 2.\nThe matrix is not invertible since it is not square (it has more columns than rows).\n\n\n\n2.2.4 (d)\nThe transpose of a matrix is an opration on matrices of any size in which the rows and columns are “flipped”. In more precise terms, if \\(A\\) is an \\(m \\times n\\) matrix, its transpose \\(A^T\\) is the \\(n \\times m\\) matrix whose element in the ith row and jth column is the element in the jth row and ith column of \\(A\\).\nConsider a matrix \\(R\\), which is equal to the transpose of \\(Q\\): \\(R = Q^T\\). What is the column rank of \\(R\\)?\n\n\nAnswer\n\nWe take the transpose:\n\\[\\begin{align*}\nR = Q^T = \\begin{bmatrix}1 & 0\\\\ 0 & 1 \\\\ 5 & 5\\end{bmatrix}\n\\end{align*}\\]\nThe column rank is 2 because neither column is a scalar multiple of the other. Thus, this matrix is full column rank.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Math Prerequisites</span>"
    ]
  },
  {
    "objectID": "disc01/disc01.html#calculus",
    "href": "disc01/disc01.html#calculus",
    "title": "2  Math Prerequisites",
    "section": "2.3 Calculus",
    "text": "2.3 Calculus\nIn this class, we will have to determine which inputs to functions minimize the output (for instance, when we choose a model and need to fit it to our data). This process involves taking derivatives. In cases where we have multiple inputs, the derivative of our function with respect to one of our inputs is called a partial derivative. For example, given a function \\(f(x, y)\\), the partial derivative with respect to \\(x\\) (denoted by \\(\\frac{\\partial f}{\\partial x}\\)) is the derivative of \\(f\\) with respect to \\(x\\), taken while treating all other variables as if they’re constants.\nSuppose we have the following scalar-values function on \\(x\\) and \\(y\\):\n\\[\\begin{align*}\nf(x, y) = x^2 + 4xy + 2y^3 + e^{-3y} + \\ln(2y)\n\\end{align*}\\]\n\n2.3.1 (a)\nCompute the partial derivative of \\(f(x, y)\\) with respect to \\(x\\).\n\n\nAnswer\n\n\\[\\begin{align*}\n\\frac{\\partial}{\\partial x}f(x, y) = 2x + 4y\n\\end{align*}\\]\n\n\n\n2.3.2 (b)\nCompute the partial derivative of \\(f(x, y)\\) with respect to \\(y\\).\n\n\nAnswer\n\n\\[\\begin{align*}\n\\frac{\\partial}{\\partial y}f(x, y) = 4x + 6y^2 - 3e^{-3y} + \\frac{1}{y}\n\\end{align*}\\]\n\n\n\n2.3.3 (c)\nThe gradient of a function \\(f(x, y)\\) is a vector of its partial derivatives. That is,\n\\[\\begin{align*}\n\\nabla f(x, y) = \\begin{bmatrix}\\frac{\\partial f}{\\partial x} & \\frac{\\partial f}{\\partial y}\\end{bmatrix}^T\n\\end{align*}\\]\nAs a vector, \\(\\nabla f(x, y)\\) has both a magnitude and direction. The direction of \\(\\nabla f(x, y)\\) corresponds to the direction in which the graph of \\(f(x, y)\\) is increasing most steeply from the point \\((x, y)\\). The magnitude gives a sense of how steep the ascent up the graph is in this particular direction. This is a generalization of the single variable case, where \\(f'(x)\\) is the rate of changes of \\(f\\), at the point \\(x\\). In the two-variable case, we specify a direction to evaluate the rate of change, since the function is technically changing in all directions from \\((x, y)\\).\nUsing your answers to the above two parts, compute \\(\\nabla f(x, y)\\) and evaluate the gradient at the point \\((x = 2, y = -1)\\).\n\n\nAnswer\n\n\\[\\begin{align*}\n\\nabla f(x, y) &=\n\\begin{bmatrix}\n2x + 4y\\\\\n4x + 6y^2 - 3e^{-3y} + \\frac{1}{y}\n\\end{bmatrix}\n\\\\\\\\\n\\nabla f(2, -1) &=\n\\begin{bmatrix}\n2(2) + 4(-1)\\\\\n4(2) + 6(-1)^2 - 3e^{-3(-1)} + \\frac{1}{(-1)}\n\\end{bmatrix}\\\\\n&= \\begin{bmatrix} 0 \\\\ 13 - 3e^3 \\end{bmatrix}\n\\end{align*}\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Math Prerequisites</span>"
    ]
  },
  {
    "objectID": "disc01/disc01.html#probability-sampling",
    "href": "disc01/disc01.html#probability-sampling",
    "title": "2  Math Prerequisites",
    "section": "2.4 Probability & Sampling",
    "text": "2.4 Probability & Sampling\n\n\n\nHouses\n\n\nIshani wants to measure interest for a party on her street. She assigns numbers and letters to each house on her street as illustrated above. She picks a letter “a”, “b”, or “c” at random and then surveys every household on the street ending in that letter.\n\n2.4.1 (a)\nWhat is the chance that two houses next door to each other are both in the sample?\n\n\nAnswer\n\nNone of the adjacent houses end in the same letter, so the chance is zero. This is an example of a cluster sample with each cluster representing each group of houses ending in the same letter.\n\nNote: We will no longer be following Ishani’s sampling method of picking a letter and surveying households ending in that letter for subsequent questions.\n\n\n2.4.2 (b)\nIshani decides to collect a simple random sample (SRS) of four houses. What is the probability that house 1a is not in Ishani’s simple random sample of four houses?\n\n\nAnswer\n\nRecall that a simple random sample is a sample drawn uniformly at random without replacement.\nThis time, we are taking a sample of 4 houses. But, we can apply a similar approach from part b to determine the probability of missing house 1a in each of the four selected houses. Then we multiply the four probabilities together to get our answer. The probability that house 1a is not in Ishani’s sample is \\(\\frac{11}{12}*\\frac{10}{11}*\\frac{9}{10}*\\frac{8}{9} = \\frac{8}{12} = \\frac{2}{3}\\)\n\n\n\n2.4.3 (c)\nInstead of surveying every member of each house from the SRS of four houses, Ishani decides to survey two members chosen without replacement from each selected house. Four people live in house 1a, one of whom is Bob. What is the probability that Bob is not chosen in Ishani’s new sample?\n\n\nAnswer\n\nThe probability that house 1a is included in Ishani’s initial SRS of four houses is \\(\\frac{1}{3}\\). Given that house 1a is selected, the probability that Bob is one of the two people surveyed is \\(\\frac{1}{2}\\). Therefore, the probability that Bob is surveyed is \\(\\frac{1}{3}*\\frac{1}{2} = \\frac{1}{6}\\). Thus, the probability that Bob is not selected is \\(1 - \\frac{1}{6} = \\frac{5}{6}\\)\n\n\n\n2.4.4 (d)\nTo increase interest in her party, Ishani randomly selects 4 houses to gift a small present. Assuming she samples with replacement (meaning that a house could be chosen multiple times), what’s the probability that all “c” houses are given a gift?\n\n\nAnswer\n\nFor all “c” houses to be selected, the first house selected can be any of the 4 “c” houses out of the 12 houses total. The second house selected can be any of the 3 “c” houses left (out of 12 houses total). the third house can be any of the 2 “c” houses left, and the final selected house must be the last “c” house left. Therefore, we get \\(\\frac{4}{12}*\\frac{3}{12}*\\frac{2}{12}*\\frac{1}{12} = \\frac{4!}{12^4}\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Math Prerequisites</span>"
    ]
  },
  {
    "objectID": "disc02/disc02.html",
    "href": "disc02/disc02.html",
    "title": "3  Pandas I",
    "section": "",
    "text": "This discussion is all about practicing using pandas, and testing your knowledge about its various functionalities to accomplish small tasks.\nWe will be using the elections dataset from lecture.\n\n# import packages\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\nelections = pd.read_csv('elections.csv')\nelections.head(10)\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\n\n\n1\n1824\nJohn Quincy Adams\nDemocratic-Republican\n113142\nwin\n42.789878\n\n\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\n\n\n3\n1828\nJohn Quincy Adams\nNational Republican\n500897\nloss\n43.796073\n\n\n4\n1832\nAndrew Jackson\nDemocratic\n702735\nwin\n54.574789\n\n\n5\n1832\nHenry Clay\nNational Republican\n484205\nloss\n37.603628\n\n\n6\n1832\nWilliam Wirt\nAnti-Masonic\n100715\nloss\n7.821583\n\n\n7\n1836\nHugh Lawson White\nWhig\n146109\nloss\n10.005985\n\n\n8\n1836\nMartin Van Buren\nDemocratic\n763291\nwin\n52.272472\n\n\n9\n1836\nWilliam Henry Harrison\nWhig\n550816\nloss\n37.721543\n\n\n\n\n\n\n\nWrite a line of code that returns the elections table sorted in descending order by \"Popular vote\". Store your result in a variable named sorted. Would calling sorted.iloc[[0], :] give the same result as sorted.loc[[0], :]?\n\n\nCode\nsorted = elections.sort_values(\"Popular vote\", ascending = False)\nsorted\n\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n178\n2020\nJoseph Biden\nDemocratic\n81268924\nwin\n51.311515\n\n\n182\n2024\nDonald Trump\nRepublican\n77303568\nwin\n49.808629\n\n\n183\n2024\nKamala Harris\nDemocratic\n75019230\nloss\n48.336772\n\n\n179\n2020\nDonald Trump\nRepublican\n74216154\nloss\n46.858542\n\n\n162\n2008\nBarack Obama\nDemocratic\n69498516\nwin\n53.023510\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n110\n1956\nT. Coleman Andrews\nStates' Rights\n107929\nloss\n0.174883\n\n\n141\n1992\nBo Gritz\nPopulist\n106152\nloss\n0.101918\n\n\n99\n1948\nClaude A. Watson\nProhibition\n103708\nloss\n0.212747\n\n\n89\n1932\nWilliam Z. Foster\nCommunist\n103307\nloss\n0.261069\n\n\n6\n1832\nWilliam Wirt\nAnti-Masonic\n100715\nloss\n7.821583\n\n\n\n\n187 rows × 6 columns\n\n\n\n\n\nExplanation\n\n\n\nWe can sort a DataFrame by a column using the .sort_values() function! Remember to specify ascending = False, or else it will sort in increasing order.\n\n\n\nsorted.iloc[[0], :]\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n178\n2020\nJoseph Biden\nDemocratic\n81268924\nwin\n51.311515\n\n\n\n\n\n\n\n\nsorted.loc[[0], :]\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\n\n\n\n\n\n\n\n\n\nExplanation\n\n \n\nThe difference is that .loc[] uses label-based indexing, while .iloc[] uses integer position-based indexing. Using .loc[] will simply grab the row with the label 0 regardless of where it is, while .iloc[] will grab the first row of the sorted DataFrame.\n\n\n\nUsing Boolean slicing, write one line of pandas code that returns a DataFrame that only contains election results from the 1900s.\n\n\nCode\nelections[(elections[\"Year\"] &gt;= 1900) & (elections[\"Year\"] &lt; 2000)]\n\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n54\n1900\nJohn G. Woolley\nProhibition\n210864\nloss\n1.526821\n\n\n55\n1900\nWilliam Jennings Bryan\nDemocratic\n6370932\nloss\n46.130540\n\n\n56\n1900\nWilliam McKinley\nRepublican\n7228864\nwin\n52.342640\n\n\n57\n1904\nAlton B. Parker\nDemocratic\n5083880\nloss\n37.685116\n\n\n58\n1904\nEugene V. Debs\nSocialist\n402810\nloss\n2.985897\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n146\n1996\nHarry Browne\nLibertarian\n485759\nloss\n0.505198\n\n\n147\n1996\nHoward Phillips\nTaxpayers\n184656\nloss\n0.192045\n\n\n148\n1996\nJohn Hagelin\nNatural Law\n113670\nloss\n0.118219\n\n\n149\n1996\nRalph Nader\nGreen\n685297\nloss\n0.712721\n\n\n150\n1996\nRoss Perot\nReform\n8085294\nloss\n8.408844\n\n\n\n\n97 rows × 6 columns\n\n\n\n\n\nExplanation\n\n \n\nWe can “filter” DataFrames by using boolean slicing! 1. Construct a boolean Series that is True if a row contains election results from the 1900s, and False otherwise. * We can use the & (and) logical operator! 1900 or after and before 2000. 2. Use the boolean Series to slice the DataFrame * df[boolean_array]\n\n\n\nWrite one line of pandas code that returns a Series, where the index is the \"Party\", and the values are how many times that party won an election. Only include parties that have won an election.\n\n\nCode\nelections[elections[\"Result\"] == \"win\"][\"Party\"].value_counts()\n\n\nParty\nRepublican               24\nDemocratic               23\nWhig                      2\nDemocratic-Republican     1\nNational Union            1\nName: count, dtype: int64\n\n\n\n\nCode\nelections[elections[\"Result\"] == \"win\"].groupby(\"Party\").size()\n\n\nParty\nDemocratic               23\nDemocratic-Republican     1\nNational Union            1\nRepublican               24\nWhig                      2\ndtype: int64\n\n\n\n\nExplanation\n\n \n\nTwo parts to this! 1. Filter DataFrame to only include winners. * Use boolean slicing again! Construct a boolean Series that has True if the row contains a winner, and False otherwise * elections[elections[\"Result\"] == \"win\"] 2. Within filtered DataFrame (let’s call this winners), count the number of times each party won an election. Two ways to do this. * Extract the Party column from winners, and call value_counts(). * winners[\"Party\"].value_counts() * Group by the Party column, and aggregate by the number of rows in each sub-DataFrame. * winners.groupby(\"Party\").size() * The two methods above return the same thing, except .value_counts() sorts by the values in decreasing order, while .groupby() sort by the index in increasing order!\n\n\n\nWrite a line of pandas code that returns a Series whose index is the years and whose values are the number of candidates that participated in those years’ elections.\n\n\nCode\nelections[\"Year\"].value_counts().head() #.head() to limit output\n\n\nYear\n1996    7\n1948    6\n2016    6\n1976    6\n2008    6\nName: count, dtype: int64\n\n\n\n\nCode\nelections.groupby(\"Year\").size().head() #.head() to limit output\n\n\nYear\n1824    2\n1828    2\n1832    3\n1836    3\n1840    2\ndtype: int64\n\n\n\n\nExplanation\n\n \n\nVery similar to Problem 3! Might even be easier, actually. Each row corresponds to one candidate per election cycle, so we simply need to count the number of times each Year appears in the elections DataFrame. Just like in Problem 3, two ways to do this.\n\nExtract the Year column as a Series, call .value_counts() on it.\n\nelections[\"Year\"].value_counts()\n\n\nGroup by the Year column, creating a sub-DataFrame for each unique Year. Aggregate by .size(), counting the number of rows in each sub-DataFrame.\n\nelections.groupby(\"Year\").size()\n\n\n\n\n\nWrite a line of pandas code that creates a filtered DataFrame named filtered_parties from the elections dataset and keeps only the parties that have at least one election % more than 50%.\n\n\nCode\nfiltered_parties = elections.groupby(\"Party\").filter(lambda df: df[\"%\"].max() &gt; 50)\nfiltered_parties\n\n\n\n\n\n\n\n\n\nYear\nCandidate\nParty\nPopular vote\nResult\n%\n\n\n\n\n0\n1824\nAndrew Jackson\nDemocratic-Republican\n151271\nloss\n57.210122\n\n\n1\n1824\nJohn Quincy Adams\nDemocratic-Republican\n113142\nwin\n42.789878\n\n\n2\n1828\nAndrew Jackson\nDemocratic\n642806\nwin\n56.203927\n\n\n4\n1832\nAndrew Jackson\nDemocratic\n702735\nwin\n54.574789\n\n\n7\n1836\nHugh Lawson White\nWhig\n146109\nloss\n10.005985\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n176\n2016\nHillary Clinton\nDemocratic\n65853514\nloss\n48.521539\n\n\n178\n2020\nJoseph Biden\nDemocratic\n81268924\nwin\n51.311515\n\n\n179\n2020\nDonald Trump\nRepublican\n74216154\nloss\n46.858542\n\n\n182\n2024\nDonald Trump\nRepublican\n77303568\nwin\n49.808629\n\n\n183\n2024\nKamala Harris\nDemocratic\n75019230\nloss\n48.336772\n\n\n\n\n99 rows × 6 columns\n\n\n\n\n\nExplanation\n\n \n\nThis filtering is different from boolean slicing! Boolean slicing considers rows individually, while .filter() considers groups of rows. Rows of a sub-DataFrame either all make it, or none make it.\n\nGroup by the Party column, creating one sub-DataFrame for each party.\n\nelections.groupby(\"Party\")\n\nFilter using .filter()\n\nPass in a function into .filter() that takes in a DataFrame and returns True or False. Can be a lambda function!\n.filter(lambda df: df[\"%\"].max() &gt; 50)\n\nIf the lambda function returns True, it means you keep the entire sub-DataFrame. False means you exclude it entirely!\n\n\n\n\n\n\nWrite a line of pandas code that uses the filtered_parties DataFrame to return a new DataFrame with row indices that correspond to the year and columns that correspond to each party. Each entry should be the total percentage of votes for all the candidates that ran during that particular year for the specified party. Missing values from the dataset (the cases where a party did not have a candidate in a particular year) should be entered as 0. Below is an example.\n\n\n\nCode\nelections_pivot = filtered_parties.pivot_table(\n    index = \"Year\",\n    columns = \"Party\",\n    values = \"%\",\n    aggfunc = np.sum,\n    fill_value = 0)\nelections_pivot.head(10)\n\n\n\n\n\n\n\n\nParty\nDemocratic\nDemocratic-Republican\nNational Union\nRepublican\nWhig\n\n\nYear\n\n\n\n\n\n\n\n\n\n1824\n0.000000\n100.0\n0.0\n0.000000\n0.000000\n\n\n1828\n56.203927\n0.0\n0.0\n0.000000\n0.000000\n\n\n1832\n54.574789\n0.0\n0.0\n0.000000\n0.000000\n\n\n1836\n52.272472\n0.0\n0.0\n0.000000\n47.727528\n\n\n1840\n46.948787\n0.0\n0.0\n0.000000\n53.051213\n\n\n1844\n50.749477\n0.0\n0.0\n0.000000\n49.250523\n\n\n1848\n42.552229\n0.0\n0.0\n0.000000\n47.309296\n\n\n1852\n51.013168\n0.0\n0.0\n0.000000\n44.056548\n\n\n1856\n45.306080\n0.0\n0.0\n33.139919\n0.000000\n\n\n1860\n0.000000\n0.0\n0.0\n39.699408\n0.000000\n\n\n\n\n\n\n\n\n\nExplanation\n\n \n\nFirst thing to notice is that the columns are values of the Party column! This tells us that what we see is a pivot table.\n\nUse the .pivot_table() function on filtered_parties\n\nindex = \"Year\" and columns = \"Party\", saying that the unique values of Year should make up the row indices, and the unique values of Party should make up the columns.\nvalues = \"%\" indicates that we populate the cells with the % values for each combination of Year, Party\naggfunc = np.sum describes how to aggregate the values in a cell\nfill_value = 0 says to impute 0 in case there is no % value for a specific Year, Party combination",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Pandas I</span>"
    ]
  },
  {
    "objectID": "disc03/disc03.html",
    "href": "disc03/disc03.html",
    "title": "4  Pandas II, EDA",
    "section": "",
    "text": "4.1 Dealing with Missing Data\nWhile exploring a Berkeley dataset (separate from babynames) with a million records, you realize that a portion of measurements in different fields are NaN values! You decide to impute these missing values before continuing your EDA. Given the empirical distribution of each of the below variables, determine how to solve the missing data problem. (Note that the data in these graphs are fictional).\nSuppose that you plot “cups of coffee sold at V&A Cafe per day” versus “inches of rain per day” across a period of 2 months, shown below. V&A Cafe is not missing any data, but 30% of the data in “inches of rain” are NaN values that have been represented with “-2”, an impossible amount of rain. Which of the following techniques would be most effective in solving the issue of missing data? (Select all that apply)\nA. Using the mean to impute the missing values\nB. Using the mode(s) to impute the missing values\nC. Using the median to impute the missing values\nD. Dropping any rows with missing values\nE. Imputing missing values through interpolation\nSuppose we examine the amount of money lost/gained in a game of poker and see that this variable is missing 1% of its values. Its distribution, shown below, is constructed from all valid (non-NaN) values. Which of the following techniques would be reasonably effective in solving this issue of missing data? (Select all that apply)\nA. Using the mean to impute the missing values\nB. Using the mode(s) to impute the missing values\nC. Using the median to impute the missing values\nD. Dropping any rows with missing values\nE. Imputing missing values through interpolation\nSuppose that the relationship between students’ time asleep (in hours) and the amount of extra credit they received in Data 100 is shown below. There is no missing data for “hours asleep”, but 0.5% of “extra credit score” is missing. Like in part a, the missing NaN values were replaced with an impossible score of -0.002, making the graph look funky. Which of the following techniques would be most effective in solving this issue of missing data? (Select all that apply)\nA. Using the mean to impute the missing values\nB. Using the mode(s) to impute the missing values\nC. Using the median to impute the missing values\nD. Dropping any rows with missing values\nE. Imputing missing values through interpolation",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pandas II, EDA</span>"
    ]
  },
  {
    "objectID": "disc03/disc03.html#dealing-with-missing-data",
    "href": "disc03/disc03.html#dealing-with-missing-data",
    "title": "4  Pandas II, EDA",
    "section": "",
    "text": "Question 1a\n\n\n\n\n\n\n\n\n\nAnswer\n\nCorrect Options: A, C, E\n\n\n\n\n\nQuestion 1b graph\n\n\n\n\n\n\n\n\n\nAnswer\n\nCorrect Options: B, D\n\n\n\n\n\nQuestion 1c graph\n\n\n\n\n\n\n\n\n\nAnswer\n\nCorrect Options: D, E",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pandas II, EDA</span>"
    ]
  },
  {
    "objectID": "disc03/disc03.html#pandas-eda-exam-prep-modeled-after-fa22-midterm-q1",
    "href": "disc03/disc03.html#pandas-eda-exam-prep-modeled-after-fa22-midterm-q1",
    "title": "4  Pandas II, EDA",
    "section": "4.2 Pandas + EDA exam prep (modeled after Fa22 Midterm Q1)",
    "text": "4.2 Pandas + EDA exam prep (modeled after Fa22 Midterm Q1)\nIt’s the annual Monopoly World Championship! The finalists: Shawn, Amanda, Neil, and Annie are playing Monopoly, a board game where players pay a price to buy properties, which can then generate income for them. Each property can be owned by only one player at a time. At the end of the game, the player with the most money wins.\nShawn wants to figure out which properties are most worth buying. He creates a DataFrame income with data on the current game state, shown on the left. He also finds a DataFrame properties with data on Monopoly properties, shown on the right.\nBoth tables have 28 rows. For brevity, only the first few rows of each DataFrame are shown.\n\n\nCode\n# First DataFrame: income\ndata_income = {\n    'Player': ['Shawn', 'Amanda', 'Neil', np.nan, 'Shawn', 'Annie', 'Amanda'],\n    'Property': ['Boardwalk', 'Park Place', 'Marvin Gardens', 'Kentucky Ave', 'Pennsylvania Ave', 'Oriental Ave', 'Baltic Ave'],\n    'Income Generated': ['$425', '$375', '$200', np.nan, '$150', '$50', '$60']\n}\n\nincome = pd.DataFrame(data_income)\nincome\n\n\n\n\n\n\n\n\n\nPlayer\nProperty\nIncome Generated\n\n\n\n\n0\nShawn\nBoardwalk\n$425\n\n\n1\nAmanda\nPark Place\n$375\n\n\n2\nNeil\nMarvin Gardens\n$200\n\n\n3\nNaN\nKentucky Ave\nNaN\n\n\n4\nShawn\nPennsylvania Ave\n$150\n\n\n5\nAnnie\nOriental Ave\n$50\n\n\n6\nAmanda\nBaltic Ave\n$60\n\n\n\n\n\n\n\nincome\n\nPlayer is the name of the player, as a str.\nProperty is a property currently owned by the player, as a str.\nIncome Generated is the amount of income a player has earned from that property so far, as a str.\n\n\n\nCode\n# Second DataFrame: properties\ndata_properties = {\n    'Property': ['Park Place', 'Oriental Ave', 'Vermont Ave', 'Pacific Ave', 'Boardwalk', 'Illinois Ave', 'Atlantic Ave'],\n    'Property Color': ['Dark Blue', 'Light Blue', 'Light Blue', 'Green', 'Dark Blue', 'Red', 'Yellow'],\n    'Purchase Price': [350.0, 100.0, 100.0, 300.0, 400.0, 240.0, 260.0]\n}\n\nproperties = pd.DataFrame(data_properties)\nproperties\n\n\n\n\n\n\n\n\n\nProperty\nProperty Color\nPurchase Price\n\n\n\n\n0\nPark Place\nDark Blue\n350.0\n\n\n1\nOriental Ave\nLight Blue\n100.0\n\n\n2\nVermont Ave\nLight Blue\n100.0\n\n\n3\nPacific Ave\nGreen\n300.0\n\n\n4\nBoardwalk\nDark Blue\n400.0\n\n\n5\nIllinois Ave\nRed\n240.0\n\n\n6\nAtlantic Ave\nYellow\n260.0\n\n\n\n\n\n\n\nproperties\n\nProperty is the name of the property, as a str. There are 28 unique properties.\nProperty Color is a color group that the property belongs to, as a str. There are 10 unique color groups, and each property belongs to a single group.\nPurchase Price is the price to buy the property, as a float.\n\nNote: For the properties that are not currently owned by any player, the Player and Income Generated columns in the income table have a NaN value.\n(a) What is the granularity of the income table?\n\n\nAnswer\n\n\nProperty\n\nEach unique property has its own row\nNotice how one player can have own multiple properties and can appear in multiple rows! This tells us that the granularity of this table is not Player.\n\n\n(b) Consider the Player and Purchase Price variables. What type of variable is each one? (quantitative, qualitative nominal, qualitative ordinal)\n\n\nAnswer\n\n\nPlayer: Qualitative nominal\nPurchase Price: Quantitative\n\n\n(c) Which of the following line(s) of code successfully returns a Series with the number of properties each player owns? Select all that apply.\n\n\nAnswer\n\n\nincome[\"Player\"].value_counts()\nincome.groupby(\"Player\").size()\n\n\n\n\nCode\nincome.groupby(\"Player\").agg(pd.value_counts)\n\n\n\n\n\n\n\n\n\nProperty\nIncome Generated\n\n\nPlayer\n\n\n\n\n\n\nAmanda\n[1, 1]\n[1, 1]\n\n\nAnnie\n1\n1\n\n\nNeil\n1\n1\n\n\nShawn\n[1, 1]\n[1, 1]\n\n\n\n\n\n\n\n\n\nCode\nincome[\"Player\"].value_counts()\n\n\nPlayer\nShawn     2\nAmanda    2\nNeil      1\nAnnie     1\nName: count, dtype: int64\n\n\n\n\nCode\n# income[\"Player\", \"Property\"].groupby(\"Player\").size()\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe above code will error! Make sure to use double brackets when selecting columns.\n\n\n\n\nCode\nincome.groupby(\"Player\")[[\"Player\"]].count()\n\n\n\n\n\n\n\n\n\nPlayer\n\n\nPlayer\n\n\n\n\n\nAmanda\n2\n\n\nAnnie\n1\n\n\nNeil\n1\n\n\nShawn\n2\n\n\n\n\n\n\n\n(d) He now decides to calculate the amount of profit from each property. He wants to store this in a column called Profit in the income DataFrame. To do this, he first has to transform the Income Generated column to be of a float datatype.\nWrite one line of code to replace the old column with a new column, also called Income Generated, with the datatype modification described above. You may assume that each entry in Income Generated consists of a dollar sign ($) followed by a number, except for the NaN values.\n\n\nCode\nincome[\"Income Generated\"] = income[\"Income Generated\"].str[1:].astype(float)\n\n\n(e) Assuming that the answer to (d) is correct, let’s add a Profit column to the income DataFrame. Fill in the following blanks to do this, and please add arguments to function class as you see appropriate.\nNote: Profit is calculated by subtracting the purchase price from generated income.\ncombined_df = income._____A_____(_______B_______)\nincome[\"Profit\"] = _______C_______\n\n\nCode\ncombined_df = income.merge(properties, on = \"Property\")\nincome[\"Profit\"] = combined_df[\"Income Generated\"] - combined_df[\"Purchase Price\"]\n\n\nShawn realizes he’s lost more money than he’s made. To solve this problem, he begins by writing some Pandas code to merge the Property Color column into the income DataFrame and drops all rows with NaN values. He calls this DataFrame merged_df. Shown below are the first few rows.\n\n\nCode\nmerged_df = pd.DataFrame({\"Player\": [\"Shawn\", \"Amanda\", \"Neil\", \"Shawn\", \"Annie\", \"Amanda\"],\n                          \"Property\": [\"Boardwalk\", \"Park Place\", \"Marvin Gardens\", \"Pennsylvania Ave\", \"Oriental Ave\", \"Baltic Ave\"],\n                          \"Income Generated\": [425., 375., 200., 150., 50., 60.],\n                          \"Profit\": [-25., 25., 50., -100., 0., 0.],\n                          \"Property Color\": [\"Dark Blue\", \"Dark Blue\", \"Yellow\", \"Green\", \"Light Blue\", \"Purple\"]})\nmerged_df\n\n\n\n\n\n\n\n\n\nPlayer\nProperty\nIncome Generated\nProfit\nProperty Color\n\n\n\n\n0\nShawn\nBoardwalk\n425.0\n-25.0\nDark Blue\n\n\n1\nAmanda\nPark Place\n375.0\n25.0\nDark Blue\n\n\n2\nNeil\nMarvin Gardens\n200.0\n50.0\nYellow\n\n\n3\nShawn\nPennsylvania Ave\n150.0\n-100.0\nGreen\n\n\n4\nAnnie\nOriental Ave\n50.0\n0.0\nLight Blue\n\n\n5\nAmanda\nBaltic Ave\n60.0\n0.0\nPurple\n\n\n\n\n\n\n\nShawn decides he will now only buy properties from a color group that he deems “profitable.” He deems a color group “profitable” if at least 50% of the properties in the group that are currently owned by players have made a positive (non-zero) profit for those players.\nFill in the following lines of code to help him display a DataFrame with a subset of the rows in merged_df: the rows with properties that belong to profitable color groups. Your solution may use fewer lines of code than we provide.\n\n\nCode\ndef func(group):\n    if np.mean(group[\"Profit\"] &gt; 0) &gt;= 0.5:\n        return True\n    return False\n\nmerged_df.groupby(\"Property Color\").filter(func)\n\n\n\n\n\n\n\n\n\nPlayer\nProperty\nIncome Generated\nProfit\nProperty Color\n\n\n\n\n0\nShawn\nBoardwalk\n425.0\n-25.0\nDark Blue\n\n\n1\nAmanda\nPark Place\n375.0\n25.0\nDark Blue\n\n\n2\nNeil\nMarvin Gardens\n200.0\n50.0\nYellow",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pandas II, EDA</span>"
    ]
  },
  {
    "objectID": "disc04/disc04.html",
    "href": "disc04/disc04.html",
    "title": "5  Regex, Visualization, and Transformation",
    "section": "",
    "text": "5.1 Regular Expressions\nRegular Expressions (RegEx for short) are an immensely powerful tool for parsing strings. However, it’s many rules make RegEx very confusing, even for veteran users, so please don’t hesitate to ask questions! Here’s a snippet of the RegEx portion of the Fall 2023 Midterm\nReference Sheet:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regex, Visualization, and Transformation</span>"
    ]
  },
  {
    "objectID": "disc04/disc04.html#regular-expressions",
    "href": "disc04/disc04.html#regular-expressions",
    "title": "5  Regex, Visualization, and Transformation",
    "section": "",
    "text": "Regex Reference\n\n\n\n5.1.1 (a)\nWhich string contains a match for the following regular expression, “\\(\\texttt{1+1\\$}\\)”?\n\n\\(\\texttt{What is 1+1}\\)\n\\(\\texttt{Make a wish at 11:11}\\)\n\\(\\texttt{111 Ways to Succeed}\\)\n\n\n\nAnswer\n\n\n\\(\\texttt{Make a wish at 11:11}\\)\n\nNote that there are two operators here, the + and the $. The $ operator states that our match must occur at the end of the string, so that already rules out the third option. Furthermore, the + operator indicates that we need “one or more 1s” which is followed by another 1 (so two or more 1s in total). The only string ending with two or more consecutive 1s is the second option.\n\n\n\n5.1.2 (b)\nWrite a regular expression that matches a string which contains only one word containing only lowercase letters and numbers (including the empty string).\n\n\nAnswer\n\n\\(\\texttt{\\^{}[a-z0-9]*\\$}\\)\n\n\\(\\texttt{[a-z0-9]}\\) to indicate that the word only include lowercase letters and/or numbers\n\\(\\texttt{*}\\) operator to state that the word can contain “0 or more” of \\(\\texttt{[a-z0-9]}\\). Note that we also want to match empty strings, which is why \\(\\texttt{*}\\) is preferred over \\(\\texttt{+}\\) here.\n\\(\\texttt{\\^{}}\\) and \\(\\texttt{\\$}\\) to indicate that the pattern must match the string at the beginning of the string, and at the end of the string respectively. This ensures that the pattern only matches strings with one word (not two or more words).\n\n\n\n\n5.1.3 (c)\nGiven \\(\\texttt{sometext = \"I've got 10 eggs, 20 gooses, and 30 giants.\"}\\), use \\(\\texttt{re.findall}\\) to extract all items and quantities from the string. The result should look like \\(\\texttt{[\"10 eggs\", \"20 gooses\", \"30 giants\"]}\\). You may assume that a space separates quantity and type, and that each item ends in s.\n\n\nAnswer\n\n\\(\\texttt{re.findall(r\"\\\\d+\\\\s\\\\w+\", sometext)}\\)\nThe strings we want to match begin with a number, followed by a space, followed by a word. We can use this to construct our pattern.\n\n\\(\\texttt{\\\\d+}\\) to match one or more digits\n\\(\\texttt{\\\\s}\\) to match a single space\n\\(\\texttt{\\\\w+}\\) to match one or more word characters \\(\\texttt{[A-Za-z0-9\\_]}\\)\n\n\n\n\n5.1.4 (d)\nFor each pattern specify the starting and ending position of the first match in the string. The index starts at zero and we are using closed intervals (both endpoints are included).\n\n\n\n\n\n\n\n\n\n\n\n\\(\\texttt{abcdefg}\\)\n\\(\\texttt{abcs!}\\)\n\\(\\texttt{ab abc}\\)\n\\(\\texttt{abc, 123}\\)\n\n\n\n\n\\(\\texttt{abc*}\\)\n[0,2]\n\n\n\n\n\n\\(\\texttt{[ \\^{} \\\\s]+}\\)\n\n\n\n\n\n\n\\(\\texttt{ab.*c}\\)\n\n\n\n\n\n\n\\(\\texttt{[a-z1,9]+}\\)\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\texttt{abcdefg}\\)\n\\(\\texttt{abcs!}\\)\n\\(\\texttt{ab abc}\\)\n\\(\\texttt{abc, 123}\\)\n\n\n\n\n\\(\\texttt{abc*}\\)\n[0,2]\n[0,2]\n[0,1]\n[0,2]\n\n\n\\(\\texttt{[ \\^{} \\\\s]+}\\)\n[0,6]\n[0,4]\n[0,1]\n[0,3]\n\n\n\\(\\texttt{ab.*c}\\)\n[0,2]\n[0,2]\n[0,5]\n[0,2]\n\n\n\\(\\texttt{[a-z1,9]+}\\)\n[0,6]\n[0,3]\n[0,1]\n[0,3]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regex, Visualization, and Transformation</span>"
    ]
  },
  {
    "objectID": "disc04/disc04.html#visualizing-bigfoot",
    "href": "disc04/disc04.html#visualizing-bigfoot",
    "title": "5  Regex, Visualization, and Transformation",
    "section": "5.2 Visualizing Bigfoot",
    "text": "5.2 Visualizing Bigfoot\nMany of you have probably heard of Bigfoot before. It’s a mysterious ape-like creature that is said to live in North American forests. Most doubt its existence, but a passionate few swear that Bigfoot is real. In this discussion, you will be working with a dataset on Bigfoot sightings, visualizing variable distributions and combinations thereof to better understand how/when/where Bigfoot is reportedly spotted, and possibly either confirm or cast doubt on its existence. The Bigfoot data contains a ton of variables about each reported Bigfoot spotting, including location information, weather, and moon phase.\n\n\nCode\n# Importing packages\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n\nCode\n# Loading bigfoot data\n\nurl = 'https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-09-13/bigfoot.csv'\ndf = pd.read_csv(url)\n\n\nThis dataset is extremely messy, with observations missing many values across multiple columns. This is normally the case with data based on citizen reports (many do not fill out all required fields). For the purposes of this discussion, we will drop all observations with any missing values and some unneeded columns. However, note this is not a good practice and you should almost never do this in real life!\n\n\nCode\n# Drop unneeded rows and observations with missing values\n\nbigfoot = df.dropna().rename({'temperature_high':'temp_high' ,'temperature_low':'temp_low'},axis = 1)\nbigfoot = bigfoot.drop(['observed', 'location_details', 'county', 'state', 'title',\n       'latitude', 'longitude', 'number', 'classification', 'geohash',\n       'temperature_mid', 'dew_point','precip_probability', 'precip_type','summary', \n       'wind_bearing'], axis = 1)\n\n\nHere are the first few entries of the bigfoot table:\n\n\nCode\nbigfoot.head(5)\n\n\n\n\n\n\n\n\n\nseason\ndate\ntemp_high\ntemp_low\nhumidity\ncloud_cover\nmoon_phase\nprecip_intensity\npressure\nuv_index\nvisibility\nwind_speed\n\n\n\n\n10\nSummer\n2016-06-07\n74.69\n53.80\n0.79\n0.61\n0.10\n0.0010\n998.87\n6.0\n9.70\n0.49\n\n\n21\nSummer\n2015-10-02\n49.06\n44.24\n0.87\n0.93\n0.67\n0.0092\n1022.92\n3.0\n9.16\n2.87\n\n\n32\nFall\n2009-10-31\n69.01\n34.42\n0.77\n0.81\n0.42\n0.0158\n1011.48\n3.0\n1.97\n3.94\n\n\n34\nSummer\n1978-07-15\n68.56\n63.05\n0.88\n0.80\n0.33\n0.0285\n1014.70\n5.0\n5.71\n5.47\n\n\n55\nSummer\n2015-11-26\n20.49\n5.35\n0.65\n0.08\n0.54\n0.0002\n1037.98\n1.0\n10.00\n0.40\n\n\n\n\n\n\n\nLet’s first look at distributions of individual quantitative variables. Let’s say we’re interested in wind_speed.\n\n5.2.1 (a)\nWhich of the following are appropriate visualizations for plotting the distribution of a quantitative variable? (Select all that apply.)\nA. Pie charts\nB. Kernel Density Plot\nC. Scatter plot\nD. Box plot\nE. Histogram\nF. Hex plot\n\n\n\nAnswer\n\nKernel Density Plot, Box plot, Histogram\nA Pie chart would not be appropriate because they are used to visualize the distribution of categories, or of a single qualitative variables. Scatter plots and Hex plots are also not appropriate as they visualize the relationship between two quantitative variables.\n\n\n5.2.2 (b)\nWrite a line of code that produces a visualization that depicts the variable’s distribution (example shown below).\n\n\n\nwindspeed\n\n\n\n\nCode\nsns.histplot(data = bigfoot, x = \"wind_speed\", kde = True);\n\n\n\n\n\n\n\n\n\nNote that kde = True is required to overlay the KDE curve over the actual histogram!\n\n\n5.2.3 (c)\nNow, let’s look at some qualitative variables. Write a line of code that produces a visualization that shows the distribution of bigfoot sightings across the variable season (example shown below).\nHint: Use seaborn’s sns.countplot or matplotlib’s plt.bar.\n\n\n\nCode\nsns.countplot(data = bigfoot, x = \"season\");\n\n\n\n\n\n\n\n\n\nIn order to also replicate the colors of the bars, you would need to manually specify them.\n\n\nCode\nsns.countplot(data = bigfoot, x = \"season\", palette = [\"blue\", \"orange\", \"green\", \"red\", \"purple\"]);\n\n\n\n\n\n\n\n\n\nYou could have alternatively used Matplotlib!\n\n\nCode\nseason_counts = bigfoot[\"season\"].value_counts()\nplt.bar(season_counts.index, season_counts.values, color = [\"blue\", \"orange\", \"green\", \"red\", \"purple\"]);\n\n\n\n\n\n\n\n\n\n\n\n5.2.4 (d)\nFinally, produce a single visualization that showcases how the prevalence of bigfoot sightings at particular combinations of moon_phase and wind_speed vary across each season.\nHint: Think about color as the third information channel in the plot.\n\n\n\nCode\nsns.scatterplot(data = bigfoot,\n                x = \"moon_phase\",\n                y = \"wind_speed\",\n                hue = \"season\",\n                alpha = 0.2);\n\n\n\n\n\n\n\n\n\nNote the two rather unfamiliar arguments:\n\nhue specifies which column of bigfoot we want to color the points according to\nalpha specifies how transparent the points should be. Higher values of alpha lead to more opaque points.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regex, Visualization, and Transformation</span>"
    ]
  },
  {
    "objectID": "disc04/disc04.html#kernel-density-estimation-kde",
    "href": "disc04/disc04.html#kernel-density-estimation-kde",
    "title": "5  Regex, Visualization, and Transformation",
    "section": "5.3 Kernel Density Estimation (KDE)",
    "text": "5.3 Kernel Density Estimation (KDE)\nKernel Density Estimation is used to estimate a probability density function (or density curve) from a set of data. A kernel with a bandwidth parameter α is placed on data observations \\(x_i\\) with \\(i ∈ \\{1, ..., n\\}\\), and the density estimation is calculated by averaging all kernels. Below, Gaussian and Boxcar kernel equations are listed:\n\nGaussian Kernel: \\(K_{\\alpha}(x, x_i) = \\frac{1}{\\sqrt{2\\pi\\alpha^2}}\\exp({-\\frac{(x-x_i)^2}{2\\alpha^2}})\\)\nBoxcar Kernel: \\(B_{\\alpha}(x, x_i) = \\begin{cases}\n    \\frac{1}{\\alpha} & \\text{ if }-\\frac{\\alpha}{2} \\leq x - x_i \\leq \\frac{\\alpha}{2} \\\\\n    0 & \\text{ else}\n\\end{cases}\\)\n\nThe KDE is calculated as follows: \\(f_\\alpha(x) = \\frac{1}{n}\\sum_{i = 1}^{n} K_\\alpha(x, x_i)\\).\n\n5.3.1 (a)\nDraw a KDE plot (by hand is fine) for data points [1, 4, 8, 9] using Gaussian Kernel and \\(\\alpha = 1\\). On the plot show \\(x\\), \\(x_i\\), \\(\\alpha\\), and the KDE.\n\n\nAnswer\n\nWith \\(\\alpha = 1\\), we get a Gaussian Kernel of \\(K_{1}(x, x_i) = \\frac{1}{\\sqrt{2 \\pi}} \\exp\\left(-\\frac{(x - x_i)^2}{2} \\right)\\).\nThis kernel is greatest when \\(x = x_i\\), giving us maximum point at \\[K_{1}(x, x) = \\frac{1}{\\sqrt{2 \\pi}} = 0.3989 \\approx 0.4\\]\nEach individual kernel is a Gaussian centered, respectively, at . Since we have 4 kernels, each with an area of 1, we normalize by dividing each kernel by 4. This gives us a maximum height of \\(0.1\\). We then sum those kernels together to obtain the final KDE plot:\n\n\n\nKDE\n\n\n\n\n\n5.3.2 (b)\nWe wish to compare the results of KDE using a Gaussian kernel and a boxcar kernel. For \\(\\alpha &gt; 0\\), which of the following statements is true? Choose all that apply.\nA. Decreasing \\(\\alpha\\) for a Gaussian kernel decreases the smoothness of the KDE.\nB. The Gaussian kernel is always better than the boxcar kernel for KDEs.\nC. Because the Gaussian kernel is smooth, we can safely use large \\(\\alpha\\) values for kernel density estimation without worrying about the actual distribution of data.\nD. The area under the boxcar kernel is 1, regardless of the value of \\(\\alpha\\).\nE. None of the above.\n\n\nAnswer\n\nCorrect options: A, D\nB is false because a boxcar kernel can perform better is \\(\\alpha\\) is not chosen properly for the Gaussian kernel\nC is false because an \\(\\alpha\\) that is too high risks including too many points in the estimate, resulting in a flatter curve.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regex, Visualization, and Transformation</span>"
    ]
  },
  {
    "objectID": "disc05/disc05.html",
    "href": "disc05/disc05.html",
    "title": "6  Transformations, Sampling, and SLR",
    "section": "",
    "text": "6.1 Logarithmic Transformations\nIshani is a development economist interested in studying the relationship between literacy rates and gross national income in countries across the world. Originally, she plotted the data on a linear (absolute) scale, shown on the left. She noticed that the non-linear relationship between the variables with a lot of points clustered towards the larger values of literacy rate, so she consults the Tukey-Mosteller Bulge diagram and decides to do a \\(\\log_{10}\\) transformation of the \\(y\\)-axis, shown on the right. The solid blue line is a “line of best fit” (we’ll formalize this later in the course).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Transformations, Sampling, and SLR</span>"
    ]
  },
  {
    "objectID": "disc05/disc05.html#logarithmic-transformations",
    "href": "disc05/disc05.html#logarithmic-transformations",
    "title": "6  Transformations, Sampling, and SLR",
    "section": "",
    "text": "log transform plot\n\n\n\n6.1.1 (a)\nInstead of using the \\(\\log_{10}\\) transformation of the \\(y\\)-axis, what other transformations could Ishani have used to attempt to linearize the relationship between literacy rate (\\(x\\)) and gross national income per capita (\\(y\\)). Select all that apply.\nA. \\(\\log_e(y)\\)\nB. \\(10^y\\)\nC. \\(\\sqrt{x}\\)\nD. \\(x^2\\)\nE. \\(y^2\\)\n\n\nAnswer\n\n\n\\(\\log_e(y)\\), \\(x^2\\)\n\nThe original plot displays a very strong non-linear relationship that looks exponential. For large values of \\(x\\), and increase in \\(x\\) is matched with a very significant increase in \\(y\\). Therefore, we would want to apply a transformation that makes \\(x\\) values larger or \\(y\\) values smaller.\n\\(x^3\\), \\(\\sqrt{y}\\), and \\(\\log(y)\\) are also valid as suggested by the Tukey-Mosteller Bulge Diagram.\n\n\n\n6.1.2 (b)\nLet \\(C\\) and \\(k\\) be some constant values and \\(x\\) and \\(y\\) represent literacy rate and gross national income per capita, respectively. Based on the plots, which of the following best describes the pattern seen in the data?\nA. \\(y = C + kx\\)\nB. \\(y = C*10^{kx}\\)\nC. \\(y = C + k\\log_{10}(x)\\)\nD. \\(y = Cx^k\\)\n\n\nAnswer\n\n\n\\(y = C*10^{kx}\\)\n\nThe basic format of a regression line is \\(y = kx + b\\). Noticed that the plot on the right applied a log transformation to the \\(y\\) axis, so we can apply that same transformation to the equation to derive the true relationship.\n\\[\\begin{align}\n\\log_{10}(y) &= kx + b\\\\\ny &= 10^{kx + b}\\\\\ny &= 10^b*10^{kx}\n\\end{align}\\]\nwhere \\(C = 10^b\\)\n\n\n\n6.1.3 (c)\nWhat parts of the plot could you use to make initial guesses on \\(C\\) and \\(k\\)?\n\n\nAnswer\n\n\n\\(C\\): \\(b\\) is the y-intercept of the transformed plot, and \\(C = 10^b\\)\n\\(k\\): Slope of the regression line in the transformed plot\n\n\n\n\n6.1.4 (d)\nIshani’s friend, Yash, points to the solid line on the transformed plot and says “since this line is going up and to the right, we can say that, in general, the higher the literacy rate, the greater the gross national income per capita”. Is this a reasonable interpretation of the plot?\n\n\nAnswer\n\nYes, the observation is equivalent to saying that the slope is positive, which means increases in \\(x\\) correspond to increases in \\(y\\). This does not mean higher literacy rates cause higher gross national incomes, just that they are positively correlated.\n\n\n\n6.1.5 (e)\nSuppose that instead of plotting positive quantities, our data contained some zero and negative values. How can we reasonably apply a logarithmic transform to this data?\n\n\nAnswer\n\nRecall that logarithms are not defined for 0 or negative values. Thus, we must first make all of our values positive. Suppose our data consists of three points, \\([-3, -2, 4]\\).\n\nAdd the magnitude of the smallest number to each value to make all values non-negative: \\([0, 1, 7]\\)\nAdd a small positive number to each value (e.g., \\(1\\)) to make all values positive: \\([1, 2, 8]\\)\n\nNow, it is possible to take the logarithm of each value! Note that the steps above are merely shifting the data, they are not changing any underlying linear or non-linear relationships.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Transformations, Sampling, and SLR</span>"
    ]
  },
  {
    "objectID": "disc05/disc05.html#data-collection-through-sampling",
    "href": "disc05/disc05.html#data-collection-through-sampling",
    "title": "6  Transformations, Sampling, and SLR",
    "section": "6.2 Data Collection through Sampling",
    "text": "6.2 Data Collection through Sampling\nIt’s time for the Data 100 midterm, and the professors want to estimate the difficulty of the exam. They decided to survey students on the exam’s difficulty with a 10-point scale and then use the mean of the student’s responses as the estimate.\n\n6.2.1 (a)\nWhat is the population the professors are interested in trying to understand?\nA. Students in Data 100\nB. Students enrolled in the Data 100 Ed\nC. Students who attend the Data 100 lectures\nD. Students who took the Data 100 midterm\n\n\nAnswer\n\nD. Students who took the Data 100 midterm\nThe professors are only interested in the students who actually took the midterm. Some students in the first three options might not have taken the midterm exam!\n\n\n\n6.2.2 (b)\nThe professors consider a few different methods for collecting the survey data about the midterm. Which of the following methods is best? (think through which considerations go into “best”)\nA. The professors add one Slido poll in the first lecture following the exam and only consider synchronous responses.\nB. The professors add a question to the homework assignments of a simple random sample of students within every discussion section.\nC. The professors make a post on Ed asking students to submit a Google Form containing the survey question.\nD. The professors choose a simple random sample of discussion sections, go to each selected section and ask each student in the group as part of the final discussion question.\n\n\nAnswer\n\nB. The professors add a question to the homework assignments of a simple random sample of students within every discussion section.\nAlthough not perfect, B describes the best method out of the four! This method samples randomly from uniformly from students across discussion times, as well as providing strong incentive for students to answer. The only issue would be the fact that the sampling frame does not include students who don’t have a discussion section.\nHere are reasons why the other options are not as good:\n\nA: Sample only from students who attend synchronously, introducing selection bias.\nC: Not all students check Ed periodically and the survey is optional, introducing selection bias and non-response bias.\nD: Room for social pressure, which can bias the results of the survey. Also, there may be systematic or inherent differences between discussion sections.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Transformations, Sampling, and SLR</span>"
    ]
  },
  {
    "objectID": "disc05/disc05.html#simple-linear-regression",
    "href": "disc05/disc05.html#simple-linear-regression",
    "title": "6  Transformations, Sampling, and SLR",
    "section": "6.3 Simple Linear Regression",
    "text": "6.3 Simple Linear Regression\nLillian and Prabhleen were watching their favorite chemistry Youtuber NileRed experimenting with turning gloves into grape soda and wanted to try it themselves. The experiment was done at various temperatures and yielded various amounts of grape soda. Since this reaction is very costly, they were only able to do it 10 times. This data set of size \\(n = 10\\) (Yield data) contains measurements of yield from an experiment done at five different temperature levels. The variables are \\(y\\) = yield in liters and \\(x\\) = temperature in degrees Fahrenheit. Below is a scatter plot of our data. \n\n\n\n\\(\\sigma_x\\)\n\\(\\sigma_y\\)\n\\(r\\)\n\\(\\bar{x}\\)\n\\(\\bar{y}\\)\n\n\n\n\n15\n0.3\n0.50\n75.00\n3\n\n\n\n\n6.3.1 (a)\nGiven the above statistics, calculate the slope (\\(\\hat{\\theta_1}\\)) and y-intercept (\\(\\hat{\\theta_0}\\)) of the line of best fit using Mean Squared Error (MSE) as our loss function and plot the line on the graph above:\n\\[\\begin{align*}\ny = \\hat{\\theta_0} + \\hat{\\theta_1}x\n\\end{align*}\\]\n\n\nAnswer\n\n\\[\\begin{align*}\n\\hat{\\theta_1} &= r*\\frac{\\sigma_y}{\\sigma_x}\\\\\n\\hat{\\theta_1} &= 0.5*\\frac{0.3}{15}\\\\\n&= 0.01\\\\\\\\\n\\hat{\\theta_0} &= \\bar{y} - \\hat{\\theta_1}\\bar{x}\\\\\n\\hat{\\theta_0} &= 3 - 75.00 * 0.01\\\\\n&= 3 - 0.75\\\\\n&= 2.25\n\\end{align*}\\]\n\n\n\nYield Temp Regression Line\n\n\nNote that the \\(\\sigma_x\\) and \\(\\sigma_y\\) values in the table are slightly different from what can be seen in the plot in order to make the calculations easier! As a result, the slope and intercept values you obtained may also be slightly different from what you see in the plot.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Transformations, Sampling, and SLR</span>"
    ]
  },
  {
    "objectID": "disc06/disc06.html",
    "href": "disc06/disc06.html",
    "title": "7  Models, OLS",
    "section": "",
    "text": "7.1 Driving with a Constant Model\nLillian is trying to use modeling to drive her car autonomously. To do this, she collects a lot of data from driving around her neighborhood and stores it in drive. She wants your help to design a model that can drive on her behalf in the future using the outputs of the models you design. First, she wants to tackle two aspects of this autonomous car modeling framework: going forward and turning. Some statistics from the collected dataset are shown below using drive.describe(), which returns the mean, standard deviation, quartiles, minimum, and maximum for the two columns in the dataset: target_speed and degree_turn.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Models, OLS</span>"
    ]
  },
  {
    "objectID": "disc06/disc06.html#driving-with-a-constant-model",
    "href": "disc06/disc06.html#driving-with-a-constant-model",
    "title": "7  Models, OLS",
    "section": "",
    "text": "target_speed\ndegree_turn\n\n\n\n\ncount\n500.000000\n500.000000\n\n\nmean\n32.923408\n143.721153\n\n\nstd\n46.678744\n153.641504\n\n\nmin\n0.231601\n0.000000\n\n\n25%\n12.350025\n6.916210\n\n\n50%\n25.820689\n45.490086\n\n\n75%\n39.788716\n323.197168\n\n\nmax\n379.919965\n359.430309\n\n\n\n\n7.1.1 (a)\nSuppose the first part of the model predicts the target speed of the car. Using constant models trained on the speeds of the collected data shown above with \\(L_1\\) and \\(L_2\\) loss functions, which of the following is true?\nA. The model trained with the \\(L_1\\) loss will always drive slower than the model trained with \\(L_2\\) loss.\nB. The model trained with the \\(L_2\\) loss will always drive slower than the model trained with \\(L_1\\) loss.\nC. The mode trained with the \\(L_1\\) loss will sometimes drive slower than the model trained with \\(L_2\\) loss.\nD. The model trained with the \\(L_2\\) loss will somtimes drive slower than the model trained with \\(L_1\\) loss.\n\n\nAnswer\n\nA. The model trained with the \\(L_1\\) loss will always drive slower than the model trained with \\(L_2\\) loss.\nRemember that when we have a constant model, using \\(L_1\\) will always predict the median while \\(L_2\\) loss will always predict the mean. Since the median (\\(25.82\\)) is lower than the mean (\\(32.92\\)), we know that the model trained on \\(L_1\\) loss will always drive slower.\n\n\n\n7.1.2 (b)\nFinding that the model trained with the \\(L_2\\) loss drives too slowly, Lillian changes the loss function for the constant model where the loss is penalized more if the true speed is higher. That way, in order to minimize loss, the model would have to output predictions closer to the true value, particularly as speeds get faster, the end result being a higher constant speed. Lillian writes this as \\(L(y, \\hat{y}) = y(y-\\hat{y})^2\\).\nFind the optimal \\(\\hat{\\theta}_0\\) for the constant model using the new empirical risk function \\(R(\\theta_0)\\) below:\n\\[\\begin{align*}\nR(\\theta_0) = \\frac{1}{n}\\sum_i y_i(y_i-\\theta_0)^2\n\\end{align*}\\]\n\n\nAnswer\n\nTake the derivative: \\[\\begin{align}\n\\frac{dR}{d\\theta_0} &= \\frac{1}{n} \\sum_i \\frac{d}{d\\theta_0}y_i (y_i - \\theta_0)^2 \\\\\n&= \\frac{1}{n} \\sum_i -2y_i (y_i - \\theta_0) = -\\frac{2}{n} \\sum_i y_i^2 - y_i\\theta_0\n\\end{align}\\] Set the derivative to 0: \\[\\begin{align}\n-\\frac{2}{n} \\sum_i y_i^2 - y_i\\theta_0 = 0 \\\\\n\\theta_0 \\sum_i y_i = \\sum_i y_i^2\\\\\n\\theta_0 = \\frac{\\sum_i y_i^2}{ \\sum_i y_i }\n\\end{align}\\]\nNote that the empirical risk function is convex, which you can show by computing the second derivative of \\(R(\\theta_0)\\) and noting that it is positive for all values of \\(\\theta_0\\).\n\\[\\frac{d^2R}{d\\theta_0^2} = \\frac{2}{n} \\sum_i y_i &gt; 0\\] since \\(y_i\\) represents speed, also validated by the fact that the minimum value is positive.\nTherefore, any critical point must be the global minimum and so the optimal value we found minimizes empirical risk.\n\n\n\n7.1.3 (c)\nLillian’s friend, Yash, also begins working on a model that predicts the degree of turning at a particular time between 0 and 359 degrees using the data in the degree_turn column. Explain why a constant model is likely inappropriate in this use case.\nExtra: If you’ve studied some physics, you may recognize the behavior of our constant model!\n\n\nAnswer\n\nAny constant model will essentially be always turning at an angle and will be unable to turn either direction or go straight (i.e., it’ll essentially go in a circle forever).\n\n\n\n7.1.4 (d)\nSuppose we finally expand our modeling framework to use simple linear regression (i.e., \\(f_\\theta(x) = \\theta_{w,0}+\\theta_{w,1}x\\)) For our first simple linear regression model, we predict the turn angle (\\(y\\)) using target speed (\\(x\\)). Our optimal parameters are: \\(\\hat{\\theta}_{w, 1}=0.019\\) and \\(\\hat{\\theta}_{w,0}=143.1\\)\nHowever, we realize that we actually want a model that predicts target speed (our new \\(y\\)) using turn angle, our new \\(x\\) (instead of the other way around)! What are our new optimal parameters for this new model?\n\n\nAnswer\n\nTo predict target speed (new \\(y\\)) from turn angle (new \\(x\\)), we need to compute \\(\\hat{\\theta}_1=r*\\frac{\\sigma_{\\text{speed}}}{\\sigma_{\\text{turn}}}\\)\nWhen we predicted the turn angle from target speed, we computed \\(\\hat{\\theta}_{w,1}=r*\\frac{\\sigma_{\\text{turn}}}{\\sigma_{\\text{speed}}} = 0.019\\)\nTherefore, \\(\\hat{\\theta}_1 = r * \\frac{\\sigma_{\\text{speed}}}{\\sigma_{\\text{turn}}} = (r*\\frac{\\sigma_{\\text{turn}}}{\\sigma_{\\text{speed}}})\\frac{\\sigma_{\\text{speed}}^2}{\\sigma_{\\text{turn}}^2} = 0.019 * \\frac{46.678744^2}{153.641504^2} \\approx 0.00175\\)\nThen, \\(\\hat{\\theta}_0 = \\bar{y} - \\hat{\\theta}_1\\bar{x} = 32.92 - 0.00175*143.72 = 32.67\\)\nNote that \\(\\bar{y}\\) and \\(\\bar{x}\\) are the means of target speed and turn angle respectively.\n\n\n\n\n\n\nNote\n\n\n\nWe can’t use the inverse function \\(f_\\theta^{-1}(x)\\) since minimizing the sum of squared vertical residuals is not the inverse problem of minimizing the sum of squared horizontal residuals.\n\nThe slope of the line that minimizes \\(MSE(y, \\hat{y})\\) is \\(r*\\frac{\\sigma_y}{\\sigma_x}\\)\nThe slope of the line that minimizes \\(MSE(x, \\hat{x})\\) is \\(r*\\frac{\\sigma_x}{\\sigma_y}\\)\n\nThey are not inverses! The visualization below shows the difference between minimizing horizontal and vertical residuals.\n\n\n\nHorizontal and Vertical Residuals",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Models, OLS</span>"
    ]
  },
  {
    "objectID": "disc06/disc06.html#geometry-of-least-squares",
    "href": "disc06/disc06.html#geometry-of-least-squares",
    "title": "7  Models, OLS",
    "section": "7.2 Geometry of Least Squares",
    "text": "7.2 Geometry of Least Squares\nSuppose we have a dataset represented with the design matrix span(\\(\\mathbb{X}\\)) and response vector \\(\\mathbb{Y}\\). We use linear regression to solve for this and obtain optimal weights as \\(\\hat{\\theta}\\). Label the following terms on the geometric interpretation of ordinary least squares:\n\n\\(\\mathbb{X}\\) (i.e., span(\\(\\mathbb{X}\\)))\nThe response vector \\(\\mathbb{Y}\\)\nThe residual vector $ - \\(\\mathbb{X}\\hat{\\theta}\\)\nThe prediction vector \\(\\mathbb{X}\\hat{\\theta}\\) (using optimal parameters)\nA prediction vector \\(\\mathbb{X}\\alpha\\) (using an arbitrary vector \\(\\alpha\\))\n\n\n\n\nGeometric Empty\n\n\n\n\nAnswer\n\n\n\n\nGeometric Answer\n\n\nA couple things to note:\n\nThe rectangle represents span(\\(\\mathbb{X}\\)), or everywhere that you’re able to get to with a linear combination of the columns of \\(\\mathbb{X}\\). Therefore, \\(\\mathbb{X}\\hat{\\alpha}\\) and \\(\\mathbb{X}\\hat{\\theta}\\) must be the arrows in the rectangle.\nWe know that the residual vector \\(\\mathbb{Y} - \\mathbb{X}\\hat{\\theta}\\) is orthogonal to span(\\(\\mathbb{X}\\))\n\\(\\mathbb{X}\\hat{\\theta}\\) is the best/closest to \\(\\mathbb{Y}\\) in span(\\(\\mathbb{X}\\)), so we know that it is the one that looks like it is the shadow of the bolded arrow representing \\(\\mathbb{Y}\\)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Models, OLS</span>"
    ]
  },
  {
    "objectID": "disc06/disc06.html#multiple-choice-questions",
    "href": "disc06/disc06.html#multiple-choice-questions",
    "title": "7  Models, OLS",
    "section": "7.3 Multiple Choice Questions",
    "text": "7.3 Multiple Choice Questions\nUsing the geometry of least squares, let’s answer a few questions about Ordinary Least Squares (OLS)!\n\n7.3.1 (a)\nWhich of the following are true about the optimal solution \\(\\hat{\\theta}\\) to OLS? Recall that the least squares estimate \\(\\hat{\\theta}\\) solves the normal equation \\((\\mathbb{X}^T\\mathbb{X})\\theta = \\mathbb{X}^T\\mathbb{Y}\\)\n\\[\\begin{align*}\n\\hat{\\theta} = (\\mathbb{X}^T\\mathbb{X})^{-1}\\mathbb{X}^T\\mathbb{Y}\n\\end{align*}\\]\n\\(\\Box\\) A. Using the normal equation, we can derive an optimal solution for simple linear regression with an \\(L_2\\) loss.\n\\(\\Box\\) B. Using the normal equation, we can derive an optimal solution for simple linear regression with an \\(L_1\\) loss.\n\\(\\Box\\) C. Using the normal equation, we can derive an optimal solution for a constant model with an \\(L_2\\) loss.\n\\(\\Box\\) D. Using the normal equation, we can derive an optimal solution for a constant model with an \\(L_1\\) loss.\n\\(\\Box\\) E. Using the normal equation, we can derive an optimal solution for the model \\(\\hat{y} = \\theta_1x + \\theta_2sin(x^2)\\)\n\\(\\Box\\) F. Using the normal equation, we can derive an optimal solution for the model \\(\\hat{y} = \\theta_1\\theta_2 + \\theta_2x^2\\)\n\n\nAnswer\n\nWe can derive solutions to both simple linear regression and constant model with an \\(L_2\\) loss since they can be represented in the form \\(y = x^T\\theta\\) in some way. Specifically, one of the two entries of \\(x\\) would be \\(1\\) for SLR (and the other would be the explanatory variable). The only entry for the constant model would be \\(1\\).\nWe cannot derive solutions for anything with the \\(L_1\\) loss since the normal equation optimizes for MSE.\nSince option E is linear with respect to \\(\\theta\\), we can use the normal equation. A good rule of thumb to determine if an equation is linear in \\(\\theta\\) is checking if an expression can be separated into a matrix product of two terms:\n\\[\\begin{align*}\n\\hat{y} = \\begin{bmatrix}x & sin(x^2)\\end{bmatrix}\\begin{bmatrix}\\theta_1 \\\\ \\theta_2\\end{bmatrix}\n\\end{align*}\\]\nConversely, option F is not linear in \\(\\theta\\) so we cannot use the normal equations.\n\n\n\n7.3.2 (b)\nWhich of the following conditions are required for the least squares estimate in the previous subpart?\n\\(\\Box\\) A. \\(\\mathbb{X}\\) must be full column rank\n\\(\\Box\\) B. \\(\\mathbb{Y}\\) must be full column rank\n\\(\\Box\\) C. \\(\\mathbb{X}\\) must be invertible\n\\(\\Box\\) D. \\(\\mathbb{X}^T\\) must be invertible\n\n\nAnswer\n\nA. \\(\\mathbb{X}\\) must be full column rank\n\\(\\mathbb{X}\\) must be full column rank in order for the normal equation to have a unique solution. Otherwise, \\(\\mathbb{X}^T\\mathbb{X}\\) will not be invertible, and there will be infinite least squares estimates. Noth that invertibility is required of \\(\\mathbb{X}^T\\mathbb{X}\\): neither \\(\\mathbb{X}\\) nor \\(\\mathbb{X}^T\\) need to be invertible. \\(\\mathbb{X}\\) and \\(\\mathbb{X}^T\\) don’t even need to be square matrices! \\(\\mathbb{Y}\\) is a vector so the matrix notion of “full column rank” does not really apply here (a vector will always span one dimension unless it is a vector of zeros).\n\n\n\n7.3.3 (c)\nWhat is always true about the residuals in the least squares regression? Select all that apply.\n\\(\\Box\\) A. They are orthogonal to the column space of the design matrix.\n\\(\\Box\\) B. They represent the errors of the predictions.\n\\(\\Box\\) C. Their sum is equal to the mean squared error.\n\\(\\Box\\) D. Their sum is equal to zero.\n\\(\\Box\\) E. None of the above.\n\n\nAnswer\n\nA, B\n(C) is wrong because the mean squared error is the mean of the sum of the squares of the residuals.\n(D) A counter-example is:\n\\[\\begin{align*}\n\\mathbb{X} = \\begin{bmatrix}2 & 3 \\\\ 1 & 5 \\\\ 2 & 4 \\end{bmatrix}, \\mathbb{Y} = \\begin{bmatrix} 1 \\\\ 3 \\\\ 2 \\end{bmatrix}\n\\end{align*}\\]\nAfter solving the least squares problem, the sum of the residuals is \\(-0.0247\\), which is not equal to zero. However, note that this statement is, in general, true if the design matrix \\(\\mathbb{X}\\) contains a column for the intercept!\n\n\n\n7.3.4 (d)\nWhich of the following are true about the predictions made by OLS? Select all that apply.\n\\(\\Box\\) A. They are projections of the observations onto the column space of the design matrix.\n\\(\\Box\\) B. They are linear combinations of the features.\n\\(\\Box\\) C. They are orthogonal to the residuals.\n\\(\\Box\\) D. They are orthogonal to the column space of the features.\n\\(\\Box\\) E. None of the above.\n\n\nAnswer\n\n(A), (B), (C)\n(A) is correct because the predictions made by OLS is the vector in the column space of \\(\\mathbb{X}\\) closest to the response vector \\(\\mathbb{Y}\\), which is the orthogonal projection of \\(\\mathbb{Y}\\) onto \\(\\mathbb{X}\\).\n(B) is correct because the predictions \\(\\mathbb{X}\\hat{\\theta}\\) is a linear combination of the columns of \\(\\mathbb{X}\\) (the features).\n(C) is correct because we know that the predictions \\(\\mathbb{X}\\hat{\\theta}\\) is the vector in span(\\(\\mathbb{X}\\)) closest to \\(\\mathbb{Y}\\). Therefore, \\(\\mathbb{Y} - \\mathbb{X}\\hat{\\theta}\\) (the residuals) must be orthogonal to the predictions.\n(D) is not correct because the predictions are in the column space of the features.\n\n\n\n7.3.5 (e)\nWhich of the following is true of the mystery quantity \\(\\vec{v} = (I - \\mathbb{X}(\\mathbb{X}^T\\mathbb{X})^{-1}\\mathbb{X}^T)\\mathbb{Y}\\)?\n\\(\\Box\\) A. The vector \\(\\vec{v}\\) represents the residuals for any linear model.\n\\(\\Box\\) B. If the matrix \\(\\mathbb{X}\\) contains the \\(\\vec{1}\\) vector, then the sum of the elements in vector \\(\\vec{v}\\) is 0 (i.e., \\(\\sum_iv_i = 0\\))\n\\(\\Box\\) C. All the column vectors \\(x_i\\) of \\(\\mathbb{X}\\) are orthogonal to \\(\\vec{v}\\).\n\\(\\Box\\) D. If \\(\\mathbb{X}\\) is of shape \\(n\\) by \\(p\\), there are \\(p\\) elements in vector \\(\\vec{v}\\).\n\\(\\Box\\) E. For any \\(\\vec{\\alpha}\\), \\(\\mathbb{X}\\vec{\\alpha}\\) is orthogonal to \\(\\vec{v}\\).\n\n\nAnswer\n\n(B), (C), (E)\nThe trick is to recognize the following:\n\\[\\begin{align*}\n\\vec{v} &= (I - \\mathbb{X}(\\mathbb{X}^T\\mathbb{X})^{-1}\\mathbb{X}^T)\\mathbb{Y}\\\\\n&= \\mathbb{Y} - \\mathbb{X}(\\mathbb{X}^T\\mathbb{X})^{-1}\\mathbb{X}^T\\mathbb{Y}\\\\\n&= \\mathbb{Y} - \\mathbb{X}\\hat{\\theta}\n\\end{align*}\\]\nwhich is the residual vector for an OLS model.\n(A) is incorrect because any linear model does not create the residual vector \\(v\\); only the optimal linear model with weights \\(\\hat{\\theta}\\) does.\n(D) is incorrect because the vector \\(v\\) is of size \\(n\\) since there are \\(n\\) data points.\nThe rest are correct by properties of orthogonality as given by the geometry of least squares.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Models, OLS</span>"
    ]
  }
]