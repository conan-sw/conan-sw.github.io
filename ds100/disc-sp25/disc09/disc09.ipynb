{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Bias and Variance\"\n",
    "execute:\n",
    "  echo: true\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "    code-tools: true\n",
    "    toc: true\n",
    "    toc-title: \"Bias and Variance\"\n",
    "    page-layout: full\n",
    "    theme:\n",
    "      - cosmo\n",
    "      - cerulean\n",
    "    callout-icon: false\n",
    "jupyter:\n",
    "  jupytext:\n",
    "    text_representation:\n",
    "      extension: .qmd\n",
    "      format_name: quarto\n",
    "      format_version: '1.0'\n",
    "      jupytext_version: 1.16.1\n",
    "  kernelspec:\n",
    "    display_name: data100quarto\n",
    "    language: python\n",
    "    name: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Link to Slides](https://docs.google.com/presentation/d/1gUzOsBf1IXG5Bw0G7tZprEZxii_ceZKTGb-AOyGkgjQ/edit?usp=sharing)\n",
    "\n",
    "## Bias-Variance Tradeoff\n",
    "\n",
    "Your team would like to train a machine learning model to predict the next YouTube video a user will click on based on the videos the user has watched. We extract up to $d$ attributes (such as length of video, view count, etc.) from each video, and our model will be based on the previous $m$ videos watched by that user. Hence, the number of features for each data point for the model is $m \\times d$. Currently, you're not sure how many videos to consider.\n",
    "\n",
    "### (a)\n",
    "Your colleague, Lillian, generates the following plot, where the value $d$ on the $x$-axis denotes the number of features used for a particular model. However, she forgot to label the $y$-axis. Assume that the features are added to the model in decreasing levels of importance: More important features are first, and less important ones are after.\n",
    "\n",
    "<center>\n",
    "    <img src=\"images/bias_var_curve1.png\" width = \"500\">\n",
    "</center>\n",
    "\n",
    "\n",
    "Which of the following could the $y$-axis represent? Select all that apply.\n",
    "\n",
    "$\\Box$ **A**. Training Error\n",
    "\n",
    "$\\Box$ **B.** Validation error\n",
    "\n",
    "$\\Box$ **C.** Bias\n",
    "\n",
    "$\\Box$ **D.** Variance\n",
    "\n",
    "<details>\n",
    "<summary><b>Answer</b></summary>\n",
    "\n",
    "**Training Error, Validaiton Error, Bias**\n",
    "\n",
    "* Training Error: Can decrease as we add more features.\n",
    "* Validation Error: This can be true depending on the underlying complexity of the data. \n",
    "* Bias: Typically decreases with increasing model complexity.\n",
    "* Variance: Typically increases with increasing model complexity.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\n",
    "\n",
    "Lillian generates the following plot, where the value $d$ is on the $x$-axis. However, she forgot to label the $y$-axis again.\n",
    "\n",
    "<center>\n",
    "    <img src=\"images/bias_var_curve3.png\" width = \"500\">\n",
    "</center>\n",
    "\n",
    "Which of the following could the $y$-axis represent? Select all that apply.\n",
    "\n",
    "$\\Box$ **A**. Training Error\n",
    "\n",
    "$\\Box$ **B.** Validation error\n",
    "\n",
    "$\\Box$ **C.** Bias\n",
    "\n",
    "$\\Box$ **D.** Variance\n",
    "\n",
    "<details>\n",
    "<summary><b>Answer</b></summary>\n",
    "\n",
    "**Validation Error**\n",
    "\n",
    "* Training Error: Cannot increase when increasing model complexity. If a feature increases training error, its weight will be set to 0.\n",
    "* Validation Error: Typically decreases up to a certain point, and then starts to increase as model becomes very complex.\n",
    "* Bias: Doesn't increase with increasing model complexity.\n",
    "* Variance: Doesn't decrease with increasing model complexity.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\n",
    "\n",
    "Explain what happens to the error on the holdout set as we increase $d$. Why?\n",
    "\n",
    "<details>\n",
    "<summary><b>Answer</b></summary>\n",
    "\n",
    "*Note*: The holdout set refers to a test set, not a validation set\n",
    "\n",
    "For smaller $d$, we are underfitting the training data since the model is less complex. The number of features we have does not allow us to fully understand the pattern and so our holdout error is high. As we increase $d$, we have more features that allow us to pick up on some of the trends in the data and so the holdout error decreases a bit. However, there comes a point after which the features result in overfitting. Our model becomes more complex and does not seem to generalize as well to unseen data resulting in a high holdout error once again.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance Tradeoff\n",
    "\n",
    "We randomly sample $n$ data points, $(x_i, y_i)$, and use them to fit a model $f_{\\hat\\theta}(x)$ according to some procedure (e.g. OLS, Ridge, LASSO). Then, we sample a new data point (independent of our existing points) from the same underlying data distribution. Furthermore, assume that we have a function $g(x)$, which represents the ground truth and $\\epsilon$, which represents random noise. $\\epsilon$ is produced by some noise generation process such that $\\mathbb{E}\\left\\lbrack\\epsilon\\right\\rbrack = 0$ and $\\text{Var}(\\epsilon)=\\sigma^2$. Whenever we query $Y$ at a given $x$, we are given $Y = g(x) + \\epsilon$, a corrupted version of the real ground truth output. A new $\\epsilon$ is generated each time, independent of the last. We showed in the lecture that:\n",
    "\n",
    "$$\\underbrace{\\mathbb{E}[(Y - f_{\\hat\\theta}(x))^2}]_{} = \\underbrace{\\sigma^2}_{} + \\underbrace{(g(x) - \\mathbb{E}[f_{\\hat\\theta}(x)]})^2_{} + \\underbrace{\\mathbb{E}[(f_{\\hat\\theta}(x) - \\mathbb{E}[f_{\\hat\\theta}(x)]_{})^2}]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\n",
    "\n",
    "Label each of the terms above using the following word bank. Not all words will be used.\n",
    "\n",
    "* Observation variance\n",
    "* Model variance\n",
    "* (Observation Bias) $^2$\n",
    "* (Model Bias) $^2$\n",
    "* Model Risk\n",
    "* Empirical Mean Squared Error\n",
    "\n",
    "<details>\n",
    "<summary><b>Answer</b></summary>\n",
    "\n",
    "$$\\underbrace{\\mathbb{E}{[(Y - f_{\\hat\\theta}(x))^2}]}_{\\text{model risk}} \n",
    "   = \\underbrace{\\sigma^2}_{\\text{observation variance}} \n",
    "   + \\underbrace{(g(x) - \\mathbb{E}[f_{\\hat\\theta}(x)])^2}_{(\\text{model bias})^2} \n",
    "   + \\underbrace{\\mathbb{E}[{ (f_{\\hat\\theta}(x) - \\mathbb{E}[{f_{\\hat\\theta}(x)}]_{})^2}]}_{\\text{model variance}}$$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\n",
    "\n",
    "What quantities are random variables in the above equation? In our assumed data-generation process, where is the randomness in each variable coming from (i.e., which part of the assumed underlying model makes each random variable \"random\")?\n",
    "\n",
    "<details>\n",
    "<summary><b>Answer</b></summary>\n",
    "\n",
    "* $Y$ - this is the new observation at $x$. Its randomness comes from the random noise $\\epsilon$.\n",
    "* $f_{\\hat\\theta}$ - this is the model fitted from the data. Its randomness comes from sampling and the random noise $\\epsilon$.\n",
    "* $\\hat{\\theta}$ - these are the optimal theta parameters. Like the model itself, the randomness stems from the sampling and the random noise $\\epsilon$.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\n",
    "\n",
    "Calculate the value of $\\mathbb{E}[\\epsilon f_{\\hat\\theta}(x')]$, where $f_{\\hat{\\theta}}(x')$ is some predicted value of the response variable at some new fixed $x'$ using a model trained on a random sample, and $\\epsilon$ is the observation error for a new data point at this fixed value of $x'$.\n",
    "\n",
    "<details>\n",
    "<summary><b>Answer</b></summary>\n",
    "\n",
    "Note that $f_{\\hat{\\theta}}(x)$ is a random variable whose randomness comes from $\\hat{\\theta}$ being random, which is itself random because of randomness in the training data. The data-generating mechanism for our new data point at $x$ is independent of all the other data used to train our model by assumption, so we have that the sources of randomness for $\\epsilon$ and $f_{\\hat{\\theta}}(x)$ are independent. Since $\\epsilon$ and $\\hat\\theta$ are independent, $$\\mathbb{E}[\\epsilon f_{\\hat\\theta}(x)] = \\mathbb{E}[\\epsilon]\\mathbb{E}[f_{\\hat\\theta}(x)]$$\n",
    "\n",
    "And since $\\mathbb{E}[\\epsilon] = 0$ by the definition we have given. Therefore, $$\\mathbb{E}[\\epsilon f_{\\hat\\theta}(x)] = \\mathbb{E}[\\epsilon]\\mathbb{E}[f_{\\hat\\theta}(x)] = 0$$\n",
    "\n",
    "Note that $\\epsilon$ here refers to the noise associated with our new data point $x'$, which is independently drawn (as stated at the start of the question). This can seem a bit confusing since we've previously used $\\epsilon$ to refer to the noise associated with each of the data points, but it is used to simplify the notation. It might help to think of the noise associated with each of our points $x_i$ as being $\\epsilon_i$ (which are i.i.d.), with the noise associated with $x'$ as being $\\epsilon'$. It then becomes clearer why $\\epsilon'$ and $f_{\\hat\\theta}(x)$ are independent.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization and Bias-Variance Tradeoff\n",
    "\n",
    "We will use a simple constant model $f_\\theta(x) = \\theta$ to show the effects of regularization on bias and variance. For the sake of simplicity, we will assume that there is no noise or observational variance, so the ground truth output is equal to the observed outputs: $Y = g(x)$. \n",
    "\n",
    "### (a) \n",
    "\n",
    "Recall that the optimal solution for the constant model with an MSE loss and a dataset $\\mathcal{D}$ with $y_1, y_2, ..., y_n$ is the mean $\\bar{y}$. \n",
    "\n",
    "We use L2 regularization with a regularization penalty of $\\lambda > 0$ to train another constant model. Derive the optimal solution to this new constant model \\textbf{with L2 regularization} to minimize the objective function below.\n",
    "\n",
    "$$\n",
    "R(\\theta) = \\arg \\min_\\theta \\left[ \\left( \\frac{1}{n} \\sum_{i=1}^n (y_i - \\theta)^2 \\right)+ \\lambda \\theta^2 \\right]\n",
    "$$\n",
    "\n",
    "**Note:** As mentioned in the lecture, we do not impose a regularization penalty on the bias term and this problem only serves as a practice.\n",
    "\n",
    "<details>\n",
    "<summary><b>Answer</b></summary>\n",
    "\n",
    "We use calculus to derive this result below:\n",
    "\n",
    "$$\\frac{dR}{d\\theta} = \\left( \\frac{1}{n} \\sum_i \\frac{d}{d\\theta} (y_i - \\theta)^2 \\right) + \\lambda \\frac{d}{d\\theta}\\theta^2 = \\left( -\\frac{2}{n} \\sum_i (y_i - \\theta) \\right) + 2\\lambda \\theta$$\n",
    "\n",
    "Set the derivative to 0, and solve for the optimal $\\hat{\\theta}$.\n",
    "\n",
    "$$ \\left( -\\frac{2}{n}\\sum_i (y_i - \\hat{\\theta}) \\right) + 2\\lambda \\hat{\\theta} = 0 \\implies \\left( -\\frac{1}{n}\\sum_i y_i \\right) + \\hat{\\theta} + \\lambda \\hat{\\theta} = 0 $$\n",
    "\n",
    "The optimal $\\hat{\\theta}$ is:\n",
    "\n",
    "\n",
    "$$\\frac{1}{n}\\sum_i y_i = \\hat{\\theta} (1+\\lambda)$$\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\frac{1}{n(1 + \\lambda)} \\sum_i y_i = \\frac{1}{1 + \\lambda} \\bar{y}\n",
    "$$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\n",
    "\n",
    "Use the bias-variance decomposition to show that for a constant model **with L2 regularization**, its optimal expected loss on a sample test point $(x, Y)$ in terms of the training data $y$ is equal to the following.\n",
    "\n",
    "$$\\mathbb{E}_\\mathcal{D}[(Y - f_{\\hat\\theta}(x))^2] = (Y - \\frac{1}{1 + \\lambda} \\mathbb{E}_\\mathcal{D}[\\bar{y}])^2 + \\frac{1}{(1 + \\lambda)^2} \\text{Var}_\\mathcal{D}(\\bar{y})$$\n",
    "\n",
    "What expected loss do we obtain when $\\lambda=0$, and what does that mean in terms of our model?\n",
    "\n",
    "**Note:** The subscript next to the expectation and variance lets you know what is random inside the expectation (i.e., what is the expectation taken over?). In this case, we calculate the expectation and variance of $\\bar{y}$ across the dataset $\\mathcal{D}$.\n",
    "\n",
    "<details>\n",
    "<summary><b>Answer</b></summary>\n",
    "\n",
    "Starting from the bias-variance decomposition presented in the lecture:\n",
    "$$\\mathbb{E}[(Y - f_{\\hat\\theta}(x))^2] = \\sigma^2 + (g(x) - \\mathbb{E}[f_{\\hat\\theta}(x)])^2 + \\text{Var}(f_{\\hat\\theta}(x))$$\n",
    "\n",
    "Recall that we are not dealing with any noise ($\\epsilon \\sim \\mathbb{N}(0, 0)$), so we can eliminate the $\\sigma^2$ term. Then, $g(x) = Y$, so we can replace $g(x)$ with $Y$.\n",
    "\n",
    "$$\\mathbb{E}[(Y - f_{\\hat\\theta}(x))^2] = (Y - \\mathbb{E}[f_{\\hat\\theta}(x)])^2 + \\text{Var}(f_{\\hat\\theta}(x))$$\n",
    "\n",
    "We can then derive $f_{\\hat\\theta}(x)$ using the assumptions of our constant model. We know that $f_{\\hat\\theta}(x) = \\frac{1}{1 + \\lambda}\\bar{y}$. We simplify the following expressions:\n",
    "\n",
    "$$\\mathbb{E}[(Y - f_{\\hat\\theta}(x))^2] = (Y - \\mathbb{E}[\\frac{1}{1 + \\lambda}\\bar{y}])^2 + \\text{Var}(\\frac{1}{1 + \\lambda}\\bar{y})$$\n",
    "\n",
    "We can simplify:\n",
    "\n",
    "$$\\mathbb{E}[(Y - f_{\\hat\\theta}(x))^2] = (Y - \\frac{1}{1 + \\lambda} \\mathbb{E}[\\bar{y}])^2 + \\frac{1}{(1 + \\lambda)^2} \\text{Var}(\\bar{y})$$\n",
    "\n",
    "Setting $\\lambda=0$ is equivalent to using a constant model **without** L2 regularization, and we get the following equation for the expected loss:\n",
    "\n",
    "$$\\mathbb{E}[(Y - f_{\\hat\\theta}(x))^2] = (Y - \\mathbb{E}[\\bar{y}])^2 + \\text{Var}(\\bar{y})$$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\n",
    "\n",
    "Remark on how regularization has affected the model bias and variance as $\\lambda$ increases. Consider what would happen to these quantities as $\\lambda \\to \\infty$.\n",
    "\n",
    "<details>\n",
    "<summary><b>Answer</b></summary>\n",
    "\n",
    "The model bias has increased since the gap between a test point $Y$ and a scaled \"best estimate\" value is likely larger than between the test point and the actual best estimate. As $\\lambda \\to \\infty$, the model bias squared will tend towards $Y^2$.\n",
    "\n",
    "The model variance has decreased since $\\frac{1}{(1 + \\lambda)^2}$ if $\\lambda > 0$ will always be less than 1. Hence, our variance compared to the vanilla bias-variance decomposition from part (b) is reduced by a factor greater than 1. As $\\lambda \\to \\infty$, the model variance will become 0.\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data100quarto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
